# A precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution.

**DOI:** 10.3389/fpls.2025.1641677
**Authors:** ['YutingWei', 'DebinZeng', 'LiangfangZheng']
**Journal:** Frontiers in plant science
**Year:** 
**Scraped from:** DOI.org
**Scraped by:** agent_b
**Scraping date:** 2025-10-21T10:52:39.995714

## Abstract

Walnut leaf brown spot disease, caused by 

## Full Text

## Abstract
| crossref full text | google scholar li, x., zha, t., liu, p., bourque, c. p.-a., jia, x., and tian, y. (2023). interannual variation in gross ecosystem production and evapotranspiration in a temperate semiarid grassland undergoing vegetation recovery. agricultural and forest meteorology , 341, 109672. doi:&#xa0;10.1016/j.agrformet.2023.109672 crossref full text | google scholar lin, k., gong, l., huang, y., liu, c., and pan, j. (2019). deep learning-based segmentation and quantification of cucumber powdery mildew using convolutional neural network. frontiers in plant science , 10, 155. doi:&#xa0;10.3389/fpls.2019.00155 pubmed abstract | crossref full text | google scholar ma, j., du, k., zhang, l., zheng, f., chu, j., and sun, z. (2017). a segmentation method for greenhouse vegetable foliar disease spots images using color information and region growing. computers and electronics in agriculture , 142, 110&#x2013;117. doi:&#xa0;10.1016/j.compag.2017.08.023 crossref full text | google scholar mao, r., wang, z., li, f., zhou, j., chen, y., and hu, x. (2023). gseyolox-s: an improved lightweight network for identifying the severity of wheat fusarium head blight. agronomy , 13, 242. doi:&#xa0;10.3390/agronomy13010242 crossref full text | google scholar mcgranahan, g. and leslie, c. (1991). walnuts (juglans). molecules , 907&#x2013;924. doi:&#xa0;10.17660/actahortic.1991.290.20 crossref full text | google scholar mircetich, s. m., sanborn, r., and ramos, d. (1980). natural spread, graft-transmission, and possible etiology of walnut blackline disease. phytopathology , 70, 962&#x2013;968. doi:&#xa0;10.1094/phyto-70-962 crossref full text | google scholar moragrega, c., matias, j., alet&#xe0;, n., montesinos, e., and rovira, m. (2011). apical necrosis and premature drop of persian (english) walnut fruit caused by xanthomonas arboricola pv. juglandis. plant disease , 95, 1565&#x2013;1570. doi:&#xa0;10.1094/pdis-03-11-0259 pubmed abstract | crossref full text | google scholar ngugi, l. c., abdelwahab, m., and abo-zahhad, m. (2020). tomato leaf segmentation algorithms for mobile phone applications using deep learning. computers and electronics in agriculture , 178, 105788. doi:&#xa0;10.1016/j.compag.2020.105788 crossref full text | google scholar ozturk, o., sarica, b., and seker, d. z. (2025). interpretable and robust ensemble deep learning framework for tea leaf disease classification. horticulturae , 11, 437. doi:&#xa0;10.3390/horticulturae11040437 crossref full text | google scholar parashar, n., johri, p., khan, a. a., gaur, n., and kadry, s. (2024). an integrated analysis of yield prediction models: a comprehensive review of advancements and challenges. computers, materials &amp; continua , 80 (1). doi:&#xa0;10.32604/cmc.2024.050240 crossref full text | google scholar picon, a., alvarez-gila, a., seitz, m., ortiz-barredo, a., echazarra, j., and johannes, a. (2019). deep convolutional neural networks for mobile capture device-based crop disease classification in the wild. computers and electronics in agriculture , 161, 280&#x2013;290. doi:&#xa0;10.1016/j.compag.2018.04.002 crossref full text | google scholar rastogi, a., arora, r., and sharma, s. (2015). &#x201c;leaf disease detection and grading using computer vision technology &amp; fuzzy logic,&#x201d; in paper presented at the 2015 2nd international conference on signal processing and integrated networks (spin). google scholar rustia, d. j. a., lee, w.-c., lu, c.-y., wu, y.-f., shih, p.-y., and chen, s.-k. (2023). edge-based wireless imaging system for continuous monitoring of insect pests in a remote outdoor mango orchard. computers and electronics in agriculture , 211, 108019. doi:&#xa0;10.1016/j.compag.2023.108019 crossref full text | google scholar shi, t., liu, y., zheng, x., hu, k., huang, h., liu, h., et al. (2023). recent advances in plant disease severity assessment using convolutional neural networks. scientific reports , 13, 2336. doi:&#xa0;10.1038/s41598-023-29230-7 pubmed abstract | crossref full text | google scholar singh, u. p., chouhan, s. s., jain, s., and jain, s. (2019). multilayer convolution neural network for the classification of mango leaves infected by anthracnose disease. ieee access , 7, 43721&#x2013;43729. doi:&#xa0;10.1109/access.2019.2907383 crossref full text | google scholar su, y., wang, j., li, j., wang, l., wang, k., li, a., et al. (2023). spatiotemporal changes and driving factors of reference evapotranspiration and crop evapotranspiration for cotton production in china from 1960 to 2019. frontiers in environmental science , 11, 1251789. doi:&#xa0;10.3389/fenvs.2023.1251789 crossref full text | google scholar tripathi, r. (2021). &#x201c;a deep learning approach for plant material disease identification,&#x201d; in paper presented at the iop conference series: materials science and engineering. google scholar vishnoi, v. k., kumar, k., kumar, b., mohan, s., and khan, a. a. (2022). detection of apple plant diseases using leaf images through convolutional neural network . ieee access , 11, 6594&#x2013;6609. doi:&#xa0;10.1109/access.2022.3232917 crossref full text | google scholar wang, f., dun, c., tang, t., duan, y., guo, x., and you, j. (2022). boeremia exigua causes leaf spot of walnut trees (juglans regia) in china. plant disease , 106, 1993. doi:&#xa0;10.1094/pdis-10-21-2304-pdn crossref full text | google scholar wang, q.-h., fan, k., li, d.-w., han, c.-m., qu, y.-y., qi, y.-k., et al. (2020). identification, virulence and fungicide sensitivity of colletotrichum gloeosporioides ss responsible for walnut anthracnose disease in china. plant disease , 104, 1358&#x2013;1368. doi:&#xa0;10.1094/pdis-12-19-2569-re pubmed abstract | crossref full text | google scholar weber, b. c. (1980). how to diagnose black walnut damage vol. 57 (north central forest experiment station, forest service, us department of agriculture). google scholar xu, w. (2024). hcf-net: hierarchicalcontextfusion network forinfrared small object detection. arxiv . arxiv, 2403.10778. doi:&#xa0;10.1109/icme57554.2024.10687431 crossref full text | google scholar yang, c., deng, y., wang, f., yang, h., xu, x., zeng, q., et al. (2021). brown leaf spot on juglans sigillata caused by ophiognomonia leptostyla in sichuan, china. plant disease , 105, 4160. doi:&#xa0;10.1094/pdis-02-21-0344-pdn crossref full text | google scholar yang, q., duan, s., and wang, l. (2022). efficient identification of apple leaf diseases in the wild using convolutional neural networks. agronomy , 12, 2784. doi:&#xa0;10.3390/agronomy12112784 crossref full text | google scholar zarei, s., taghavi, s. m., banihashemi, z., hamzehzarghani, h., and osdaghi, e. (2019). etiology of leaf spot and fruit canker symptoms on stone fruits and nut trees in iran. journal of plant pathology , 101, 1133&#x2013;1142. doi:&#xa0;10.1007/s42161-019-00283-w crossref full text | google scholar zhang, y., li, x., &#x160;im&#x16f;nek, j., shi, h., chen, n., and hu, q. (2023). quantifying water and salt movement in a soil-plant system of a corn field using hydrus (2d/3d) and the stable isotope method. agricultural water management , 288, 108492. doi:&#xa0;10.1016/j.agwat.2023.108492 crossref full text | google scholar keywords: walnut, brown spot disease ( ophiognomonia leptostyla ), hierarchical feature selection, edge features perception, adaptive multi-scale dilated convolution, disease grading citation: wei y, zeng d and zheng l (2025) a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution. front. plant sci. 16:1641677. doi: 10.3389/fpls.2025.1641677 received: 05 june 2025; accepted: 09 september 2025; published: 03 october 2025. edited by: ravinder kumar , indian agricultural research institute (icar), india reviewed by: geza bujdoso , hungarian university of agricultural and life sciences, hungary vasudha vedula , university of texas of the permian basin, united states copyright &#xa9; 2025 wei, zeng and zheng. this is an open-access article distributed under the terms of the creative commons attribution license (cc by) . the use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. no use, distribution or reproduction is permitted which does not comply with these terms. *correspondence: liangfang zheng, mtgxnjaynta0othamtyzlmnvbq== &#x2020; these authors have contributed equally to this work disclaimer: all claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. download article download pdf readcube epub xml share on export citation endnote reference manager simple text file bibtex 1,063 total views 103 downloads citation numbers are available from dimensions view article impact view altmetric score share on edited by r k ravinder kumar reviewed by g b geza bujdoso v v vasudha vedula table of contents abstract 1 introduction 2 materials and methods 3 experiments and results analysis 4 conclusion 5 discussion and future work data availability statement author contributions funding conflict of interest generative ai statement publisher&#x2019;s note references export citation endnote reference manager simple text file bibtex check for updates frontiers&#39; impact articles published with frontiers have received 12 million total citations your research is the real superpower - learn how we maximise its impact through our leading community journals explore our impact metrics supplementary material download article download download pdf readcube epub xml guidelines author guidelines services for authors policies and publication ethics editor guidelines fee policy explore articles research topics journals how we publish outreach frontiers forum frontiers policy labs frontiers for young minds frontiers planet prize connect help center emails and alerts contact us submit career opportunities follow us © 2025 frontiers media s.a. all rights reserved privacy policy | terms and conditions [["shallowreactive",1],{"data":2,"state":4,"once":10,"_errors":11,"serverrendered":13,"path":14,"pinia":15},["shallowreactive",3],{},["reactive",5],{"$ssite-config":6},{"env":7,"name":8,"url":9},"production","frontiers articles","https://article-pages-2024.frontiersin.org/",["set"],["shallowreactive",12],{},true,"/journals/plant-science/articles/10.3389/fpls.2025.1641677/full",["reactive",16],{"main":17,"user":534,"article":535,"articlehub":760,"mainheader":764},{"ibar":18,"footer":284,"newslettercomponent":-1,"snackbaritem":366,"toggleshowsnackbar":367,"contentfuljournal":368,"graphjournal":434,"settingsfeaturesswitchers":438,"templatetogglebanner":439,"tenantconfig":499},{"tenantlogo":19,"journallogo":19,"aboutus":20,"submiturl":113,"showsubmitbutton":13,"journal":114,"sectionterm":215,"aboutjournal":216,"mainlinks":265,"journallinks":272,"helpcenterlink":281},"",[21,38,47,71,87],{"title":22,"links":23},"who we are",[24,29,32,35],{"text":25,"url":26,"target":27,"arialabel":28},"mission and values","https://www.frontiersin.org/about/mission","_self",null,{"text":30,"url":31,"target":27,"arialabel":28},"history","https://www.frontiersin.org/about/history",{"text":33,"url":34,"target":27,"arialabel":28},"leadership","https://www.frontiersin.org/about/leadership",{"text":36,"url":37,"target":27,"arialabel":28},"awards","https://www.frontiersin.org/about/awards",{"title":39,"links":40},"impact and progress",[41,44],{"text":42,"url":43,"target":27,"arialabel":28},"frontiers' impact","https://www.frontiersin.org/about/impact",{"text":45,"url":46,"target":27,"arialabel":28},"our annual reports","https://www.frontiersin.org/about/annual-reports",{"title":48,"links":49},"publishing model",[50,53,56,59,62,65,68],{"text":51,"url":52,"target":27,"arialabel":28},"how we publish","https://www.frontiersin.org/about/how-we-publish",{"text":54,"url":55,"target":27,"arialabel":28},"open access","https://www.frontiersin.org/about/open-access",{"text":57,"url":58,"target":27,"arialabel":28},"peer review","https://www.frontiersin.org/about/peer-review",{"text":60,"url":61,"target":27,"arialabel":28},"research integrity","https://www.frontiersin.org/about/research-integrity",{"text":63,"url":64,"target":27,"arialabel":28},"research topics","https://www.frontiersin.org/about/research-topics",{"text":66,"url":67,"target":27,"arialabel":28},"fair² data management","https://www.frontiersin.org/about/fair-data-management",{"text":69,"url":70,"target":27,"arialabel":28},"fee policy","https://www.frontiersin.org/about/fee-policy",{"title":72,"links":73},"services",[74,78,81,84],{"text":75,"url":76,"target":77,"arialabel":28},"societies","https://publishingpartnerships.frontiersin.org/","_blank",{"text":79,"url":80,"target":27,"arialabel":28},"national consortia","https://www.frontiersin.org/open-access-agreements/consortia",{"text":82,"url":83,"target":27,"arialabel":28},"institutional partnerships","https://www.frontiersin.org/about/open-access-agreements",{"text":85,"url":86,"target":27,"arialabel":28},"collaborators","https://www.frontiersin.org/about/collaborators",{"title":88,"links":89},"more from frontiers",[90,94,97,101,105,109],{"text":91,"url":92,"target":77,"arialabel":93},"frontiers forum","https://forum.frontiersin.org/","this link will take you to the frontiers forum website",{"text":95,"url":96,"target":27,"arialabel":28},"frontiers planet prize","https://www.frontiersin.org/about/frontiers-planet-prize",{"text":98,"url":99,"target":77,"arialabel":100},"press office","https://pressoffice.frontiersin.org/","this link will take you to the frontiers press office website",{"text":102,"url":103,"target":27,"arialabel":104},"sustainability","https://www.frontiersin.org/about/sustainability","link to information about frontiers' sustainability",{"text":106,"url":107,"target":77,"arialabel":108},"career opportunities","https://careers.frontiersin.org/","this link will take you to the frontiers careers website",{"text":110,"url":111,"target":27,"arialabel":112},"contact us","https://www.frontiersin.org/about/contact","this link will take you to the help pages to contact our support team","https://www.frontiersin.org/submission/submit?domainid=1&fieldid=66&specialtyid=0&entitytype=2&entityid=373",{"id":115,"name":116,"slug":117,"sections":118},373,"frontiers in plant science","plant-science",[119,123,127,131,135,139,143,147,151,155,159,163,167,171,175,179,183,187,191,195,199,203,207,211],{"id":120,"name":121,"slug":122},1553,"aquatic photosynthetic organisms","aquatic-photosynthetic-organisms",{"id":124,"name":125,"slug":126},1356,"crop and product physiology","crop-and-product-physiology",{"id":128,"name":129,"slug":130},467,"functional plant ecology","functional-plant-ecology",{"id":132,"name":133,"slug":134},2844,"functional and applied plant genomics","functional-and-applied-plant-genomics",{"id":136,"name":137,"slug":138},2843,"photosynthesis and photobiology","photosynthesis-and-photobiology",{"id":140,"name":141,"slug":142},1312,"plant abiotic stress","plant-abiotic-stress",{"id":144,"name":145,"slug":146},2183,"plant bioinformatics","plant-bioinformatics",{"id":148,"name":149,"slug":150},589,"plant biophysics and modeling","plant-biophysics-and-modeling",{"id":152,"name":153,"slug":154},560,"plant biotechnology","plant-biotechnology",{"id":156,"name":157,"slug":158},468,"plant breeding","plant-breeding",{"id":160,"name":161,"slug":162},577,"plant cell biology","plant-cell-biology",{"id":164,"name":165,"slug":166},474,"plant development and evodevo","plant-development-and-evodevo",{"id":168,"name":169,"slug":170},2845,"plant genetics, epigenetics and chromosome biology","plant-genetics-epigenetics-and-chromosome-biology",{"id":172,"name":173,"slug":174},479,"plant membrane traffic and transport","plant-membrane-traffic-and-transport",{"id":176,"name":177,"slug":178},481,"plant metabolism and chemodiversity","plant-metabolism-and-chemodiversity",{"id":180,"name":181,"slug":182},486,"plant nutrition","plant-nutrition",{"id":184,"name":185,"slug":186},485,"plant pathogen interactions","plant-pathogen-interactions",{"id":188,"name":189,"slug":190},226,"plant physiology","plant-physiology",{"id":192,"name":193,"slug":194},580,"plant proteomics and protein structural biology","plant-proteomics-and-protein-structural-biology",{"id":196,"name":197,"slug":198},1570,"plant symbiotic interactions","plant-symbiotic-interactions",{"id":200,"name":201,"slug":202},1428,"plant systematics and evolution","plant-systematics-and-evolution",{"id":204,"name":205,"slug":206},483,"plant systems and synthetic biology","plant-systems-and-synthetic-biology",{"id":208,"name":209,"slug":210},2277,"sustainable and intelligent phytoprotection","sustainable-and-intelligent-phytoprotection",{"id":212,"name":213,"slug":214},484,"technical advances in plant science","technical-advances-in-plant-science","sections",[217,241],{"title":218,"links":219},"scope",[220,223,226,229,232,235,238],{"text":221,"url":222,"target":27,"arialabel":28},"field chief editors","https://www.frontiersin.org/journals/plant-science/about#about-editors",{"text":224,"url":225,"target":27,"arialabel":28},"mission & scope","https://www.frontiersin.org/journals/plant-science/about#about-scope",{"text":227,"url":228,"target":27,"arialabel":28},"facts","https://www.frontiersin.org/journals/plant-science/about#about-facts",{"text":230,"url":231,"target":27,"arialabel":28},"journal sections","https://www.frontiersin.org/journals/plant-science/about#about-submission",{"text":233,"url":234,"target":27,"arialabel":28},"open access statement","https://www.frontiersin.org/journals/plant-science/about#about-open",{"text":236,"url":237,"target":27,"arialabel":28},"copyright statement","https://www.frontiersin.org/journals/plant-science/about#copyright-statement",{"text":239,"url":240,"target":27,"arialabel":28},"quality","https://www.frontiersin.org/journals/plant-science/about#about-quality",{"title":242,"links":243},"for authors",[244,247,250,253,256,259,262],{"text":245,"url":246,"target":27,"arialabel":28},"why submit?","https://www.frontiersin.org/journals/plant-science/for-authors/why-submit",{"text":248,"url":249,"target":27,"arialabel":28},"article types","https://www.frontiersin.org/journals/plant-science/for-authors/article-types",{"text":251,"url":252,"target":27,"arialabel":28},"author guidelines","https://www.frontiersin.org/journals/plant-science/for-authors/author-guidelines",{"text":254,"url":255,"target":27,"arialabel":28},"editor guidelines","https://www.frontiersin.org/journals/plant-science/for-authors/editor-guidelines",{"text":257,"url":258,"target":27,"arialabel":28},"publishing fees","https://www.frontiersin.org/journals/plant-science/for-authors/publishing-fees",{"text":260,"url":261,"target":27,"arialabel":28},"submission checklist","https://www.frontiersin.org/journals/plant-science/for-authors/submission-checklist",{"text":263,"url":264,"target":27,"arialabel":28},"contact editorial office","https://www.frontiersin.org/journals/plant-science/for-authors/contact-editorial-office",[266,269],{"text":267,"url":268,"target":27,"arialabel":28},"all journals","https://www.frontiersin.org/journals",{"text":270,"url":271,"target":27,"arialabel":28},"all articles","https://www.frontiersin.org/articles",[273,276,278],{"text":274,"url":275,"target":27,"arialabel":28},"articles","articles",{"text":63,"url":277,"target":27,"arialabel":28},"research-topics",{"text":279,"url":280,"target":27,"arialabel":28},"editorial board","editors",{"text":282,"url":283,"target":77,"arialabel":282},"help center","https://helpcenter.frontiersin.org",{"blocks":285,"sociallinks":339,"copyright":363,"termsandconditionsurl":364,"privacypolicyurl":365},[286,300,310,324],{"title":287,"links":288},"guidelines",[289,291,294,297,299],{"text":251,"url":290,"target":27,"arialabel":28},"https://www.frontiersin.org/guidelines/author-guidelines",{"text":292,"url":293,"target":27,"arialabel":28},"services for authors","https://www.frontiersin.org/for-authors/author-services",{"text":295,"url":296,"target":27,"arialabel":28},"policies and publication ethics","https://www.frontiersin.org/guidelines/policies-and-publication-ethics",{"text":254,"url":298,"target":27,"arialabel":28},"https://www.frontiersin.org/guidelines/editor-guidelines",{"text":69,"url":70,"target":27,"arialabel":28},{"title":301,"links":302},"explore",[303,304,307,309],{"text":274,"url":271,"target":27,"arialabel":28},{"text":305,"url":306,"target":27,"arialabel":28},"research topics ","https://www.frontiersin.org/research-topics",{"text":308,"url":268,"target":27,"arialabel":28},"journals",{"text":51,"url":52,"target":27,"arialabel":28},{"title":311,"links":312},"outreach",[313,316,319,323],{"text":314,"url":92,"target":77,"arialabel":315},"frontiers forum ","frontiers forum website",{"text":317,"url":318,"target":77,"arialabel":28},"frontiers policy labs ","https://policylabs.frontiersin.org/",{"text":320,"url":321,"target":77,"arialabel":322},"frontiers for young minds","https://kids.frontiersin.org/","frontiers for young minds journal",{"text":95,"url":96,"target":27,"arialabel":28},{"title":325,"links":326},"connect",[327,328,332,335,338],{"text":282,"url":283,"target":77,"arialabel":282},{"text":329,"url":330,"target":77,"arialabel":331},"emails and alerts ","https://subscription-management.frontiersin.org/emails/preferences#block0","subscribe to frontiers emails",{"text":333,"url":111,"target":27,"arialabel":334},"contact us ","subscribe to newsletter",{"text":336,"url":337,"target":27,"arialabel":28},"submit","https://www.frontiersin.org/submission/submit",{"text":106,"url":107,"target":77,"arialabel":28},[340,348,353,358],{"link":341,"type":344,"color":345,"icon":346,"size":347,"hiddentext":13},{"text":342,"url":343,"target":77,"arialabel":342},"frontiers facebook","https://www.facebook.com/frontiersin","link","grey","facebook","medium",{"link":349,"type":344,"color":345,"icon":352,"size":347,"hiddentext":13},{"text":350,"url":351,"target":77,"arialabel":28},"frontiers twitter","https://twitter.com/frontiersin","twitter",{"link":354,"type":344,"color":345,"icon":357,"size":347,"hiddentext":13},{"text":355,"url":356,"target":77,"arialabel":28},"frontiers linkedin","https://www.linkedin.com/company/frontiers","linkedin",{"link":359,"type":344,"color":345,"icon":362,"size":347,"hiddentext":13},{"text":360,"url":361,"target":77,"arialabel":28},"frontiers instagram","https://www.instagram.com/frontiersin_","instagram","frontiers media s.a. all rights reserved","https://www.frontiersin.org/legal/terms-and-conditions","https://www.frontiersin.org/legal/privacy-policy",{},false,{"__typename":369,"identifier":115,"name":116,"slug":117,"banner":370,"description":427,"mission":428,"palette":429,"impactfactor":430,"citescore":431,"citations":432,"showtagline":28,"twitter":433},"journal",[371],{"id":372,"src":373,"name":374,"tags":375,"type":385,"width":386,"height":387,"idhash":388,"archive":389,"brandid":390,"limited":389,"filesize":391,"ispublic":392,"original":393,"copyright":394,"extension":395,"thumbnails":397,"datecreated":405,"description":406,"orientation":407,"usercreated":408,"watermarked":389,"datemodified":405,"datepublished":409,"ecsarchivefiles":410,"propertyoptions":411,"property_channel":416,"property_sub-type":418,"property_asset_type":420,"activeoriginalfocuspoint":422,"property_office_department":425},"450e9326-0272-405c-b8d614c72bed9f89","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/webimage-00a7f7fd-61fc-4329-b3bfcc0119b4b276.jpg","fpls_main visual_green_website",[376,377,378,379,380,381,382,383,384],"medical","vitality","decoration","five","cosmetic","r","cure","aloe vera","spike","image",4928,3264,"720507a162925515",0,"22c10171-81b3-4da6-99342f272a32e8bb",11359471,1,"https://brand.frontiersin.org/m/720507a162925515/original/fpls_main-visual_green_website.jpeg","copyright (c) 2017 sabine hortebusch/shutterstock. no use without permission.",[396],"jpeg",{"mini":398,"thul":399,"webimage":373,"guidelines":400,"websitejpg_xl":401,"websitewebp_l":402,"websitewebp_m":403,"websitewebp_xl":404},"https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/mini-006feee0-2d70-4630-b9abfb0a0fa410aa.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/thul-c817db1b-00e8-4b29-b71c4e5a6058947f.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/guidelines-fpls_main visual_green_website.png","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitejpg_xl-fpls_main visual_green_website.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_l-fpls_main visual_green_website.webp","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_m-fpls_main visual_green_website.webp","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_xl-fpls_main visual_green_website.webp","2022-06-27t10:00:48z","spiral aloe vera with water drops, closeup","landscape","caroline sutter","2022-06-27t09:27:09z",[],[412,413,414,415],"414fb2d4-2283-43fd-be14e534eca67928","6c18119b-14bd-4951-b437696f4357bd33","7c692885-db25-4858-b1fb4ff47b241e9b","d88c0047-ec30-4506-a7df28a4d765e1cf",[417],"frontiersin_org",[419],"main_visual",[421],"photography",{"x":423,"y":424},2464,1632,[426],"publishing","the most cited plant science journal, advancing our understanding of plant biology for sustainable food security, functional ecosystems and human health.","\u003cp>frontiers in plant science is a leading, multidisciplinary journal that seeks to advance our understanding of fundamental processes in plant biology.\u003c/p>\n\n\u003cp>led by field chief editor prof. chun-ming liu (institute of botany, chinese academy of sciences) and indexed in pubmed, pubmed central, and scopus, among others, the journal seeks original and significant contributions that cultivate plant biology and its applications. the journal has the long-term goal of supporting sustainable development, food security, functional ecosystems, biotechnology (including biofuels and biomaterials), and human health.\u003c/p>\n\u003cp>frontiers in plant science welcomes original research, review, opinion, and perspective articles, among other submission types, covering the journal’s specialty sections:\u003c/p>\n\n\u003cdiv> &bull; aquatic photosynthetic organisms\u003c/div>\n\u003cdiv> &bull; crop and product physiology\u003c/div>\n\u003cdiv> &bull; functional plant ecology\u003c/div>\n\u003cdiv> &bull; functional and applied plant genomics\u003c/div>\n\u003cdiv> &bull; photosynthesis and photobiology\u003c/div>\n\u003cdiv> &bull; plant abiotic stress\u003c/div>\n\u003cdiv> &bull; plant bioinformatics\u003c/div>\n\u003cdiv> &bull; plant biophysics and modeling\u003c/div>\n\u003cdiv> &bull; plant biotechnology\u003c/div>\n\u003cdiv> &bull; plant breeding\u003c/div>\n\u003cdiv> &bull; plant cell biology\u003c/div>\n\u003cdiv> &bull; plant development and evodevo\u003c/div>\n\u003cdiv> &bull; plant genetics, epigenetics and chromosome biology\u003c/div>\n\u003cdiv> &bull; plant membrane traffic and transport\u003c/div>\n\u003cdiv> &bull; plant metabolism and chemodiversity\u003c/div>\n\u003cdiv> &bull; plant nutrition\u003c/div>\n\u003cdiv> &bull; plant pathogen interactions\u003c/div>\n\u003cdiv> &bull; plant physiology\u003c/div>\n\u003cdiv> &bull; plant proteomics and protein structural biology\u003c/div>\n\u003cdiv> &bull; plant symbiotic interactions\u003c/div>\n\u003cdiv> &bull; plant systematics and evolution\u003c/div>\n\u003cdiv> &bull; plant systems and synthetic biology\u003c/div>\n\u003cdiv> &bull; sustainable and intelligent phytoprotection\u003c/div>\n\u003cdiv> &bull; technical advances in plant science.\u003c/div>\n\u003cbr>\n\u003cp>furthermore, the journal welcomes submissions that support and advance the un's sustainable development goals (sdgs), notably sdg 13: climate action and sdg 15: life on land.\u003c/p> \n\n\u003cp>frontiers in plant science is committed to advancing developments in the field of plant biology by allowing unrestricted access to articles and communicating scientific knowledge to researchers and the public alike, to enable the scientific breakthroughs of the future.\u003c/p> \n \n\u003cp>requirements\u003cp>\n\u003cp>manuscripts that focus on non-plant-related microbiology, human or animal genetics, and medical and pharmacological research are not suitable for publication in this journal. pure field agriculture studies such as those focusing on fertilizer application or yield optimization, without relevance to plant science, are also not within the scope of this journal.\u003c/p>\nstudies falling into the categories below will not be considered for review in this journal unless they are expanded and provide insight into the biological process being studied:\u003c/p>\n\n\u003cp>i) descriptive collections of transcripts, proteins, or metabolites, including comparative sets as a result of different conditions or treatments;\u003c/p>\n\u003cp>ii) descriptive studies that define gene families using pure phylogenetics and the assignment of cursory functional attributions (e.g. expression profiles, promoter analysis, and bioinformatic parameters).\u003c/p>\n\n\u003cp>quantitative analysis needs to be performed on a minimum of three biological replicates in order to enable an assessment of significance. this includes quantitative omics studies (transcriptomics, proteomics, metabolomics) as well as phenotypic measurements, quantitative assays, and qpcr expression analysis. studies that do not comply with these replication requirements will not be considered for review.\u003cp>\n\n\u003cp>studies using transgenic or mutant lines (plants and algae), for example, t-dna, transposon, rnai, crispr/cas9, chemically induced, overexpressors and reporter fusions (gus, gfps, luc), should be based on data from multiple alleles (minimum of two) displaying a common and stable phenotype. qualitative data can be presented from a single allele but should be indicative of observations from multiple alleles which should be explicitly stated in the text. quantitative data should be derived from multiple alleles (at least two) and should be displayed separately for each allele (with at least three independent replications for each allele). studies reporting single alleles may be considered acceptable when:\u003c/p>\n\n\u003cp>i) complementation via transformation is used for confirmation;\u003c/p> \n\u003cp>ii) the allele has been previously characterized and published, and is representative of multiple independent lines;\u003c/p>\n\u003cp>iii) in situations where genetic transformation is difficult or not yet possible, alternative evidence should be presented\u003c/p>\n\n","green","5.6","7.1","927148","@frontplantsci",{"id":115,"name":116,"slug":117,"abbreviation":435,"isonline":13,"isopenforsubmissions":13,"citescore":436,"impactfactor":437},"fpls",8.8,4.8,{"displaytitlepilllabels":13,"displayrelatedarticlesbox":13,"showeditors":13,"showreviewers":13,"showloopimpactlink":13,"enablefigshare":367,"usexmlimages":13},{"ispublic":13,"allowcompanyusers":367,"whitelistemails":440,"enablealljournals":13,"whitelistjournals":462},[441,442,443,444,445,446,447,448,449,446,450,451,452,453,454,455,456,457,458,459,460,461],"y2fybg9zlmxvcmnhqgzyb250awvyc2lulm9yzw==","zgfuawvslmx1ekbmcm9udgllcnnpbi5vcmc=","bgf1cmeudgvuyubmcm9udgllcnnpbi5vcmc=","cmfmywvslnjpyw5jag9aznjvbnrpzxjzaw4ub3jn","amftzxmuc21hbgxib25lqgzyb250awvyc2lulm9yzw==","znjhbi5tb3jlbm9aznjvbnrpzxjzaw4ub3jn","c2ftdwvslmzlcm5hbmrlekbmcm9udgllcnnpbi5vcmc=","ymfyymfyys5jyxjjyw5naxvaznjvbnrpzxjzaw4ub3jn","axzhbi5zyw50yw1hcmlhqgzyb250awvyc2lulm9yzw==","ahvllnryyw5aznjvbnrpzxjzaw4ub3jn","z29uy2fsby52yxjnyxnaznjvbnrpzxjzaw4ub3jn","bhvjawuuc2vubkbmcm9udgllcnnpbi5vcmc=","bg9ybi5mcmfzzxjaznjvbnrpzxjzaw4ub3jn","zwxzys5jyxjyb25aznjvbnrpzxjzaw4ub3jn","bwfhcnrlbi52yw5kawpja0bmcm9udgllcnnpbi5vcmc=","ymluzgh1lmtyaxnobmfuqgzyb250awvyc2lulm9yzw==","bwf0dghldy5hdhr3yxrlcnnaznjvbnrpzxjzaw4ub3jn","z2l1bglhlnzhbhnly2noaubmcm9udgllcnnpbi5vcmc=","zmfiawfulmrlbgxhbw9ydgvaznjvbnrpzxjzaw4ub3jn","dmlqyxlhbi5wcebwaxrzb2x1dglvbnmuy29t","z3vpbgxhdw1llnnldxjhdebmcm9udgllcnnpbi5vcmc=",[463,464,465,466,467,468,392,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498],2232,1729,2357,2456,2176,2333,1843,602,106,616,310,657,3123,1468,2137,628,1566,2726,2086,1490,451,141,1335,2655,1238,604,698,1547,176,1440,403,1239,755,2136,609,1534,{"spaceid":392,"name":392,"availablejournalpages":500,"announcement":504},[275,280,277,501,502,503],"volumes","about","community-reviewers",{"__typename":505,"sys":506,"preheader":42,"title":508,"description":509,"image":510,"link":532},"announcement",{"id":507},"5ittnttqgazppgh6rx0alm","articles published with frontiers have received 12 million total citations","your research is the real superpower - learn how we maximise its impact through our leading community journals",[511],{"archive":389,"brandid":390,"copyright":28,"datecreated":512,"datemodified":513,"datepublished":514,"description":28,"extension":515,"filesize":517,"height":518,"id":519,"ispublic":389,"limited":389,"name":520,"orientation":407,"original":28,"thumbnails":521,"type":385,"watermarked":389,"width":528,"videopreviewurls":529,"tags":530,"textmetaproperties":531,"src":522},"2025-06-18t12:58:01z","2025-06-18t14:15:34z","2025-06-18t12:57:32z",[516],"png",1365026,920,"7c5269c4-3c83-4591-951a74d15b95daea","impact metrics banner for article pages",{"webimage":522,"thul":523,"mini":524,"websitewebp_l":525,"websitewebp_m":526,"guidelines":527},"https://brand.frontiersin.org/m/59ee6275cb9a849c/webimage-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/m/59ee6275cb9a849c/thul-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/m/59ee6275cb9a849c/mini-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/websitewebp_l/impact-metrics-banner-for-article-pages.webp","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/websitewebp_m/impact-metrics-banner-for-article-pages.webp","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/guidelines/impact-metrics-banner-for-article-pages.png",1600,[],[],[],{"text":533,"url":43,"target":27,"arialabel":28},"explore our impact metrics",{"loggeduserinfo":-1},{"currentarticle":536,"ispreviewpage":367,"hassupplementaldata":367,"showcrossmarkwidget":13,"articletemplate":648,"currentarticlepagemetainfo":649,"xmlparsedarticlecontent":-1,"xmlparsingerror":-1},{"id":537,"doi":538,"title":539,"acceptancedate":540,"receptiondate":541,"publicationdate":542,"lastmodifieddate":543,"ispublished":13,"abstract":544,"researchtopic":545,"articletype":551,"stage":554,"keywords":556,"authors":563,"editors":587,"reviewers":595,"journal":610,"section":617,"impactmetrics":619,"volume":622,"articlevolume":623,"relatedarticles":624,"ispublishedv2":13,"contents":625,"files":628},1641677,"10.3389/fpls.2025.1641677","a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution","2025-09-09t14:48:37.000z","2025-06-05t09:39:42.000z","2025-10-03t00:00:00.000z","2025-10-21t02:25:15.907z","walnut leaf brown spot disease, caused by ophiognomonia leptostyla, is among the most destructive fungal diseases in walnut cultivation. in the development of smart agriculture, precision grading of plant diseases remains a core technical challenge; specifically, this disease is plagued by blurred lesion edges and inefficient extraction of complex features, which directly limits the accurate grading of the disease. to address these issues, this study proposes a disease grading method integrating hierarchical feature selection and adaptive multi-scale dilated convolution, and develops the cogfuse-mobilevit model. this model overcomes the limitations of the standard mobilevitv3 model in capturing blurred edges of tiny lesions via three innovative modules: specifically, the hierarchical feature screening module (hfsm) enables hierarchical screening of disease-related features; the edge feature focus module (ecfm) works in synergy with the hfsm to enhance the focus on lesion edge features; and the adaptive multi-scale dilated convolution fusion module (amsdidcm) achieves dynamic multi-scale fusion of lesion textures and global structures. experimental results demonstrate that the proposed model achieves an accuracy of 86.61% on the test set, representing an improvement of 7.8 percentage points compared with the original mobilevitv3 model and significantly outperforming other mainstream disease grading models. this study confirms that the cogfuse-mobilevit model can effectively resolve the issues of blurred edges and inefficient feature extraction in this disease, provides a reliable technical solution for its precision grading, and holds practical application value for the intelligent diagnosis of plant diseases in smart agriculture.",{"id":546,"title":547,"articlescount":548,"ismagazinepage":367,"slug":549,"isopenforsubmission":13,"views":550},66487,"innovative field diagnostics for real-time plant pathogen detection and management",9,"innovative-field-diagnostics-for-real-time-plant-pathogen-detection-and-management",14940,{"id":552,"name":553},24,"original research",{"id":555,"name":19},18,[557,558,559,560,561,562],"walnut","brown spot disease (ophiognomonia leptostyla)","hierarchical feature selection","edge features perception","adaptive multi-scale dilated convolution","disease grading",[564,575,581],{"id":565,"firstname":566,"middlename":19,"lastname":567,"givennames":568,"iscorresponding":367,"isprofilepublic":13,"userid":565,"email":-1,"affiliations":569},3089871,"yuting","wei","yuting ",[570,573],{"organizationname":571,"countryname":572,"cityname":19,"statename":19,"zipcode":19},"college of information engineering, tarim university","china",{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},"key laboratory of tarim oasis agriculture, ministry of education, tarim university",{"id":389,"firstname":576,"middlename":19,"lastname":577,"givennames":578,"iscorresponding":367,"isprofilepublic":367,"userid":389,"email":-1,"affiliations":579},"debin","zeng","debin ",[580],{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},{"id":389,"firstname":582,"middlename":19,"lastname":583,"givennames":584,"iscorresponding":13,"isprofilepublic":367,"userid":389,"email":19,"affiliations":585},"liangfang","zheng","liangfang ",[586],{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},[588],{"id":589,"firstname":590,"middlename":19,"lastname":591,"givennames":592,"iscorresponding":367,"isprofilepublic":13,"userid":589,"email":-1,"affiliations":593},1037951,"ravinder","kumar","ravinder ",[594],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},[596,603],{"id":597,"firstname":598,"middlename":19,"lastname":599,"givennames":600,"iscorresponding":367,"isprofilepublic":13,"userid":597,"email":-1,"affiliations":601},2082015,"geza","bujdoso","geza ",[602],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},{"id":604,"firstname":605,"middlename":19,"lastname":606,"givennames":607,"iscorresponding":367,"isprofilepublic":13,"userid":604,"email":-1,"affiliations":608},3078480,"vasudha","vedula","vasudha ",[609],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},{"id":115,"slug":117,"name":116,"shortname":611,"electronicissn":612,"field":613,"specialtyid":28,"journalsectionpaths":615},"front. plant sci.","1664-462x",{"id":614,"domainid":392},66,[616],{"section":617},{"id":208,"name":209,"slug":210,"specialtyid":618},2685,{"views":620,"downloads":621,"citations":389},1063,103,16,"volume 16 - 2025",[],{"titlehtml":539,"fulltexthtml":626,"menuhtml":627},"\u003cdiv class=\"journalabstract\">\u003ca id=\"h1\" name=\"h1\">\u003c/a>\u003cdiv class=\"authors\">\u003cspan class=\"author-wrapper notranslate\">\u003ca href=\"https://loop.frontiersin.org/people/3089871/overview\" class=\"user-id-3089871/overview\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/images/profile/3089871/overview/74\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\" alt=\"yuting wei,&#x;\">yuting wei\u003c/a>\u003csup>1,2&#x2020;\u003c/sup>\u003c/span>\u003cspan class=\"author-wrapper notranslate\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/cdn/images/profile/default_32.jpg\" alt=\"debin zeng&#x;\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\">debin zeng\u003csup>2&#x2020;\u003c/sup>\u003c/span>\u003cspan class=\"author-wrapper notranslate\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/cdn/images/profile/default_32.jpg\" alt=\"liangfang zheng*\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\">liangfang zheng\u003csup>2*\u003c/sup>\u003c/span>\u003c/div>\u003cul class=\"notes\">\u003cli>\u003cspan>\u003csup>1\u003c/sup>\u003c/span>college of information engineering, tarim university, alaer, china\u003c/li>\u003cli>\u003cspan>\u003csup>2\u003c/sup>\u003c/span>key laboratory of tarim oasis agriculture, ministry of education, tarim university, alaer, china\u003c/li>\u003c/ul>\u003cp>walnut leaf brown spot disease, caused by \u003ci>ophiognomonia leptostyla\u003c/i>, is among the most destructive fungal diseases in walnut cultivation. in the development of smart agriculture, precision grading of plant diseases remains a core technical challenge; specifically, this disease is plagued by blurred lesion edges and inefficient extraction of complex features, which directly limits the accurate grading of the disease. to address these issues, this study proposes a disease grading method integrating hierarchical feature selection and adaptive multi-scale dilated convolution, and develops the cogfuse-mobilevit model. this model overcomes the limitations of the standard mobilevitv3 model in capturing blurred edges of tiny lesions via three innovative modules: specifically, the hierarchical feature screening module (hfsm) enables hierarchical screening of disease-related features; the edge feature focus module (ecfm) works in synergy with the hfsm to enhance the focus on lesion edge features; and the adaptive multi-scale dilated convolution fusion module (amsdidcm) achieves dynamic multi-scale fusion of lesion textures and global structures. experimental results demonstrate that the proposed model achieves an accuracy of 86.61% on the test set, representing an improvement of 7.8 percentage points compared with the original mobilevitv3 model and significantly outperforming other mainstream disease grading models. this study confirms that the cogfuse-mobilevit model can effectively resolve the issues of blurred edges and inefficient feature extraction in this disease, provides a reliable technical solution for its precision grading, and holds practical application value for the intelligent diagnosis of plant diseases in smart agriculture.\u003c/p>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"journalfulltext\">\u003ca id=\"h2\" name=\"h2\">\u003c/a>\u003ch2>1 introduction\u003c/h2>\u003cp class=\"mb15\">walnut diseases pose a serious threat to walnut production in xinjiang. the spread of crop diseases significantly exacerbates the security risks of the walnut industry if timely prevention and control measures are not taken (\u003ca href=\"#b7\">cooke, 2006\u003c/a>; \u003ca href=\"#b12\">khan et&#xa0;al., 2021\u003c/a>). as a core component of the disease prevention and control system, early precise grading plays a critical role in agricultural production management (\u003ca href=\"#b2\">adaskaveg et&#xa0;al., 2009\u003c/a>; \u003ca href=\"#b6\">chiang et&#xa0;al., 2016\u003c/a>; \u003ca href=\"#b14\">lamichhane, 2014\u003c/a>). walnut brown spot, caused primarily by the fungus \u003ci>ophiognomonia leptostyla\u003c/i>, typically forms specific symptoms on organs such as leaves, flowers, and fruits. among them, leaves, as the primary carrier of plant diseases, exhibit characteristics such as lesion morphology and color changes on their surfaces, which often serve as important bases for disease grading (\u003ca href=\"#b8\">ghaiwat and arora, 2014\u003c/a>; \u003ca href=\"#b35\">weber, 1980\u003c/a>; \u003ca href=\"#b39\">zarei et&#xa0;al., 2019\u003c/a>). traditional disease identification relies on manual experience (\u003ca href=\"#b21\">moragrega et&#xa0;al., 2011\u003c/a>; \u003ca href=\"#b34\">wang et&#xa0;al., 2020\u003c/a>), a method that is not only time- color system to reduce interference from light and leaf veins, and lesion edge detection using the sobel operator, ultimately achieving fast and accurate grading based on the ratio of lesion area to leaf area. \u003ca href=\"#b10\">jadhav and patil (2016)\u003c/a> developed a leaf image partitioning technique based on k-means clustering and squared euclidean distance, automatically quantifying damaged leaf area through the pixel ratio of lesions to leaves for disease grading. \u003ca href=\"#b3\">arivazhagan et al. (2013)\u003c/a> proposed a four-step processing system involving color conversion, green pixel removal, segmentation, and texture feature classification for leaf disease grading. however, traditional methods for acquiring disease information suffer from insufficient segmentation accuracy for plant leaf disease images with complex textures and unclear lesion boundaries, thereby resulting in poor disease severity grading performance.\u003c/p>\u003cp class=\"mb15\">with the continuous advancement of computational power, deep learning has increasingly become a key technology for addressing complex lesion segmentation, primarily due to its superior capability in automatic feature extraction (\u003ca href=\"#b18\">mao et&#xa0;al., 2023\u003c/a>). \u003ca href=\"#b5\">chen et al. (2021)\u003c/a> proposed the blsnet method based on the unet semantic segmentation network, enhancing lesion segmentation accuracy through the introduction of attention mechanisms and multi-scale feature fusion. experiments showed that its segmentation and classification accuracy outperformed benchmark models such as deeplabv3+ and unet, preliminarily verifying the reliability of this method in automatic assessment of bls disease severity. \u003ca href=\"#b22\">ngugi et al. (2020)\u003c/a> developed the kijaninet segmentation network based on fully convolutional neural networks, demonstrating excellent performance in tomato leaf segmentation under complex backgrounds; \u003ca href=\"#b16\">lin et al. (2019)\u003c/a> developed a cnn semantic segmentation model that achieved high-precision pixel-level segmentation of cucumber powdery mildew on leaves. additionally, some studies have fused traditional features with deep learning: \u003ca href=\"#b31\">tripathi (2021)\u003c/a> proposed a convolutional neural network method based on alexnet, achieving disease grading by fusing features extracted by the model with external leaf segmentation features; \u003ca href=\"#b17\">ma et al. (2017)\u003c/a> introduced comprehensive color features (ccf) combining hyper-red index, hsv/h and lab/b components, and achieved lesion segmentation via interactive region growing, with experiments confirming that this method enables accurate grading of disease images such as cucumber downy mildew under actual field conditions. however, although such methods have improved grading accuracy to some extent, they still suffer from insufficient feature capture of blurred disease edges, making it difficult to achieve fine-grained differentiation of subtle differences between disease grades (\u003ca href=\"#b26\">rastogi et&#xa0;al., 2015\u003c/a>).\u003c/p>\u003cp class=\"mb15\">in recent years, the application of deep learning in plant disease classification has continued to expand. \u003ca href=\"#b24\">parashar et al. (2024)\u003c/a> systematically validated the capability of complex feature modeling for crop yield prediction, further substantiating the critical role of adaptive feature extraction in agricultural intelligent decision-making. concurrently, \u003ca href=\"#b32\">vishnoi et al. (2022)\u003c/a> enhanced small-target detection precision in apple disease identification through a spatial attention mechanism-based cnn architecture for leaf disease diagnosis. \u003ca href=\"#b23\">ozturk et al. (2025)\u003c/a> constructed an ensemble learning classification model based on resnet50, mobile net, efficientnetb0, and densenet121, enhancing generalization performance through statistical cross-validation and improving decision interpretability via grad-cam visualization. experimental results showed that the ensemble model achieved stable high-precision classification performance across multiple validation rounds. these studies provide robust and interpretable solutions for intelligent plant disease recognition through technical integration and architectural innovation. \u003ca href=\"#b9\">hu et al. (2021)\u003c/a> integrated retinex enhancement, faster r-cnn detection, and vgg16 classification to improve the grading and detection accuracy of blurred diseased leaves. \u003ca href=\"#b25\">picon et al. (2019)\u003c/a> proposed an adaptive deep residual neural network algorithm for the classification of three european wheat diseases (septoria leaf blotch, brown spot, and rust) in real-world scenarios, which effectively improved the classification accuracy of wheat diseases. \u003ca href=\"#b13\">kim and ahn (2021)\u003c/a> employed deep cnn architectures such as resnet, xception, and densenet, combined with transfer learning and fine-tuning, to classify 9 categories of pests, diseases, and healthy states in tomatoes. although densenet combined with the rmsprop algorithm achieved an accuracy of 98.63%, single cnn architectures have clear bottlenecks in handling complex plant lesions and overlapping multiple diseases, including insufficient specificity in feature extraction and limited ability to distinguish subtle differences between lesions. \u003ca href=\"#b28\">shi et al. (2023)\u003c/a> analyzed the application of cnns in evaluating the severity of plant diseases, pointing out that hybrid architectures such as classical cnns, improved cnns, and segmentation networks have limitations in handling complex plant lesions: classification errors caused by concurrent multiple diseases, as well as constraints on model generalization ability due to unbalanced datasets and insufficient annotation accuracy. these issues significantly restrict their precise application in practical agricultural scenarios.\u003c/p>\u003cp class=\"mb15\">although significant advancements have been made in plant lesion segmentation and disease classification using deep learning, the grading task for walnut leaf brown spot disease&#x2014;characterized by blurred edge representation and complex small lesions&#x2014;still faces notable challenges. current mobilevit-based hybrid models fall into two categories: general architectures (mobilevitv1/v2/v3) focus on optimizing the cnn-transformer balance for natural images; domain-improved models (mobile-former/edgevit) are oriented toward medical/industrial tasks, with their task focus on localization rather than grading, which mismatches the pain points and fails to address the blurriness of agricultural lesions, tissue interference, and morphological variations. thus, the present study proposes a novel cogfuse-mobilevit model specifically designed for disease grading, with its specific contributions as follows:\u003c/p>\u003cp style=\"margin-top:1em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">1. a diverse field-collected walnut leaf dataset was constructed, with images divided into four distinct severity levels based on qualitative assessment of disease severity, ensuring the accuracy of disease severity grading.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">2. the hierarchical feature selection module (hfsm) enhances the fusion of local details (such as lesion texture and color) with global context through local and global attention mechanisms and task-driven feature selection, while suppressing interference from healthy regions.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">3. the edge convolution fusion module (ecfm) strengthens edge details through sobel convolution and residual connections, achieving effective integration of edge-specific features with general features, further enhancing edge details and enabling more precise capture of lesion contour details.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:1em;margin-left:1em;text-indent:-1em;text-align:left\">4. the adaptive multi-scale dilated dense inception convolution module (amsddicm) extracts differentiated features using multi-shaped convolution kernels and adaptively fuses multi-scale information via a dynamic weight mechanism, capturing lesion morphologies at different pathogenesis stages.\u003c/p>\u003ca id=\"h3\" name=\"h3\">\u003c/a>\u003ch2>2 materials and methods\u003c/h2>\u003ch3>2.1 characteristics of walnut leaf brown spot and classification of disease severity levels\u003c/h3>\u003cp class=\"mb0\">in the local standard technical regulations for prevention and control of walnut brown spot (\u003ca href=\"#b19\">mcgranahan and leslie, 1991\u003c/a>), walnut brown spot is divided into four severity levels. the severity of plant leaf diseases is generally determined using the spot coverage method. in this study, walnut leaves with different disease severities were processed to separate disease-infected spots from healthy leaf areas, and the percentage of lesion area to total leaf area was calculated. with reference to the local standard, the disease grades specified in the standard were re-determined through calculations in this study, as shown in \u003ca href=\"#t1\">table&#xa0;1\u003c/a>. classification of different severity levels of walnut leaf brown spot was conducted in accordance with technical regulations for prevention and control of walnut brown spot (\u003ca href=\"#b33\">wang et&#xa0;al., 2022\u003c/a>; \u003ca href=\"#b37\">yang et&#xa0;al., 2021\u003c/a>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;1\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" name=\"table&#xa0;1\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" alt=\"www.frontiersin.org\" id=\"t1\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;1\u003c/b>. walnut leaf brown spot disease severity grading criteria.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">walnut leaf brown spot is caused by infection with the fungus \u003ci>ophiognomonia leptostyla\u003c/i> (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>). in the early infection stage, near-circular or irregular small spots appear on the leaves, with a gray-brown center and dark yellow-green to purplish-brown edges, and diseased leaves tend to fall off prematurely. in the middle stage, elongated elliptical or irregular slightly sunken dark brown lesions form, with larger spot size and light brown edges; longitudinal cracks are often present in the center of the lesions. in the late stage, lesions often coalesce to form large scorched necrotic areas, surrounded by yellow to golden-yellow zones, and small black granules (conidiomata and conidia of the pathogen) are scattered on the surface of the diseased tissue (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>). according to the degree of color and texture feature changes in infected leaves, walnut leaf disease is divided into four stages: healthy, early, middle, and late stages, corresponding to disease severity levels: healthy (level 0), mild (level 1), moderate (level 2), and severe (level 3). the different severity levels of walnut leaf brown spot are shown in \u003ca href=\"#f1\">figure&#xa0;1\u003c/a>.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;1\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" alt=\"four leaves are displayed, labeled a to d. a shows a healthy leaf with no spots. b features a mildly infected leaf with a few small spots. c presents a moderately infected leaf with numerous spots. d shows a severely infected leaf covered with dark spots.\" id=\"f1\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;1\u003c/b>. walnut leaves infected with brown spot disease at different severity levels. \u003cb>(a)\u003c/b> healthy leaves; \u003cb>(b)\u003c/b> mildly infected leaves; \u003cb>(c)\u003c/b> moderately infected leaves; \u003cb>(d)\u003c/b> severely infected leaves.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">\u003ca href=\"#t1\">table&#xa0;1\u003c/a> shows the ratio of walnut leaf brown spot lesion area to total leaf area. level 0 (healthy): healthy leaves without symptoms. level 1 (mild): near-circular or irregular small white spots appear on leaves, accounting for less than 5% of the leaf area. level 2 (moderate): lesions expand into irregular dark brown spots, covering 5% to 30% of the leaf area. level 3 (severe): abundant lesion coalesce to form large scorched necrotic areas, exceeding 30% of the leaf area.\u003c/p>\u003ch3>2.2 calculation algorithm for walnut leaf brown spot disease severity levels\u003c/h3>\u003cp class=\"mb0\">after capturing walnut leaf brown spot samples using a camera, to minimize subjective bias, this study strictly adhered to the objective quantitative criteria defined in the technical regulations for the control of walnut brown leaf spot (\u003ca href=\"#t1\">table&#xa0;1\u003c/a>), utilizing the lesion area percentage (k) as the primary grading indicator. we re-determined the severity levels through computational analysis, with the specific determination process shown in \u003ca href=\"#f2\">figure&#xa0;2\u003c/a>.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;2\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" alt=\"flowchart illustrating a process for identifying disease areas on leaves. images of leaves are converted through morphological color space to grayscale. the grayscale is used to highlight disease area s1 and leaf area s2. the ratio s1/s2 is applied, producing a final analysis image showing leaf disease marked in red.\" id=\"f2\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;2\u003c/b>. computational method for different severity levels of brown spot infection in walnut leaves.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">first, a python-based color segmentation algorithm was employed to convert images from the bgr color space to the hsv color space for processing. the hsv color space offers inherent advantages in color segmentation tasks. by analyzing the hue, saturation, and value characteristics of diseased regions, the color threshold range in the hsv space was determined (\u003ca href=\"#b4\">chaudhary et&#xa0;al., 2012\u003c/a>). after generating an initial disease region mask based on this threshold range, morphological operations such as closing and opening were applied to optimize mask quality, effectively eliminating noise while preserving the integrity of lesion contours. subsequently, the original image was converted to grayscale mode, and the leaf region was extracted using an adaptive threshold binarization algorithm. leaf boundaries were localized via contour detection technology, and a complete leaf mask was generated through contour filling. pixel statistical analysis was performed on both the leaf mask and disease mask to calculate the total leaf area and diseased region area, respectively. when a valid leaf area (&gt;0) was detected, the disease severity index was calculated using the following formula: disease severity index \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im1\">\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmo mathsize=\"10.5pt\">%\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">s\u003c/mi>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003c/mrow>\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">s\u003c/mi>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmn mathsize=\"10.5pt\">100\u003c/mn>\u003cmo mathsize=\"10.5pt\">%\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;&#xa0;&#xa0;s\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>: total area of diseased regions \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im2\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>: total area of the complete leaf.\u003c/p>\u003ch3>2.3 dataset construction\u003c/h3>\u003cp class=\"mb0\">data were collected from the walnut plantation base of tarim university (alar, xinjiang) between may and october 2024. leaves from three locally dominant early-fruiting cultivars (&#x2018;wen 185&#x2019;, &#x2018;xinxin 2&#x2019;, &#x2018;zha 343&#x2019;)widely cultivated in southern xinjiang were vertically photographed using an iphone 13. this genotypic diversity ensures model generalizability by encompassing varied disease phenotypes. the orchard follows standardized cultivation with 4m plant spacing, 5m row spacing (&#x2248;50 plants/mu), and conventional management practices. sampled trees were in full fruiting stage. to capture disease traits across microenvironments, samples were collected from safely accessible crown layers. the dataset includes healthy and brown spot-infected leaves three severity levels, spanning varied time periods, light conditions, and angles. after removing duplicates and invalid images,5,120 high-quality jpgs were retained for analysis.\u003c/p>\u003ch4>2.3.1 development of the walnut leaf brown spot dataset\u003c/h4>\u003cp class=\"mb0\">disease severity grading was established through quantitative lesion area analysis (\u003ca href=\"#f2\">figure&#xa0;2\u003c/a>). a representative subset of 512 images (10% of the full 5,120-image dataset) was selected via stratified random sampling, accurately preserving the original severity distribution. to ensure labeling rigor, two plant protection specialists (&gt;5 years&#x2019; expertise in walnut pathology, certified in local protocols) executed standardized annotation: pre-annotation training unified diagnostic criteria for ambiguous cases, with formal annotation commencing only after achieving pre-calibration inter-rater agreement (kappa coefficient &#x2265;0.75). as validated in \u003ca href=\"#t2\">table&#xa0;2\u003c/a>, dual statistical metrics confirmed gold-standard reliability&#x2014;inter-rater cohen&#x2019;s kappa reached 0.71, meeting landis &amp; koch&#x2019;s &#x201c;substantial agreement&#x201d; threshold; algorithm-expert consensus attained a weighted fleiss&#x2019; kappa of 0.77, substantially outperforming conventional cohen&#x2019;s kappa in multi-annotator scenarios. a three-stage standardization pipeline eliminated preprocessing discrepancies. the final dataset, partitioned into training/testing sets (8:2 ratio), strictly adheres to plant disease survey protocols and provides benchmarked data for deep learning model development (\u003ca href=\"#t3\">table&#xa0;3\u003c/a>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;2\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" name=\"table&#xa0;2\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" alt=\"www.frontiersin.org\" id=\"t2\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;2\u003c/b>. algorithm vs. expert consensus agreement evaluation.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;3\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" name=\"table&#xa0;3\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" alt=\"www.frontiersin.org\" id=\"t3\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;3\u003c/b>. walnut leaf brown spot disease image acquisition data.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>2.4 construction of walnut leaf brown spot disease severity grading model\u003c/h3>\u003ch4>2.4.1 the optimized mobilevitv3 network: cogfuse-mobilevit\u003c/h4>\u003cp class=\"mb15\">to address the challenge of difficult feature capture for small lesions in walnut leaf brown spot severity grading, this study proposes an innovative model, cogfuse-mobilevit. mobilevitv3 was selected as the backbone network due to its suitability for walnut brown spot grading. its hybrid mobilenet-vit architecture integrates cnn-based local feature extraction essential for lesion detail capture with transformer-enabled global contextual modeling critical for analyzing lesion spatial distribution. this design aligns with pathological requirements for severity grading, necessitating concurrent attention to local lesion characteristics and global infection patterns. the lightweight architecture further supports real-time processing on embedded devices, enabling future field deployment.\u003c/p>\u003cp class=\"mb0\">the model embodies a &#x201c;grading task-driven&#x201d; design principle. conceptually, it establishes a disease-specific framework of &#x201c;hierarchical screening to edge enhancement to dynamic fusion.&#x201d; this framework elevates general feature extraction to targeted solutions addressing three major agricultural challenges: suppressing interference from healthy tissues via hierarchical screening resolving feature confusion; strengthening blurred lesion contours through edge enhancement overcoming the bottleneck of edge blurriness; adaptively handling multi-stage morphological variations using dynamic fusion addressing morphological variation challenges. structurally, as illustrated in \u003ca href=\"#f3\">figure&#xa0;3\u003c/a>, the network implements a progressive feature extraction strategy. the hierarchical feature selection module (hfsm) first fuses shallow and middle-layer features. it employs a hierarchical attention mechanism to enhance semantic consistency while preserving spatial details. the edge convolution fusion module (ecfm) then processes these features. it utilizes learnable sobel operators to extract edge information, which is fused with conventional convolutional features via residual connections to augment edge perception capability. finally, the adaptive multi-scale dilated inception convolution module (amsddicm) enables adaptive processing of multi-scale edge features. this module is capable of both capturing fine edge changes in minute lesions and grasping the overall contour structure of large lesions, thereby comprehensively covering edge features across different developmental stages. based on these enhanced edge features, the network accurately outputs the final disease severity grade.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;3\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" alt=\"diagram illustrating a deep learning process for leaf classification. input images of leaves undergo convolution and mobilevit blocks, reducing dimensions and processing through hfsm, ecfm, and amsddicm modules. outputs are pooled and linearly transformed to logits.\" id=\"f3\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;3\u003c/b>. cogfuse-mobilevit architecture diagram (hfsm suppresses healthy regions through dynamic masks, ecfm explicitly extracts edges via sobel convolution, and amsddicm fuses multi-scale features through dynamic weights).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>2.4.2 the hierarchical feature selection module\u003c/h4>\u003cp class=\"mb0\">the hfsm (hierarchical feature selection module) is an innovative architecture proposed in this study. unlike mobilevit&#x2019;s direct feature concatenation, hfsm utilizes learnable prompt vectors (\u003ca href=\"#eq1\">equation 1\u003c/a>) to generate spatial masks, dynamically suppressing non-lesion regions. such task-driven selection is crucial for tiny objects. in the grading task of walnut leaf brown spot disease, this module utilizes a local attention mechanism to focus on micro-regions of walnut leaves, fusing shallow and middle-layer features. meanwhile, the hierarchical feature selection mechanism enables precise capture of local detail features such as lesion texture and color, effectively suppressing interference from healthy leaf regions and enhancing disease features. this provides high-discriminative feature representations for brown spot disease grading while preparing for subsequent lesion edge processing. as depicted in \u003ca href=\"#f4\">figure&#xa0;4\u003c/a>, the module processes two hierarchical feature maps. initial 1&#xd7;1 convolutions reduce both maps&#x2019; channels to half the output dimension curtailing computational load. one reduced map undergoes bilinear upsampling for spatial alignment with the other. these aligned features are summed and processed by a 3&#xd7;3 convolution to generate base-path features. simultaneously, the original reduced map and upsampled map are fed into dual local-global attention modules for parallel processing (\u003ca href=\"#b36\">xu, 2024\u003c/a>). each group contains local and global branches. during branch processing, the feature map is partitioned into p&#xd7;p non-overlapping patches \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im3\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">p\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmi mathsize=\"10.5pt\">j\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math> via the unfold operation. after calculating the mean of pixel features within each patch, the result is processed using the following core formula: attention distribution generation.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;4\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" alt=\"diagram illustrating a neural network architecture with multiple components. the upper section shows two convolution layers, lga modules, concatenation, and rep blocks. the lower section features processes like unfold, mean calculation, softmax, and feature selection, highlighting token and channel selection. various operations and components like multiplication, addition, and dropout are labeled, with symbols explaining their functions.\" id=\"f4\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;4\u003c/b>. architecture diagram of the hierarchical feature selection module (hfsm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">z\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ml\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">layernor\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ml\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">mean\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>1\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">a\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">softmax\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">z\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>2\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq1\">equations 1\u003c/a> and \u003ca href=\"#eq2\">2\u003c/a> convert the mean value of patch features into a high-dimensional feature vector through multi-layer perceptrons (mlp) and layer normalization (layernorm) (wherein(\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im4\">\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">mlp\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;and&#xa0;&#xa0;mlp\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>) \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im5\">\u003cmrow>\u003cmsup>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">achieve&#xa0;dimensional&#xa0;transformation&#xa0;p\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msup>\u003cmo mathsize=\"10.5pt\">&#x2192;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ouc\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmo mathsize=\"10.5pt\">&#x2192;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ouc\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/mrow>\u003c/math>)), and then generates the attention distribution via the softmax function \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im6\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">a\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmi mathsize=\"10.5pt\">j\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>. this mechanism enables the model to focus on the key features of lesion regions, weaken the interference from healthy leaf areas, enhance the specificity of feature selection, and provide high-quality features for subsequent precise grading. after the local and global features output by the two modules are spliced along the channel dimension, they are combined with the basic path features to generate the output features of the final processing module.\u003c/p>\u003cp class=\"mb0\">for leaf images with over 80% healthy tissue, dynamic masks are generated via learnable prompt vectors to suppress green texture features while enhancing the gray-brown features of lesions (\u003ca href=\"#b19\">mcgranahan and leslie, 1991\u003c/a>).\u003c/p>\u003ch4>2.4.3 ecfm edge convolutional fusion module\u003c/h4>\u003cp class=\"mb0\">in the walnut leaf brown spot grading task, lesion edge features are crucial for accurately determining disease severity. the standard mobilevit relies on cnn-transformer blocks to implicitly learn edges, whereas the ecfm (edge convolution fusion module) incorporates sobel convolutions, conventional convolutions, and residual connections, effectively fusing edge features with general features and thereby enhancing edge details. as shown in \u003ca href=\"#f5\">figure&#xa0;5\u003c/a>, the sobel branch employs sobel convolution to extract image edge information, accurately capturing the contour details of brown spot lesions; the convolutional branch captures general features such as leaf color and texture through standard convolution. the two realize feature fusion via the following core formula (\u003ca href=\"#eq3\">equation 3\u003c/a>):\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;5\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" alt=\"diagram showing a network flow. input x is split into two paths: one goes through a sobel filter and the other through a convolution (conv) layer. outputs are concatenated (concat) and followed by another conv layer. the result is summed with a bypass connection and passed through a final conv layer.\" id=\"f5\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;5\u003c/b>. architecture diagram of the edge convolutional fusion module (ecfm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">sobelconv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>3\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">processing the input feature map \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im7\">\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmi mathsize=\"10.5pt\">x\u003c/mi>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> through the sobel convolution operator (sobelconv) specifically extracts edge information of lesions (such as the boundary contours between lesions and healthy tissues, and the edge textures of small lesions). for the commonly seen blurred edges in brown spot disease, sobel convolution can enhance edge gradient changes, making the originally blurred lesion contours clearer, thus providing key edge feature support for subsequent grading. conventional feature extraction and fusion (\u003ca href=\"#eq4\">equation 4\u003c/a>):\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>4\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im8\">\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;c\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> extracting the overall features of leaves (such as the color distribution of lesions and the overall texture of leaves) through standard convolution forms a complement to edge features, preventing the model from focusing only on local edges while ignoring global lesion information.\u003c/p>\u003cp class=\"mb15\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im9\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> concatenating the edge feature s and the conventional feature c along the channel dimension achieves the initial fusion of edge details and global features. this fusion enables the model to both identify the fine contours of lesions and judge the disease condition by combining the overall color and texture changes of lesions, thereby improving the accuracy of grading. the module also introduces feature addition and subsequent convolution operations: the fused features are first processed by the first convolution layer \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im10\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math>, the corresponding operation results are added to the original input, and the final output is generated through the second convolution layer \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im11\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">final\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math>. in this process, the residual connection retains the original feature information, further strengthens the integration of edge features and conventional features, and ensures the effective transmission and enhancement of multi-level features.\u003c/p>\u003cp class=\"mb0\">the edge gradient information extracted by sobel convolution (such as grayscale differences between diseased spots and healthy tissues) and the texture features from conventional convolution (such as the roughness of necrotic areas) are fused via residual connections. this not only preserves the blurred edges of early-stage lesions but also prevents edge features from being disconnected from the overall texture (\u003ca href=\"#b8\">ghaiwat and arora, 2014\u003c/a>).\u003c/p>\u003ch4>2.4.4 amsddicm adaptive multi-scale dilated depthwise inseparable convolution module\u003c/h4>\u003cp class=\"mb0\">this module differs from mobilevit&#x2019;s single-scale convolution, leverages multi-shaped convolution kernels to accurately extract lesion features of circular, irregular, and other shapes from multiple dimensions, enabling the identification of subtle differences in lesion edges and internal colors to enrich feature dimensions. meanwhile, depth wise separable convolutions are employed to accommodate lesion scales at different stages of disease development. specifically, the input features are first split into two groups, each entering the amsddicm module to extract features via multi-scale depth wise separable convolutions. a dynamic weighting mechanism (global pooling + convolution + softmax) is used to generate weights for fusing multi-scale features. the processed features from the two groups are concatenated, and finally, cross-channel fusion is completed via 1&#xd7;1 convolution to output the final optimized features. the entire process integrates multi-scale convolution, dynamic weight allocation, and feature fusion to enhance the model&#x2019;s ability to capture complex features, as shown in \u003ca href=\"#f6\">figure&#xa0;6\u003c/a>. in response to the morphological differences between early-stage small spots (1-3mm in diameter) and late-stage fused spots (over 20mm in diameter), the dynamic weight mechanism enables the model to adaptively allocate the contributions of 3&#xd7;3 kernels for capturing local textures and 11&#xd7;1 kernels for extracting strip-shaped spreading features. this addresses the limitation of fixed weights in traditional inception architectures in adapting to multi-scale morphologies.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;6\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" alt=\"diagram showing a neural network process. the top section involves splitting input channels, applying a dms 2d convolution, concatenation, and another convolution. the bottom section includes a pooling layer, convolution, rearrangement, softmax, summation, batch normalization, and activation. the process results in output \\(x^{''}\\).\" id=\"f6\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;6\u003c/b>. architecture diagram of the adaptive multi-scale dilated depth wise inseparable convolution module (amsddicm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">when the input feature tensor x with the number of channels c enters the amsddicm module, it first undergoes a feature grouping operation: the input features are evenly split into two groups along the channel dimension, with each group containing half of the original number of channels (c//2). this grouping strategy not only reduces computational complexity but also creates conditions for subsequent multi-scale feature extraction. each feature group is fed into an independent dmsconv2d module, which adopts different configurations such as 3&#xd7;3, 5&#xd7;5 square convolution kernels and 1&#xd7;11, 11&#xd7;1 strip-shaped convolution kernels &#x2014; the square kernels capture local textures, while the strip-shaped kernels extract direction-sensitive long-range dependencies. this dynamic weight allocation draws inspiration from adaptive strategies in agricultural iot systems. similar to how salinity-aware ets models prioritize soil conductivity features under salt stress (\u003ca href=\"#b40\">zhang et&#xa0;al., 2023\u003c/a>), our mechanism selectively amplifies lesion morphological features critical for severity grading. the dynamic weighting mechanism of dmsconv2d generates weights through global average pooling and 1&#xd7;1 convolution, with the core formulas as follows:\u003c/p>\u003cp class=\"mb15\">weighted basic feature generation:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">rearrange\u003c/mtext>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003c/msub>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">avgpoo\u003c/mtext>\u003cmn mathsize=\"10.5pt\">l2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">d\u003c/mtext>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>5\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq5\">equation 5\u003c/a> compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information (such as the overall distribution characteristics of lesions at different scales), and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmo mathsize=\"10.5pt\">.\u003c/mo>\u003cmtext mathsize=\"10.5pt\">softmax\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>6\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq6\">equation 6\u003c/a> compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information, such as the overall distribution characteristics of lesions at different scales, and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">0\u003c/mn>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msubsup>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">dwcon\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">v\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>7\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb0\">\u003ca href=\"#eq7\">equation 7\u003c/a> is based on dynamic weights \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im12\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>) for different convolution kernels \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im13\">\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;dwconv\u003c/mtext>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> weighted fusion is performed on the extracted features. for walnut brown spot lesions exhibiting size heterogeneity and morphological complexity&#x2014;ranging from early-stage circular micro-lesions to late-stage coalesced irregular lesions&#x2014;this mechanism adaptively modulates multi-scale feature contributions. it prioritizes retention of morphology-discriminative features, local textures in small lesions or directional distributions in large lesions, thereby comprehensively capturing pathological characteristics across developmental stages. the processed features undergo channel-wise concatenation to generate optimized outputs (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>).\u003c/p>\u003ch3>2.5 experimental process for severity grading of walnut leaf brown spot disease\u003c/h3>\u003cp class=\"mb0\">\u003ca href=\"#f7\">figure&#xa0;7\u003c/a> is the overall flow chart of severity grading of walnut leaf spot disease. first is the image acquisition stage, where raw images of walnut leaves are captured and collected to construct an image database containing pictures to be classified (\u003ca href=\"#b29\">singh et&#xa0;al., 2019\u003c/a>). next is the image preprocessing stage, which involves sequentially calculating and classifying disease severity levels, establishing image labels, and performing image preprocessing operations. subsequently, the processed data are used to train the constructed dataset. finally, in the model training and performance evaluation stage, the preprocessed images are input into the model for classifying walnut leaf brown spot disease, after which performance evaluation is conducted on the model&#x2019;s classification results to determine the model&#x2019;s effectiveness and accuracy.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;7\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" alt=\"flowchart illustrating a process for walnut leaf brown spot disease classification. it consists of three main phases: image acquisition, preprocessing, and model training and evaluation. the image acquisition phase involves collecting walnut leaf images and storing them in a database. the preprocessing phase includes classification of disease grades, establishing image labels, and training the dataset. finally, the model classifies the disease, resulting in output images and a performance evaluation matrix for accuracy verification. the chart uses labeled boxes, arrows, and image samples for illustration.\" id=\"f7\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;7\u003c/b>. overall flow chart for severity grading of walnut leaf brown spot disease.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>2.6 experimental parameters and evaluation metrics\u003c/h3>\u003ch4>2.6.1 test environment and hyperparameter setting\u003c/h4>\u003cp class=\"mb15\">the experimental setup of this study is based on deep learning technology and leverages high-performance computing resources. all experiments were conducted on the windows 10 operating system. the hardware platform consists of an amd ryzen 7 3700x processor and an nvidia rtx 2080ti graphics card. the software environment was built using python 3.8, the pytorch 1.13.0 deep learning framework, and the cuda 11.3 parallel computing platform. additionally, pytorch 1.13.0&#x2014;a widely adopted open-source deep learning library renowned for its high flexibility&#x2014;was selected, making it well-suited for research and development.\u003c/p>\u003cp class=\"mb0\">during model training, multiple adjustments were made to the hyperparameters to compare test results and select the optimal hyperparameter combination. the model accepts input images sized at 224&#xd7;224 pixels, with a configured batch size of 32 during training and a maximum of 100 training epochs. the optimizer employs an initial learning rate of 0.003.\u003c/p>\u003ch4>2.6.2 evaluation metrics\u003c/h4>\u003cp class=\"mb15\">this paper aims to evaluate the performance of the model and verify the effectiveness of the improvement measures. we selected multiple evaluation metrics, and all subsequent experimental results adopt the method of calculating averages via 5-fold cross-validation, aiming to comprehensively and reliably evaluate the model performance. including precision (p), recall (r), etc. (\u003ca href=\"#eq8\">equations 8\u003c/a>&#x2013;\u003ca href=\"#eq16\">16\u003c/a>) these metrics can be calculated using the following formulas.\u003c/p>\u003cp class=\"mb15\">the arithmetic mean of the metric values across the five folds:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmover accent=\"true\">\u003cmtext mathsize=\"10.5pt\">m\u003c/mtext>\u003cmo mathsize=\"10.5pt\">&#xaf;\u003c/mo>\u003c/mover>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmn mathsize=\"10.5pt\">5\u003c/mn>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">5\u003c/mn>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">m\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>8\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">precision\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>9\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">recall\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fn\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>10\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">score\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003c/mrow>\u003cmrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fn\u003c/mtext>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>11\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">macro&#xa0;precision=\u003c/mtext>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>12\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">macro&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">r\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>13\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;precision=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&#xb7;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">precisio\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>14\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&#xb7;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">precisio\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>15\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmsubsup>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003cmtext mathsize=\"10.5pt\">t\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>16\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003ca id=\"h4\" name=\"h4\">\u003c/a>\u003ch2>3 experiments and results analysis\u003c/h2>\u003ch3>3.1 core module design and validity experimental verification\u003c/h3>\u003ch4>3.1.1 comparative test of necessity of hfsm module\u003c/h4>\u003cp class=\"mb0\">to validate the necessity of the hierarchical feature selection module (hfsm) within the cogfuse-mobilevit framework, this study conducted comparative tests against two mainstream lightweight attention modules: se (squeeze-and-excitation) and cbam (convolutional block attention module). as illustrated in \u003ca href=\"#t4\">table&#xa0;4\u003c/a>, the hfsm module achieves a significantly higher accuracy of 86.61%, outperforming the se module and cbam module by 7.38% and 4.46%, respectively. this demonstrates that its hierarchical feature selection mechanism more effectively captures discriminative features. in terms of computational efficiency, the parameters and flops of hfsm remain comparable with those of the se module and cbam module, indicating that its performance breakthrough stems from innovative structural design under equivalent lightweight constraints. comprehensive results confirm that replacing hfsm with se or cbam modules would incur a performance degradation exceeding 4%, thereby validating the indispensable value of hfsm in enabling efficient feature selection within the cogfuse-mobilevit framework.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;4\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" name=\"table&#xa0;4\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" alt=\"www.frontiersin.org\" id=\"t4\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;4\u003c/b>. performance comparison of attention modules within the cogfuse-mobilevit framework.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.2 convolutional kernel selection in amsddicm\u003c/h4>\u003cp class=\"mb0\">rigorous validation via kernel combination ablation studies (\u003ca href=\"#t5\">table&#xa0;5\u003c/a>) demonstrates. the hybrid configuration (3&#xd7;3 \u003cb>+\u003c/b> 5&#xd7;5 \u003cb>+\u003c/b> 1&#xd7;11) significantly outperformed square-only kernels (3&#xd7;3 \u003cb>+\u003c/b> 5&#xd7;5) accuracy 86.61% and 82.15% stage-specific recall gains. mid-stage lesions (level 2) raise 9.4 percentage points. late-stage lesions (level 3) raise 6.1 percentage points. this empirically validates the necessity of 1&#xd7;11 rectangular kernels for capturing linear pathological features, overcoming the limitation of isotropic kernels in detecting anisotropic structures.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;5\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" name=\"table&#xa0;5\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" alt=\"www.frontiersin.org\" id=\"t5\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;5\u003c/b>. comparison of different convolution kernels in amsddicm.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.3 the impact of new modules on computational complexity\u003c/h4>\u003cp class=\"mb0\">\u003ca href=\"#f8\">figure&#xa0;8\u003c/a> shows that the hfsm module incurs a 169% flops increase to achieve a breakthrough improvement in early-stage lesion detection&#x2014;reducing level 0/1 misclassification by 24%, thereby establishing the pathological foundation for severity grading. the ecfm module contributes a mere 3% flops increment yet drives a 3.68 percentage point accuracy gain through enhanced edge feature representation. with only a 0.028g flops overhead 1.3%, the amsddicm module enables adaptive fusion of multiscale pathological deformations, culminating in a 7.80-pp accuracy leap. these modules form a cascaded optimization paradigm: hfsm&#x2019;s substantial cost resolves the core pathological bottleneck, while subsequent modules deliver superlinear returns&#x2014;harvesting 6.85-pp accuracy gain with just 14% additional flops&#x2014;collectively establishing the globally optimal computation-performance equilibrium.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;8\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" alt=\"bar graphs comparing different configurations of mobilevit models across three metrics. panel a shows accuracy percentages, with values from 78.81% to 86.61%. panel b displays flops, ranging from 0.74g to 2.1g. panel c illustrates parameters in millions, with values between 1.94m and 2.02m. each graph compares mobilevit, mobilevit plus hfsm, mobilevit plus hfsm plus ecfm, and cogfuse-mobilevit.\" id=\"f8\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;8\u003c/b>. comparison of the impact of each newly added module on computational complexity \u003cb>(a)\u003c/b> accuracy (%); \u003cb>(b)\u003c/b> flops (g); \u003cb>(c)\u003c/b> params (m). measured on nvidia rtx 2080ti gpu (pytorch 1.13.0, cuda 11.3) with 224&#xd7;224 input.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.4 comparison of the influence of different module fusion on model performance\u003c/h4>\u003cp class=\"mb0\">to validate the effect of module fusion, \u003ca href=\"#t6\">table&#xa0;6\u003c/a> compares model performances with different combinations. when only hfsm (hierarchical feature selection module) is introduced, precision increases from the baseline of 82.23% to 83.77%, but recall decreases by 3 percentage points to 72.31%, causing the f1 score to slightly decline to 78.04%. accuracy rises to 82.15%, indicating improved overall classification correctness of the model, but with potential risk of missed detections.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;6\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" name=\"table&#xa0;6\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" alt=\"www.frontiersin.org\" id=\"t6\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;6\u003c/b>. impact of fusion of different modules on model performance.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">when hfsm and ecfm (edge-context feature module) are synergistically introduced, precision increases to 86.34%, recall recovers to 74.69%, and both f1 score and accuracy are significantly optimized. hfsm fuses shallow and middle-layer features through a hierarchical attention mechanism, laying the foundation for subsequent edge feature processing; ecfm enhances lesion edge features. the combination of the two effectively improves the integrity of feature representation and the clarity of lesion boundaries.\u003c/p>\u003cp class=\"mb15\">the combination of hfsm and amsddicm (adaptive multi-scale dilated depthwise inseparable convolution module) further pushes recall to 76.24%, accuracy to 83.94%, and f1 score to 80.79%, outperforming the hfsm+ecfm combination. amsddicm compensates for hfsm&#x2019;s deficiency in local feature refinement through attention-guided multi-scale detail fusion, which integrates multi-layer detail feature weights, especially suitable for scenarios with small targets or blurred features.\u003c/p>\u003cp class=\"mb0\">when the three modules work synergistically, all indicators reach optimal levels: precision, recall, f1 score, and accuracy increase by 12.19%, 7.67%, 9.62%, and 10.00% respectively compared with the baseline. among them, hfsm lays the foundation for cross-layer feature fusion, ecfm effectively integrates edge-specific features with general features to enhance image edge information, and amsddicm adaptively fuses multi-scale and multi-type features through an attention mechanism, forming a progressive optimization chain from &#x201c;hierarchical feature extraction&#x201d; to &#x201c;edge semantic enhancement&#x201d; and then to &#x201c;multi-layer detail fusion&#x201d;. the experimental results show a balanced improvement in both precision and recall, indicating that the fusion of the three modules enables the model to focus more on learning the features of small brown spot lesions, thereby improving its classification performance. this synergistic task-specific design, combining hierarchical selection, explicit edge enhancement, and adaptive multi-scale fusion, fundamentally distinguishes cogfuse-mobilevit from prior mobilevit hybrids designed for general vision or localization tasks.\u003c/p>\u003ch4>3.1.5 influence of different module combinations on f1-score of level (0-3)\u003c/h4>\u003cp class=\"mb0\">in order to verify the targeted improvement of each module for a specific disease level, we conducted the following comparative experiments as shown in the \u003ca href=\"#t6\">table&#xa0;6\u003c/a>, when hfsm is introduced alone, the f1-score of level 0 increases from 83.10% to 85.60%, effectively reducing feature confusion between healthy leaves and early-stage lesions. after adding ecfm, the f1-score of level 1 rises from 73.20% (with hfsm alone) to 79.80%, significantly mitigating the recognition bias caused by blurred edges of small lesions. amsddicm increases the f1-score of level 3 from 80.37% to 85.54%, enhancing the adaptability to the morphology of large-scale fused scorched areas. with the collaboration of the three modules, the f1-scores of level 0&#x2013;3 reach 93.99%, 82.57%, 84.28%, and 85.54% respectively, achieving balanced optimization of performance across all levels.\u003c/p>\u003ch3>3.2 results comparison of different algorithms and statistical significance verification\u003c/h3>\u003ch4>3.2.1 comparison of grading results for different classification models\u003c/h4>\u003cp class=\"mb0\">to verify the effectiveness of the cogfuse-mobilevit model, we selected 9 commonly used classification models to compare with the optimized cogfuse-mobilevit model, and the results are the average of 5-fold cross-validation over 120 epochs. as shown in \u003ca href=\"#t7\">table&#xa0;7\u003c/a>, the proposed cogfuse-mobilevit model achieved the highest grading performance in terms of precision, recall, and f1-score for identifying walnut leaf brown spot disease at different severity levels among all compared models. the improved cogfuse-mobilevit model exhibits an accuracy of 86.61%, representing a 7.80-percentage-point improvement over the original model. overall, the experimental results highlight that the enhanced cogfuse-mobilevit model is more conducive to focusing on lesion edge details and accurately learning the features of different severity levels of walnut leaf brown spot disease, thereby improving the model&#x2019;s classification performance.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;7\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" name=\"table&#xa0;7\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" alt=\"www.frontiersin.org\" id=\"t7\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;7\u003c/b>. comparison of grading results for different classification models.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.2.2 comparison of performance and reliability validation of different algorithms\u003c/h4>\u003cp class=\"mb0\">in model training, to quantify the reliability of results and avoid random biases from single experiments, this study employs 5-fold cross-validation to generate 95% confidence intervals, as shown in the \u003ca href=\"#f9\">figure&#xa0;9\u003c/a>. the results demonstrate that cogfuse-mobilevit takes a significant lead with an accuracy of 86.61% (95% ci: [85.24%, 87.89%]). its narrowest confidence interval range indicates the strongest generalization capability. among the comparative models, mobilevitv3 has a 95% ci of [77.20%, 80.42%]; its lower bound is significantly lower than that of cogfuse-mobilevit, confirming that the 7.8% performance improvement is not due to random fluctuations. furthermore, the confidence intervals constructed through 5-fold cross-validation further highlight the reliability of the experimental results.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;9\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" alt=\"bar chart showing confidence intervals for accuracy rates of different classification models. models with higher accuracy include cogfuse-mobilevit at 86.61 and mobilevitv3 at 78.81. other models range from 73.63 to 68.72. each model is color-coded, with a corresponding legend on the right.\" id=\"f9\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;9\u003c/b>. confidence intervals of accuracy for different classification models in 5-fold cross-validation.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.2.3 statistical significance verification of model improvement\u003c/h4>\u003cp class=\"mb0\">to statistically evaluate the performance improvement of cogfuse-mobilevit over the baseline mobilevitv3, an independent samples t-test was conducted (\u003ca href=\"#t8\">table&#xa0;8\u003c/a>). for each model architecture, five independent training runs. the null hypothesis stated that the mean accuracy of cogfuse-mobilevit was equal to that of mobilevitv3 \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im14\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">h\u003c/mi>\u003cmn mathsize=\"10.5pt\">0\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">:\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">c\u003c/mi>\u003cmtext mathsize=\"10.5pt\">og\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">m\u003c/mi>\u003cmi mathsize=\"10.5pt\">o\u003c/mi>\u003cmi mathsize=\"10.5pt\">b\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>), while the alternative hypothesis stated that cogfuse-mobilevit had a higher mean accuracy (\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im15\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">h\u003c/mi>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">:\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">c\u003c/mi>\u003cmtext mathsize=\"10.5pt\">og\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&gt;\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">m\u003c/mi>\u003cmi mathsize=\"10.5pt\">o\u003c/mi>\u003cmi mathsize=\"10.5pt\">b\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>)cogfuse-mobilevit achieved a mean accuracy, significantly outperforming mobilevitv3. the independent samples t-test confirmed this improvement (t(8)=18.92,p=3.7&#xd7;10 \u003csup>&#x2212;8\u003c/sup>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;8\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" name=\"table&#xa0;8\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" alt=\"www.frontiersin.org\" id=\"t8\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;8\u003c/b>. comparison of accuracy results between mobilevitv3 and cogfuse-mobilevit using independent samples t-test.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">under 5-fold cross-validation (\u003ca href=\"#f10\">figure&#xa0;10\u003c/a>), cogfuse-mobilevit achieves rapid convergence within the first 20 epochs, demonstrating more efficient feature learning capability. upon entering the steady-state phase, its validation loss is reduced by over 5-fold compared to the baseline mobilevitv3, directly confirming smaller prediction errors and superior generalization capability. meanwhile, the smooth and minimally fluctuating loss curve of cogfuse-mobilevit reflects strong robustness against data noise and distribution variations. combined with the previous statistical findings from accuracy confidence intervals and independent t-tests, these results collectively validate the statistical significance of the improved model.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;10\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" alt=\"line graph showing validation loss over epochs for two models: cogfuse-mobilevit (blue) and mobilevitv3 (orange). loss decreases sharply initially, then stabilizes, with cogfuse-mobilevit exhibiting a consistently lower loss.\" id=\"f10\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;10\u003c/b>. comparison of loss curves between the cogfuse-mobilevit model and the original model in 5-fold cross-validation within 120 epochs.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.3 model result analysis performance comparison of different models\u003c/h3>\u003cp class=\"mb0\">\u003ca href=\"#f11\">figure&#xa0;11a\u003c/a> shows that in the walnut leaf brown spot disease severity grading task, the classification accuracy of all models gradually improved and stabilized with the increase of training epochs, indicating that the models continuously optimized during learning until convergence. cogfuse-mobilevit stood out prominently: it achieved rapid accuracy improvement, reached a high level in relatively early training epochs with minimal subsequent fluctuations, and stably maintained the highest accuracy, demonstrating fast convergence and strong generalization ability to efficiently extract features distinguishing different disease levels. densenet also maintained high accuracy in the late training stage, but its accuracy improvement was slower, with slightly inferior convergence speed and final stability compared to cogfuse-mobilevit. models like resnet and regnet showed limited accuracy gains with gentle upward trends, and their final stable accuracy values were significantly lower, reflecting insufficient feature extraction capabilities possibly due to network architecture or parameter optimization efficiency.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;11\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" alt=\"panel a shows a line graph comparing the accuracy of different neural networks over 100 epochs. cogfuse-mobilevit reaches the highest accuracy, achieving over 90 percent. panel b displays a line graph comparing loss over epochs. cogfuse-mobilevit has the lowest loss, declining swiftly to under 0.2. the legend lists networks like densenet, efficientnet, and mobilevitv3.\" id=\"f11\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;11\u003c/b>. comparison of accuracy and loss across different models \u003cb>(a)\u003c/b> accuracy line chart; \u003cb>(b)\u003c/b> loss line chart.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">in \u003ca href=\"#f11\">figure&#xa0;11b\u003c/a>, all models showed high initial loss values that dropped rapidly and then stabilized, following the typical learning process of iterative optimization. cogfuse-mobilevit achieved significant early loss decline and stabilized near the minimum, indicating efficient feature learning and strong fitting capability. while densenet also reached a relatively low loss level, it remained slightly higher than cogfuse-mobilevit. other models had higher late-stage loss values: some showed fast initial decline but converged to higher levels, indicating shortcomings in capturing key disease features.\u003c/p>\u003ch3>3.4 analysis of detection results for different classification models\u003c/h3>\u003cp class=\"mb0\">the confusion matrix is a key indicator for evaluating classification models: the higher the diagonal values, the higher the classification accuracy for the corresponding category, and the lower the off-diagonal values, the fewer misjudgments. \u003ca href=\"#f12\">figure&#xa0;12\u003c/a> shows the classification results of different models for the four disease grades (0&#x2013;3) of walnut leaf brown spot disease. it is clearly evident that the overall classification performance, particularly the accuracy of cogfuse-mobilevit across all categories, is remarkably high with minimal misjudgments, demonstrating that the model has enhanced discrimination ability for disease grades and performs optimally in distinguishing the four disease levels.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;12\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" alt=\"six normalized confusion matrices labeled a to f show predicted versus true values ranging from zero to three. each matrix illustrates classification performance with varying accuracy per class, indicated by different shades of red. brighter reds denote higher accuracy, while lighter shades indicate lower accuracy or misclassification.\" id=\"f12\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;12\u003c/b>. confusion matrices of the improved cogfuse-mobilevit model and traditional classification models \u003cb>(a)\u003c/b> densenet; \u003cb>(b)\u003c/b> efficientnet; \u003cb>(c)\u003c/b> efficientnetv2; \u003cb>(d)\u003c/b> swin transformer; \u003cb>(e)\u003c/b> mobilevitv3; \u003cb>(f)\u003c/b> cogfuse-mobilevit.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">the edge-enhanced feature module (ecfm) effectively addressed level 0 and level 1 misclassification by capturing incipient lesion features. baseline analysis revealed substantial false negatives for early-stage lesions, with level 1 and level 0 misclassification reaching 32%. ecfm reduced this rate to 8%, demonstrating its capacity to resolve texture confusion through enhanced edge contour delineation. similarly, the adaptive multi-scale dilated convolution module (amsddicm) significantly mitigated level 2 and level 3 mutual misclassification. amsddicm drastically reduced both errors by enhancing local fine-grained features in mid-stage lesions (level 2) while strengthening global fusion features in late-stage coalesced lesions (level 3). this resolves grading ambiguity arising from the morphological continuum of lesion progression.\u003c/p>\u003cp class=\"mb0\">to establish a comprehensive performance evaluation framework and quantify the model&#x2019;s overall discriminative capability, \u003ca href=\"#f13\">figure&#xa0;13\u003c/a> presents the receiver operating characteristic (roc) curves of cogfuse-mobilevit across four severity levels. these curves compare the true positive rate (tpr) against the false positive rate (fpr) by dynamically adjusting the classification threshold. the area under the curve (auc) serves as the primary performance metric, where a higher value indicates stronger classification ability. notably, the auc values for all categories significantly exceed the level of random guessing (auc=0.5), fully demonstrating that the model possesses robust discriminative capability across different severity levels.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;13\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" alt=\"receiver operating characteristic (roc) curve chart showing true positive rate versus false positive rate for four classes: class 0 (blue), class 1 (orange), class 2 (green), and class 3 (red), along with an average roc curve (dashed purple). the curves are above the diagonal, indicating good model performance.\" id=\"f13\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;13\u003c/b>. roc curves of the four different severity levels for cogfuse-mobilevit.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.5 tsne visualization of features extracted by different models\u003c/h3>\u003cp class=\"mb0\">the tsne visualization of features extracted from the models is shown in \u003ca href=\"#f14\">figures&#xa0;14a&#x2013;d\u003c/a>. this study analyzed the features of the original model at the 20th, 50th, and 100th training epochs, as well as the improved model at the 100th epoch. after 20 epochs of training, the original model showed a highly discrete feature distribution. limited by the number of training epochs, the model failed to fully learn discriminative features, resulting in insufficient distinction between categories and significant overlap among different classes. this indicates that under this training intensity, the original model&#x2019;s ability to capture meaningful patterns was relatively limited. when the original model was trained to 50 epochs, compared with (a), the feature aggregation significantly improved. however, inter-class overlap still existed, suggesting that although prolonged training aided feature learning, the original model&#x2019;s architecture had inherent defects in achieving clear feature separation. at 100 epochs of training, the original model exhibited more prominent feature clustering. nevertheless, some regions still lacked clear separation between different categories, indicating that even after prolonged training, the original model faced challenges in maximizing inter-class distance and intra-class compactness. in figure (d), the improved model after 100 epochs of training demonstrated a more superior feature distribution: each category was tightly clustered, achieving high intra-class compactness, while distinct boundaries between different categories were established, resulting in significant inter-class distance. this suggests that the model improvements effectively enhanced its ability to extract discriminative features, greatly reduced feature ambiguity, and improved feature discriminative power. compared with the original model, it showed stronger feature discrimination and optimization potential.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;14\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" alt=\"four scatter plots labeled a, b, c, and d illustrate data points grouped by colors representing levels zero to three. each plot displays varying cluster formations. plot a shows two distinct clusters. plot b has dispersed clusters, with one elongated group. plot c features overlapping clusters, and plot d displays four well-separated clusters.\" id=\"f14\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;14\u003c/b>. tsne visualization of features extracted by different models. after the three modules work synergistically, the boundaries of feature clustering between level 0 (healthy) and level 1 (early-stage) are significantly clearer, which verifies the effectiveness of edge-texture collaborative learning \u003cb>(a)\u003c/b> the original model was trained for 20 epochs; \u003cb>(b)\u003c/b> the original model was trained for 50 epochs; \u003cb>(c)\u003c/b> the original model was trained for 100 epochs; \u003cb>(d)\u003c/b> the improved model was trained for 100 epochs.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.6 radar chart for comparison of classification performance between original and improved models\u003c/h3>\u003cp class=\"mb0\">as shown in \u003ca href=\"#f15\">figure&#xa0;15\u003c/a>, a radar chart compares the performance of the original model and the improved cogfuse-mobilevit model across multiple evaluation metrics. in terms of accuracy, cogfuse-mobilevit exhibits significantly higher values than the original mobilevitv3 model, indicating that the improved model achieves overall higher classification accuracy. macro-average precision, which considers the average precision of each category, clearly shows that cogfuse-mobilevit also performs better, meaning it has superior precision across all categories. meanwhile, cogfuse-mobilevit also demonstrates better macro-average recall. when examining weighted-average precision and weighted-average recall&#x2014;metrics that account for class imbalance&#x2014;cogfuse-mobilevit outperforms mobilevitv3 in both, indicating that in practical applications, even with uneven class distribution, the cogfuse-mobilevit model can deliver better performance. these results further demonstrate the model&#x2019;s reliability and practicality.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;15\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" alt=\"radar chart comparing two models: mobilevitv3 (red) and cogfuse-mobilevit (blue). metrics include accuracy, macro average precision, macro average recall, weighted average precision, and weighted average recall. each axis ranges from fifty to one hundred.\" id=\"f15\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;15\u003c/b>. multi-metric comparison between mobilevitv3 and improved cogfuse-mobilevit model.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.7 public data set experiment\u003c/h3>\u003cp class=\"mb15\">to further validate the efficacy and generalizability of the proposed cogfuse-mobilevit model, experiments were conducted on the public dataset appleleaf9 (\u003ca href=\"#b38\">yang et&#xa0;al., 2022\u003c/a>). this dataset comprises healthy apple leaves and eight categories of apple leaf diseases captured in field environments without restrictions on imaging angles or noise interference. the dataset was partitioned into training and test sets at an 8:2 ratio. all images were resized to 224&#xd7;224 pixels to optimize deep learning model training efficiency. following the hyperparameter configurations specified in &#x201c;experimental parameters and evaluation metrics&#x201d;, both the baseline mobilevitv3 and cogfuse-mobilevit models were trained and evaluated.\u003c/p>\u003cp class=\"mb0\">cross-species validation demonstrates that the cogfuse-mobilevit model delivers exceptional performance on the appleleaf9 dataset. as presented in \u003ca href=\"#t9\">table&#xa0;9\u003c/a>, the model comprehensively surpasses the baseline mobilevitv3 across all four core metrics: precision increases by 0.16 percentage points, recall achieves a breakthrough improvement of 3.45 percentage points, f1-score rises by 1.18 percentage points, and accuracy elevates by 1.14 percentage points. the synergistic enhancement in both precision and recall signifies that performance gains stem from strengthened feature discriminability rather than metric trade-off compromises. the remarkable percentage points recall gain substantially mitigates leaf disease omission rates, while the holistic advance in f1-score further attests to the model&#x2019;s robustness. these results collectively validate the generalizability of cogfuse-mobilevit&#x2019;s core innovations in agricultural fine-grained disease grading, establishing a transferable paradigm for small-target pathology identification across plant species.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;9\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" name=\"table&#xa0;9\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" alt=\"www.frontiersin.org\" id=\"t9\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;9\u003c/b>. experimental results of the original model and cogfuse-mobilevit model on appleleaf9 data set.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ca id=\"h5\" name=\"h5\">\u003c/a>\u003ch2>4 conclusion\u003c/h2>\u003cp class=\"mb0\">this study addresses the challenges in precise grading of walnut leaf brown spot disease. by adopting dynamic feature filtering, edge gradient reinforcement, and multi-scale morphological adaptation, it effectively resolves three key limitations of existing hybrid architectures and the baseline model mobilevitv3 in plant disease grading weak capability in capturing blurred edges, poor multi-scale adaptability, and interference from healthy tissues. experimental results show that the model achieves an 86.61% grading accuracy on a dataset encompassing diverse lighting conditions, cultivars, and lesion stages, representing a 7.80% improvement over the baseline mobilevitv3 and outperforming nine state-of-the-art models. at the same time, this study confirmed that the performance improvement of cogfuse-mobilevit was statistically significant and stable through strict statistical tests, providing a reliable method for accurate classification of walnut leaf spot disease. the constructed image dataset and proposed grading re-measurement method lay the foundation for accuracy. this approach provides a new paradigm for small-target disease grading, with core modules transferable to multi-crop disease recognition. beyond disease classification, context-aware ai frameworks demonstrate significant efficacy in agricultural management (\u003ca href=\"#b1\">acharya et&#xa0;al., 2022\u003c/a>). iot systems dynamically adjust fertilization recommendations by integrating real-time soil-crop data, while salinity-corrected evapotranspiration (ets) models optimize irrigation strategies for saline-alkali soils (\u003ca href=\"#b15\">li et&#xa0;al., 2023\u003c/a>; \u003ca href=\"#b30\">su et&#xa0;al., 2023\u003c/a>). similarly, multimodal fusion techniques enhance reference evapotranspiration (eto) prediction accuracy, facilitating precision water allocation (\u003ca href=\"#b11\">jo et&#xa0;al., 2021\u003c/a>; \u003ca href=\"#b27\">rustia et&#xa0;al., 2023\u003c/a>). these breakthroughs collectively validate the robust capability of adaptive feature processing in complex agricultural environments&#x2014;a core principle that resonates with our dynamic weighting strategy for disease feature extraction. future research should therefore integrate cross-modal and cross-domain capabilities to establish multidimensional assessment systems and regional dynamic monitoring frameworks, thereby delivering comprehensive technical solutions for small-target disease control.\u003c/p>\u003ca id=\"h6\" name=\"h6\">\u003c/a>\u003ch2>5 discussion and future work\u003c/h2>\u003cp class=\"mb15\">compared with existing methods, this study overcomes the limitation of traditional deep learning models relying on fixed convolution kernels, enabling adaptive capture of lesion features with multi-shaped convolution kernels to accurately extract edge textures of circular micro-lesions and regional contours of irregular lesions. although the constructed multi-source dataset covers diverse lighting conditions, cultivars, and lesion development stages, the homogeneous internal features of severe diseases lead to limited recall rate. meanwhile, when lesions are severely overlapped or mixed with mechanical damage, insect damage, or other types of injuries, the discriminative accuracy of the model is affected. for ultra-small lesions, the local feature information is too weak and easily overlooked, and the computational efficiency still needs optimization on some hardware platforms. furthermore, the adaptability of the current model in extreme field scenarios, such as high-density occlusion and compound damage from pests and diseases, remains to be further verified. in these scenarios, the coupling of complex interference factors exacerbates feature confusion, affecting grading reliability.\u003c/p>\u003cp class=\"mb15\">to address these issues, future work will focus on the following research directions. future research will focus on enhancing the model&#x2019;s performance in complex field environments through multiple synergistic strategies. this includes constructing multi-interference factor coupled datasets that incorporating insect holes, lesions, and soil adhesion to strengthen robustness against extreme field disturbances; introducing attention-based multi-damage feature decoupling modules and dedicated micro-lesion enhancement modules combining super-resolution and feature interpolation to improve discriminability in complex scenarios and for tiny lesions;&#xa0;optimizing dynamic weight calculations via approximate computation or hardware-friendly reconstruction, while developing lightweight real-time deployment frameworks integrated with uav near-ground sensing technology to boost efficiency and practical monitoring coverage; and establishing cross-crop pathological transfer learning mechanisms, extending the model to multi-crop disease recognition tasks with self-supervised pre-training to enhance algorithm universality and low-contrast lesion feature mining capabilities. through the above research, it is expected to further improve the applicability and practicality of the model in complex field environments, promoting the development of intelligent plant disease grading technology towards more accurate, efficient, and universal directions.\u003c/p>\u003ca id=\"h7\" name=\"h7\">\u003c/a>\u003ch2>data availability statement\u003c/h2>\u003cp class=\"mb0\">the raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.\u003c/p>\u003ca id=\"h8\" name=\"h8\">\u003c/a>\u003ch2>author contributions\u003c/h2>\u003cp class=\"mb0\">yw: writing &#x2013; original draft. dz: writing &#x2013; original draft. lz:&#xa0;writing &#x2013; review &amp; editing.\u003c/p>\u003ca id=\"h9\" name=\"h9\">\u003c/a>\u003ch2>funding\u003c/h2>\u003cp class=\"mb0\">the author(s) declare financial support was received for the research and/or publication of this article. this work was supported in part by the science and technology key project of the xinjiang production and construction corps (2023ab063) development and application demonstration of green technology for online monitoring of fresh fruits and cold chain logistics in xinjiang.\u003c/p>\u003ca id=\"h10\" name=\"h10\">\u003c/a>\u003ch2>conflict of interest\u003c/h2>\u003cp class=\"mb0\">the authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\u003c/p>\u003ca id=\"h11\" name=\"h11\">\u003c/a>\u003ch2>generative ai statement\u003c/h2>\u003cp class=\"mb15\">the author(s) declare that no generative ai was used in the creation of this manuscript.\u003c/p>\u003cp class=\"mb0\">any alternative text (alt text) provided alongside figures in this article has been generated by frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. if you identify any issues, please contact us.\u003c/p>\u003ca id=\"h12\" name=\"h12\">\u003c/a>\u003ch2>publisher&#x2019;s note\u003c/h2>\u003cp class=\"mb15\">all claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.\u003c/p>\u003ca id=\"h13\" name=\"h13\">\u003c/a>\u003ch2>references\u003c/h2>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b1\" id=\"b1\">\u003c/a> acharya, p., burgers, t., and nguyen, k.-d. (2022). ai-enabled droplet detection and tracking for agricultural spraying systems. \u003ci>computers and electronics in agriculture\u003c/i>, 202, 107325. doi:&#xa0;10.1016/j.compag.2022.107325\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2022.107325\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=p.+acharya&amp;author=t.+burgers&amp;author=k.-d.+nguyen&amp;publication_year=2022&amp;title=ai-enabled%20droplet%20detection%20and%20tracking%20for%20agricultural%20spraying%20systems&amp;journal=computers+and+electronics+in+agriculture&amp;volume=202&amp;pages=107325\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b2\" id=\"b2\">\u003c/a> adaskaveg, j., f&#xf6;rster, h., thompson, d., driever, g., connell, j., buchner, r., et al. (2009). epidemiology and management of walnut blight. \u003ci>walnut res. rep\u003c/i>, 94, 225&#x2013;256. doi:&#xa0;10.1016/j.inpa.2016.10.005\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.inpa.2016.10.005\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.+adaskaveg&amp;author=h.+f%c3%b6rster&amp;author=d.+thompson&amp;author=g.+driever&amp;author=j.+connell&amp;author=r.+buchner&amp;publication_year=2009&amp;title=epidemiology%20and%20management%20of%20walnut%20blight&amp;journal=walnut+res.+rep&amp;volume=94&amp;pages=225\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b3\" id=\"b3\">\u003c/a> arivazhagan, s., shebiah, r. n., ananthi, s., and varthini, s. v. (2013). detection of unhealthy region of plant leaves and classification of plant leaf diseases using texture features. \u003ci>agricultural engineering international: cigr journal\u003c/i>, 15, 211&#x2013;217.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+arivazhagan&amp;author=r.%20n.+shebiah&amp;author=s.+ananthi&amp;author=s.%20v.+varthini&amp;publication_year=2013&amp;title=detection%20of%20unhealthy%20region%20of%20plant%20leaves%20and%20classification%20of%20plant%20leaf%20diseases%20using%20texture%20features&amp;journal=agricultural+engineering+international:+cigr+journal&amp;volume=15&amp;pages=211\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b4\" id=\"b4\">\u003c/a> chaudhary, p., chaudhari, a. k., cheeran, a., and godara, s. (2012). color transform based approach for disease spot detection on plant leaf. \u003ci>international journal of computer science and telecommunications\u003c/i>, 3, 65&#x2013;70.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=p.+chaudhary&amp;author=a.%20k.+chaudhari&amp;author=a.+cheeran&amp;author=s.+godara&amp;publication_year=2012&amp;title=color%20transform%20based%20approach%20for%20disease%20spot%20detection%20on%20plant%20leaf&amp;journal=international+journal+of+computer+science+and+telecommunications&amp;volume=3&amp;pages=65\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b5\" id=\"b5\">\u003c/a> chen, s., zhang, k., zhao, y., sun, y., ban, w., chen, y., et al. (2021). an approach for rice bacterial leaf streak disease segmentation and disease severity estimation. \u003ci>agriculture\u003c/i>, 11, 420. doi:&#xa0;10.3390/agriculture11050420\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agriculture11050420\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+chen&amp;author=k.+zhang&amp;author=y.+zhao&amp;author=y.+sun&amp;author=w.+ban&amp;author=y.+chen&amp;publication_year=2021&amp;title=an%20approach%20for%20rice%20bacterial%20leaf%20streak%20disease%20segmentation%20and%20disease%20severity%20estimation&amp;journal=agriculture&amp;volume=11&amp;pages=420\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b6\" id=\"b6\">\u003c/a> chiang, k.-s., bock, c., el jarroudi, m., delfosse, p., lee, i., and liu, h. (2016). effects of rater bias and assessment method on disease severity estimation with regard to hypothesis testing. \u003ci>phytopathology\u003c/i>, 65, 523&#x2013;535. doi:&#xa0;10.1111/ppa.12435\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1111/ppa.12435\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=k.-s.+chiang&amp;author=c.+bock&amp;author=m.+el%20jarroudi&amp;author=p.+delfosse&amp;author=i.+lee&amp;author=h.+liu&amp;publication_year=2016&amp;title=effects%20of%20rater%20bias%20and%20assessment%20method%20on%20disease%20severity%20estimation%20with%20regard%20to%20hypothesis%20testing&amp;journal=phytopathology&amp;volume=65&amp;pages=523\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b7\" id=\"b7\">\u003c/a> cooke, b. (2006). &#x201c;disease assessment and yield loss,&#x201d; in \u003ci>the epidemiology of plant diseases\u003c/i> (dordrecht: springer), 43&#x2013;80.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=b.+cooke&amp;publication_year=2006&amp;title=disease%20assessment%20and%20yield%20loss&amp;book=the+epidemiology+of+plant+diseases&amp;pages=43\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b8\" id=\"b8\">\u003c/a> ghaiwat, s. n. and arora, p. (2014). detection and classification of plant leaf diseases using image processing techniques: a review. \u003ci>international journal of recent advances in engineering &amp; technology\u003c/i>, 2, 1&#x2013;7.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20n.+ghaiwat&amp;author=p.+arora&amp;publication_year=2014&amp;title=detection%20and%20classification%20of%20plant%20leaf%20diseases%20using%20image%20processing%20techniques%3a%20a%20review&amp;journal=international+journal+of+recent+advances+in+engineering+&amp;+technology&amp;volume=2&amp;pages=1\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b9\" id=\"b9\">\u003c/a> hu, g., wang, h., zhang, y., and wan, m. (2021). detection and severity analysis of tea leaf blight based on deep learning. \u003ci>computers &amp; electrical engineering\u003c/i>, 90, 107023. doi:&#xa0;10.1016/j.compeleceng.2021.107023\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compeleceng.2021.107023\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=g.+hu&amp;author=h.+wang&amp;author=y.+zhang&amp;author=m.+wan&amp;publication_year=2021&amp;title=detection%20and%20severity%20analysis%20of%20tea%20leaf%20blight%20based%20on%20deep%20learning&amp;journal=computers+&amp;+electrical+engineering&amp;volume=90&amp;pages=107023\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b10\" id=\"b10\">\u003c/a> jadhav, s. b. and patil, s. b. (2016). grading of soybean leaf disease based on segmented image using k-means clustering. \u003ci>iaes international journal of artificial intelligence\u003c/i>, 5, 13&#x2013;13. doi:&#xa0;10.11591/ijai.v5.i1.pp13-21\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.11591/ijai.v5.i1.pp13-21\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20b.+jadhav&amp;author=s.%20b.+patil&amp;publication_year=2016&amp;title=grading%20of%20soybean%20leaf%20disease%20based%20on%20segmented%20image%20using%20k-means%20clustering&amp;journal=iaes+international+journal+of+artificial+intelligence&amp;volume=5&amp;pages=13\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b11\" id=\"b11\">\u003c/a> jo, w. j., kim, d. s., sim, h. s., ahn, s. r., lee, h. j., moon, y. h., et al. (2021). estimation of evapotranspiration and water requirements of strawberry plants in greenhouses using environmental data. \u003ci>frontiers in sustainable food systems\u003c/i>, 5, 684808. doi:&#xa0;10.3389/fsufs.2021.684808\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3389/fsufs.2021.684808\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=w.%20j.+jo&amp;author=d.%20s.+kim&amp;author=h.%20s.+sim&amp;author=s.%20r.+ahn&amp;author=h.%20j.+lee&amp;author=y.%20h.+moon&amp;publication_year=2021&amp;title=estimation%20of%20evapotranspiration%20and%20water%20requirements%20of%20strawberry%20plants%20in%20greenhouses%20using%20environmental%20data&amp;journal=frontiers+in+sustainable+food+systems&amp;volume=5&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b12\" id=\"b12\">\u003c/a> khan, m. a., ali, m., shah, m., mahmood, t., ahmad, m., jhanjhi, n., et al. (2021). machine learning-based detection and classification of walnut fungi diseases. \u003ci>intelligent automation &amp; soft computing\u003c/i>, 30, 771&#x2013;785. doi:&#xa0;10.32604/iasc.2021.018039\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.32604/iasc.2021.018039\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=m.%20a.+khan&amp;author=m.+ali&amp;author=m.+shah&amp;author=t.+mahmood&amp;author=m.+ahmad&amp;author=n.+jhanjhi&amp;publication_year=2021&amp;title=machine%20learning-based%20detection%20and%20classification%20of%20walnut%20fungi%20diseases&amp;journal=intelligent+automation+&amp;+soft+computing&amp;volume=30&amp;pages=771\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b13\" id=\"b13\">\u003c/a> kim, s.-k. and ahn, j.-g. (2021). tomato crop diseases classification models using deep cnn-based architectures. \u003ci>j korea acad-ind coop soc\u003c/i>, 22, 7&#x2013;14. doi:&#xa0;10.5762/kais.2021.22.5.7\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.5762/kais.2021.22.5.7\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.-k.+kim&amp;author=j.-g.+ahn&amp;publication_year=2021&amp;title=tomato%20crop%20diseases%20classification%20models%20using%20deep%20cnn-based%20architectures&amp;journal=j+korea+acad-ind+coop+soc&amp;volume=22&amp;pages=7\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b14\" id=\"b14\">\u003c/a> lamichhane, j. r. (2014). xanthomonas arboricola diseases of stone fruit, almond, and walnut trees: progress toward understanding and management. \u003ci>plant disease\u003c/i>, 98, 1600&#x2013;1610. doi:&#xa0;10.1094/pdis-08-14-0831-fe\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30703892/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-08-14-0831-fe\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.%20r.+lamichhane&amp;publication_year=2014&amp;title=xanthomonas%20arboricola%20diseases%20of%20stone%20fruit%2c%20almond%2c%20and%20walnut%20trees%3a%20progress%20toward%20understanding%20and%20management&amp;journal=plant+disease&amp;volume=98&amp;pages=1600\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b15\" id=\"b15\">\u003c/a> li, x., zha, t., liu, p., bourque, c. p.-a., jia, x., and tian, y. (2023). interannual variation in gross ecosystem production and evapotranspiration in a temperate semiarid grassland undergoing vegetation recovery. \u003ci>agricultural and forest meteorology\u003c/i>, 341, 109672. doi:&#xa0;10.1016/j.agrformet.2023.109672\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.agrformet.2023.109672\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=x.+li&amp;author=t.+zha&amp;author=p.+liu&amp;author=c.%20p.-a.+bourque&amp;author=x.+jia&amp;author=y.+tian&amp;publication_year=2023&amp;title=interannual%20variation%20in%20gross%20ecosystem%20production%20and%20evapotranspiration%20in%20a%20temperate%20semiarid%20grassland%20undergoing%20vegetation%20recovery&amp;journal=agricultural+and+forest+meteorology&amp;volume=341&amp;pages=109672\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b16\" id=\"b16\">\u003c/a> lin, k., gong, l., huang, y., liu, c., and pan, j. (2019). deep learning-based segmentation and quantification of cucumber powdery mildew using convolutional neural network. \u003ci>frontiers in plant science\u003c/i>, 10, 155. doi:&#xa0;10.3389/fpls.2019.00155\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30891048/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.3389/fpls.2019.00155\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=k.+lin&amp;author=l.+gong&amp;author=y.+huang&amp;author=c.+liu&amp;author=j.+pan&amp;publication_year=2019&amp;title=deep%20learning-based%20segmentation%20and%20quantification%20of%20cucumber%20powdery%20mildew%20using%20convolutional%20neural%20network&amp;journal=frontiers+in+plant+science&amp;volume=10&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b17\" id=\"b17\">\u003c/a> ma, j., du, k., zhang, l., zheng, f., chu, j., and sun, z. (2017). a segmentation method for greenhouse vegetable foliar disease spots images using color information and region growing. \u003ci>computers and electronics in agriculture\u003c/i>, 142, 110&#x2013;117. doi:&#xa0;10.1016/j.compag.2017.08.023\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2017.08.023\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.+ma&amp;author=k.+du&amp;author=l.+zhang&amp;author=f.+zheng&amp;author=j.+chu&amp;author=z.+sun&amp;publication_year=2017&amp;title=a%20segmentation%20method%20for%20greenhouse%20vegetable%20foliar%20disease%20spots%20images%20using%20color%20information%20and%20region%20growing&amp;journal=computers+and+electronics+in+agriculture&amp;volume=142&amp;pages=110\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b18\" id=\"b18\">\u003c/a> mao, r., wang, z., li, f., zhou, j., chen, y., and hu, x. (2023). gseyolox-s: an improved lightweight network for identifying the severity of wheat fusarium head blight. \u003ci>agronomy\u003c/i>, 13, 242. doi:&#xa0;10.3390/agronomy13010242\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agronomy13010242\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=r.+mao&amp;author=z.+wang&amp;author=f.+li&amp;author=j.+zhou&amp;author=y.+chen&amp;author=x.+hu&amp;publication_year=2023&amp;title=gseyolox-s%3a%20an%20improved%20lightweight%20network%20for%20identifying%20the%20severity%20of%20wheat%20fusarium%20head%20blight&amp;journal=agronomy&amp;volume=13&amp;pages=242\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b19\" id=\"b19\">\u003c/a> mcgranahan, g. and leslie, c. (1991). walnuts (juglans). \u003ci>molecules\u003c/i>, 907&#x2013;924. doi:&#xa0;10.17660/actahortic.1991.290.20\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.17660/actahortic.1991.290.20\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=g.+mcgranahan&amp;author=c.+leslie&amp;publication_year=1991&amp;title=walnuts%20%28juglans%29&amp;journal=molecules&amp;pages=907\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b20\" id=\"b20\">\u003c/a> mircetich, s. m., sanborn, r., and ramos, d. (1980). natural spread, graft-transmission, and possible etiology of walnut blackline disease. \u003ci>phytopathology\u003c/i>, 70, 962&#x2013;968. doi:&#xa0;10.1094/phyto-70-962\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/phyto-70-962\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20m.+mircetich&amp;author=r.+sanborn&amp;author=d.+ramos&amp;publication_year=1980&amp;title=natural%20spread%2c%20graft-transmission%2c%20and%20possible%20etiology%20of%20walnut%20blackline%20disease&amp;journal=phytopathology&amp;volume=70&amp;pages=962\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b21\" id=\"b21\">\u003c/a> moragrega, c., matias, j., alet&#xe0;, n., montesinos, e., and rovira, m. (2011). apical necrosis and premature drop of persian (english) walnut fruit caused by xanthomonas arboricola pv. juglandis. \u003ci>plant disease\u003c/i>, 95, 1565&#x2013;1570. doi:&#xa0;10.1094/pdis-03-11-0259\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30732020/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-03-11-0259\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=c.+moragrega&amp;author=j.+matias&amp;author=n.+alet%c3%a0&amp;author=e.+montesinos&amp;author=m.+rovira&amp;publication_year=2011&amp;title=apical%20necrosis%20and%20premature%20drop%20of%20persian%20%28english%29%20walnut%20fruit%20caused%20by%20xanthomonas%20arboricola%20pv.%20juglandis&amp;journal=plant+disease&amp;volume=95&amp;pages=1565\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b22\" id=\"b22\">\u003c/a> ngugi, l. c., abdelwahab, m., and abo-zahhad, m. (2020). tomato leaf segmentation algorithms for mobile phone applications using deep learning. \u003ci>computers and electronics in agriculture\u003c/i>, 178, 105788. doi:&#xa0;10.1016/j.compag.2020.105788\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2020.105788\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=l.%20c.+ngugi&amp;author=m.+abdelwahab&amp;author=m.+abo-zahhad&amp;publication_year=2020&amp;title=tomato%20leaf%20segmentation%20algorithms%20for%20mobile%20phone%20applications%20using%20deep%20learning&amp;journal=computers+and+electronics+in+agriculture&amp;volume=178&amp;pages=105788\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b23\" id=\"b23\">\u003c/a> ozturk, o., sarica, b., and seker, d. z. (2025). interpretable and robust ensemble deep learning framework for tea leaf disease classification. \u003ci>horticulturae\u003c/i>, 11, 437. doi:&#xa0;10.3390/horticulturae11040437\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/horticulturae11040437\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=o.+ozturk&amp;author=b.+sarica&amp;author=d.%20z.+seker&amp;publication_year=2025&amp;title=interpretable%20and%20robust%20ensemble%20deep%20learning%20framework%20for%20tea%20leaf%20disease%20classification&amp;journal=horticulturae&amp;volume=11&amp;pages=437\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b24\" id=\"b24\">\u003c/a> parashar, n., johri, p., khan, a. a., gaur, n., and kadry, s. (2024). an integrated analysis of yield prediction models: a comprehensive review of advancements and challenges. \u003ci>computers, materials &amp; continua\u003c/i>, 80 (1). doi:&#xa0;10.32604/cmc.2024.050240\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.32604/cmc.2024.050240\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=n.+parashar&amp;author=p.+johri&amp;author=a.%20a.+khan&amp;author=n.+gaur&amp;author=s.+kadry&amp;publication_year=2024&amp;title=an%20integrated%20analysis%20of%20yield%20prediction%20models%3a%20a%20comprehensive%20review%20of%20advancements%20and%20challenges&amp;journal=computers,+materials+&amp;+continua&amp;volume=80&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b25\" id=\"b25\">\u003c/a> picon, a., alvarez-gila, a., seitz, m., ortiz-barredo, a., echazarra, j., and johannes, a. (2019). deep convolutional neural networks for mobile capture device-based crop disease classification in the wild. \u003ci>computers and electronics in agriculture\u003c/i>, 161, 280&#x2013;290. doi:&#xa0;10.1016/j.compag.2018.04.002\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2018.04.002\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=a.+picon&amp;author=a.+alvarez-gila&amp;author=m.+seitz&amp;author=a.+ortiz-barredo&amp;author=j.+echazarra&amp;author=a.+johannes&amp;publication_year=2019&amp;title=deep%20convolutional%20neural%20networks%20for%20mobile%20capture%20device-based%20crop%20disease%20classification%20in%20the%20wild&amp;journal=computers+and+electronics+in+agriculture&amp;volume=161&amp;pages=280\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b26\" id=\"b26\">\u003c/a> rastogi, a., arora, r., and sharma, s. (2015). &#x201c;leaf disease detection and grading using computer vision technology &amp; fuzzy logic,&#x201d; in paper presented at the 2015 2nd international conference on signal processing and integrated networks (spin).\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=a.+rastogi&amp;author=r.+arora&amp;author=s.+sharma&amp;publication_year=2015&amp;title=leaf%20disease%20detection%20and%20grading%20using%20computer%20vision%20technology%20%26%20fuzzy%20logic&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b27\" id=\"b27\">\u003c/a> rustia, d. j. a., lee, w.-c., lu, c.-y., wu, y.-f., shih, p.-y., and chen, s.-k. (2023). edge-based wireless imaging system for continuous monitoring of insect pests in a remote outdoor mango orchard. \u003ci>computers and electronics in agriculture\u003c/i>, 211, 108019. doi:&#xa0;10.1016/j.compag.2023.108019\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2023.108019\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=d.%20j.%20a.+rustia&amp;author=w.-c.+lee&amp;author=c.-y.+lu&amp;author=y.-f.+wu&amp;author=p.-y.+shih&amp;author=s.-k.+chen&amp;publication_year=2023&amp;title=edge-based%20wireless%20imaging%20system%20for%20continuous%20monitoring%20of%20insect%20pests%20in%20a%20remote%20outdoor%20mango%20orchard&amp;journal=computers+and+electronics+in+agriculture&amp;volume=211&amp;pages=108019\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b28\" id=\"b28\">\u003c/a> shi, t., liu, y., zheng, x., hu, k., huang, h., liu, h., et al. (2023). recent advances in plant disease severity assessment using convolutional neural networks. \u003ci>scientific reports\u003c/i>, 13, 2336. doi:&#xa0;10.1038/s41598-023-29230-7\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/36759626/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1038/s41598-023-29230-7\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=t.+shi&amp;author=y.+liu&amp;author=x.+zheng&amp;author=k.+hu&amp;author=h.+huang&amp;author=h.+liu&amp;publication_year=2023&amp;title=recent%20advances%20in%20plant%20disease%20severity%20assessment%20using%20convolutional%20neural%20networks&amp;journal=scientific+reports&amp;volume=13&amp;pages=2336\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b29\" id=\"b29\">\u003c/a> singh, u. p., chouhan, s. s., jain, s., and jain, s. (2019). multilayer convolution neural network for the classification of mango leaves infected by anthracnose disease. \u003ci>ieee access\u003c/i>, 7, 43721&#x2013;43729. doi:&#xa0;10.1109/access.2019.2907383\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/access.2019.2907383\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=u.%20p.+singh&amp;author=s.%20s.+chouhan&amp;author=s.+jain&amp;author=s.+jain&amp;publication_year=2019&amp;title=multilayer%20convolution%20neural%20network%20for%20the%20classification%20of%20mango%20leaves%20infected%20by%20anthracnose%20disease&amp;journal=ieee+access&amp;volume=7&amp;pages=43721\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b30\" id=\"b30\">\u003c/a> su, y., wang, j., li, j., wang, l., wang, k., li, a., et al. (2023). spatiotemporal changes and driving factors of reference evapotranspiration and crop evapotranspiration for cotton production in china from 1960 to 2019. \u003ci>frontiers in environmental science\u003c/i>, 11, 1251789. doi:&#xa0;10.3389/fenvs.2023.1251789\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3389/fenvs.2023.1251789\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=y.+su&amp;author=j.+wang&amp;author=j.+li&amp;author=l.+wang&amp;author=k.+wang&amp;author=a.+li&amp;publication_year=2023&amp;title=spatiotemporal%20changes%20and%20driving%20factors%20of%20reference%20evapotranspiration%20and%20crop%20evapotranspiration%20for%20cotton%20production%20in%20china%20from%201960%20to%202019&amp;journal=frontiers+in+environmental+science&amp;volume=11&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b31\" id=\"b31\">\u003c/a> tripathi, r. (2021). &#x201c;a deep learning approach for plant material disease identification,&#x201d; in paper presented at the iop conference series: materials science and engineering.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=r.+tripathi&amp;publication_year=2021&amp;title=a%20deep%20learning%20approach%20for%20plant%20material%20disease%20identification&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b32\" id=\"b32\">\u003c/a> vishnoi, v. k., kumar, k., kumar, b., mohan, s., and khan, a. a. (2022). detection of apple plant diseases using leaf images through convolutional neural network . \u003ci>ieee access\u003c/i>, 11, 6594&#x2013;6609. doi:&#xa0;10.1109/access.2022.3232917\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/access.2022.3232917\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=v.%20k.+vishnoi&amp;author=k.+kumar&amp;author=b.+kumar&amp;author=s.+mohan&amp;author=a.%20a.+khan&amp;publication_year=2022&amp;title=detection%20of%20apple%20plant%20diseases%20using%20leaf%20images%20through%20convolutional%20neural%20network&amp;journal=ieee+access&amp;volume=11&amp;pages=6594\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b33\" id=\"b33\">\u003c/a> wang, f., dun, c., tang, t., duan, y., guo, x., and you, j. (2022). boeremia exigua causes leaf spot of walnut trees (juglans regia) in china. \u003ci>plant disease\u003c/i>, 106, 1993. doi:&#xa0;10.1094/pdis-10-21-2304-pdn\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/pdis-10-21-2304-pdn\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=f.+wang&amp;author=c.+dun&amp;author=t.+tang&amp;author=y.+duan&amp;author=x.+guo&amp;author=j.+you&amp;publication_year=2022&amp;title=boeremia%20exigua%20causes%20leaf%20spot%20of%20walnut%20trees%20%28juglans%20regia%29%20in%20china&amp;journal=plant+disease&amp;volume=106&amp;pages=1993\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b34\" id=\"b34\">\u003c/a> wang, q.-h., fan, k., li, d.-w., han, c.-m., qu, y.-y., qi, y.-k., et al. (2020). identification, virulence and fungicide sensitivity of colletotrichum gloeosporioides ss responsible for walnut anthracnose disease in china. \u003ci>plant disease\u003c/i>, 104, 1358&#x2013;1368. doi:&#xa0;10.1094/pdis-12-19-2569-re\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/32196416/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-12-19-2569-re\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=q.-h.+wang&amp;author=k.+fan&amp;author=d.-w.+li&amp;author=c.-m.+han&amp;author=y.-y.+qu&amp;author=y.-k.+qi&amp;publication_year=2020&amp;title=identification%2c%20virulence%20and%20fungicide%20sensitivity%20of%20colletotrichum%20gloeosporioides%20ss%20responsible%20for%20walnut%20anthracnose%20disease%20in%20china&amp;journal=plant+disease&amp;volume=104&amp;pages=1358\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b35\" id=\"b35\">\u003c/a> weber, b. c. (1980). \u003ci>how to diagnose black walnut damage\u003c/i> vol. 57 (north central forest experiment station, forest service, us department of agriculture).\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=b.%20c.+weber&amp;publication_year=1980&amp;book=how+to+diagnose+black+walnut+damage&amp;volume=57&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b36\" id=\"b36\">\u003c/a> xu, w. (2024). hcf-net: hierarchicalcontextfusion network forinfrared small object detection. \u003ci>arxiv\u003c/i>. arxiv, 2403.10778. doi:&#xa0;10.1109/icme57554.2024.10687431\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/icme57554.2024.10687431\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=w.+xu&amp;publication_year=2024&amp;title=hcf-net%3a%20hierarchicalcontextfusion%20network%20forinfrared%20small%20object%20detection&amp;journal=arxiv&amp;volume=arxiv&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b37\" id=\"b37\">\u003c/a> yang, c., deng, y., wang, f., yang, h., xu, x., zeng, q., et al. (2021). brown leaf spot on \u003ci>juglans sigillata\u003c/i> caused by \u003ci>ophiognomonia leptostyla\u003c/i> in sichuan, china. \u003ci>plant disease\u003c/i>, 105, 4160. doi:&#xa0;10.1094/pdis-02-21-0344-pdn\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/pdis-02-21-0344-pdn\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=c.+yang&amp;author=y.+deng&amp;author=f.+wang&amp;author=h.+yang&amp;author=x.+xu&amp;author=q.+zeng&amp;publication_year=2021&amp;title=brown%20leaf%20spot%20on%20juglans%20sigillata%20caused%20by%20ophiognomonia%20leptostyla%20in%20sichuan%2c%20china&amp;journal=plant+disease&amp;volume=105&amp;pages=4160\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b38\" id=\"b38\">\u003c/a> yang, q., duan, s., and wang, l. (2022). efficient identification of apple leaf diseases in the wild using convolutional neural networks. \u003ci>agronomy\u003c/i>, 12, 2784. doi:&#xa0;10.3390/agronomy12112784\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agronomy12112784\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=q.+yang&amp;author=s.+duan&amp;author=l.+wang&amp;publication_year=2022&amp;title=efficient%20identification%20of%20apple%20leaf%20diseases%20in%20the%20wild%20using%20convolutional%20neural%20networks&amp;journal=agronomy&amp;volume=12&amp;pages=2784\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b39\" id=\"b39\">\u003c/a> zarei, s., taghavi, s. m., banihashemi, z., hamzehzarghani, h., and osdaghi, e. (2019). etiology of leaf spot and fruit canker symptoms on stone fruits and nut trees in iran. \u003ci>journal of plant pathology\u003c/i>, 101, 1133&#x2013;1142. doi:&#xa0;10.1007/s42161-019-00283-w\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1007/s42161-019-00283-w\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+zarei&amp;author=s.%20m.+taghavi&amp;author=z.+banihashemi&amp;author=h.+hamzehzarghani&amp;author=e.+osdaghi&amp;publication_year=2019&amp;title=etiology%20of%20leaf%20spot%20and%20fruit%20canker%20symptoms%20on%20stone%20fruits%20and%20nut%20trees%20in%20iran&amp;journal=journal+of+plant+pathology&amp;volume=101&amp;pages=1133\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b40\" id=\"b40\">\u003c/a> zhang, y., li, x., &#x160;im&#x16f;nek, j., shi, h., chen, n., and hu, q. (2023). quantifying water and salt movement in a soil-plant system of a corn field using hydrus (2d/3d) and the stable isotope method. \u003ci>agricultural water management\u003c/i>, 288, 108492. doi:&#xa0;10.1016/j.agwat.2023.108492\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.agwat.2023.108492\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=y.+zhang&amp;author=x.+li&amp;author=j.+%c5%a0im%c5%afnek&amp;author=h.+shi&amp;author=n.+chen&amp;author=q.+hu&amp;publication_year=2023&amp;title=quantifying%20water%20and%20salt%20movement%20in%20a%20soil-plant%20system%20of%20a%20corn%20field%20using%20hydrus%20%282d%2f3d%29%20and%20the%20stable%20isotope%20method&amp;journal=agricultural+water+management&amp;volume=288&amp;pages=108492\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003c/div>\u003cdiv class=\"thinlinem20\">\u003c/div>\u003cdiv class=\"abstractsummary\">\u003cp>\u003cspan>keywords:\u003c/span> walnut, brown spot disease (\u003ci>ophiognomonia leptostyla\u003c/i>), hierarchical feature selection, edge features perception, adaptive multi-scale dilated convolution, disease grading\u003c/p>\u003cp>\u003cspan>citation:\u003c/span> wei y, zeng d and zheng l (2025) a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution. \u003ci>front. plant sci.\u003c/i> 16:1641677. doi: 10.3389/fpls.2025.1641677\u003c/p>\u003cp id=\"timestamps\">\u003cspan>received:\u003c/span> 05 june 2025; \u003cspan>accepted:\u003c/span> 09 september 2025;\u003cbr>\u003cspan>published:\u003c/span> 03 october 2025.\u003c/p>\u003cdiv>\u003cp>edited by:\u003c/p>\u003ca href=\"https://loop.frontiersin.org/people/1037951\">ravinder kumar\u003c/a>, indian agricultural research institute (icar), india\u003c/div>\u003cdiv>\u003cp>reviewed by:\u003c/p>\u003ca href=\"https://loop.frontiersin.org/people/2082015\">geza bujdoso\u003c/a>, hungarian university of agricultural and life sciences, hungary\u003cbr>\r\n\u003ca href=\"https://loop.frontiersin.org/people/3078480\">vasudha vedula\u003c/a>, university of texas of the permian basin, united states\u003c/div>\u003cp>\u003cspan>copyright\u003c/span> &#xa9; 2025 wei, zeng and zheng. this is an open-access article distributed under the terms of the \u003ca rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\" target=\"_blank\">creative commons attribution license (cc by)\u003c/a>. the use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. no use, distribution or reproduction is permitted which does not comply with these terms.\u003c/p>\u003cp>\u003cspan>*correspondence:\u003c/span> liangfang zheng, \u003ca id=\"encmail\">mtgxnjaynta0othamtyzlmnvbq==\u003c/a>\u003c/p>\u003cp>\u003cspan>\u003csup>&#x2020;\u003c/sup>\u003c/span>these authors have contributed equally to this work\u003c/p>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>","\u003cul class=\"flyoutjournal\">\u003cli>\u003ca href=\"#h1\">abstract\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h2\">1 introduction\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h3\">2 materials and methods\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h4\">3 experiments and results analysis\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h5\">4 conclusion\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h6\">5 discussion and future work\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h7\">data availability statement\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h8\">author contributions\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h9\">funding\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h10\">conflict of interest\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h11\">generative ai statement\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h12\">publisher&#x2019;s note\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h13\">references\u003c/a>\u003c/li>\u003c/ul>",[629,636,642],{"name":630,"fileserverpackageentryid":19,"fileserverid":631,"fileserverversionnumber":632,"type":633},"epub.epub","1641677/epub",2,{"code":634,"name":635},"epub","epub",{"name":637,"fileserverpackageentryid":637,"fileserverid":638,"fileserverversionnumber":632,"type":639},"fpls-16-1641677.xml","1641677/xml",{"code":640,"name":641},"nlm_xml","xml",{"name":643,"fileserverpackageentryid":19,"fileserverid":644,"fileserverversionnumber":632,"type":645},"publishers-proof.pdf","1641677/publishers-proof",{"code":646,"name":647},"pdf","pdf","v3",{"title":650,"link":651,"meta":655,"script":748},"frontiers | a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution",[652],{"rel":653,"href":654},"canonical","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/full",[656,659,662,664,667,671,673,677,680,683,686,688,690,692,694,696,699,702,704,707,709,711,714,717,720,723,726,730,734,737,740,743,745],{"hid":657,"property":657,"name":657,"content":658},"description","walnut leaf brown spot disease, caused by ophiognomonia leptostyla, is among the most destructive fungal diseases in walnut cultivation. in the development o...",{"hid":660,"property":660,"name":661,"content":650},"og:title","title",{"hid":663,"property":663,"name":657,"content":658},"og:description",{"hid":665,"name":665,"content":666},"keywords","walnut,brown spot disease (ophiognomonia leptostyla),hierarchical feature selection,edge features perception,adaptive multi-scale dilated convolution,disease grading",{"hid":668,"property":668,"name":669,"content":670},"og:site_name","site_name","frontiers",{"hid":672,"property":672,"name":385,"content":404},"og:image",{"hid":674,"property":674,"name":675,"content":676},"og:type","type","article",{"hid":678,"property":678,"name":679,"content":654},"og:url","url",{"hid":681,"name":681,"content":682},"twitter:card","summary_large_image",{"hid":684,"name":684,"content":685},"citation_volume","16",{"hid":687,"name":687,"content":116},"citation_journal_title",{"hid":689,"name":689,"content":670},"citation_publisher",{"hid":691,"name":691,"content":611},"citation_journal_abbrev",{"hid":693,"name":693,"content":612},"citation_issn",{"hid":695,"name":695,"content":538},"citation_doi",{"hid":697,"name":697,"content":698},"citation_firstpage","1641677",{"hid":700,"name":700,"content":701},"citation_language","english",{"hid":703,"name":703,"content":539},"citation_title",{"hid":705,"name":705,"content":706},"citation_keywords","walnut; brown spot disease (ophiognomonia leptostyla); hierarchical feature selection; edge features perception; adaptive multi-scale dilated convolution; disease grading",{"hid":708,"name":708,"content":544},"citation_abstract",{"hid":710,"name":710,"content":553},"citation_article_type",{"hid":712,"name":712,"content":713},"citation_pdf_url","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/pdf",{"hid":715,"name":715,"content":716},"citation_xml_url","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/xml",{"hid":718,"name":718,"content":719},"citation_fulltext_world_readable","yes",{"hid":721,"name":721,"content":722},"citation_online_date","2025/09/09",{"hid":724,"name":724,"content":725},"citation_publication_date","2025/10/03",{"hid":727,"name":728,"content":729},"citation_author_0","citation_author","wei, yuting ",{"hid":731,"name":732,"content":733},"citation_author_institution_0","citation_author_institution","college of information engineering, tarim university, china",{"hid":735,"name":728,"content":736},"citation_author_1","zeng, debin ",{"hid":738,"name":732,"content":739},"citation_author_institution_1","key laboratory of tarim oasis agriculture, ministry of education, tarim university, china",{"hid":741,"name":728,"content":742},"citation_author_2","zheng, liangfang ",{"hid":744,"name":732,"content":739},"citation_author_institution_2",{"hid":746,"name":746,"content":747},"dc.identifier","doi:10.3389/fpls.2025.1641677",[749,752,754,756,758],{"src":750,"body":13,"type":751,"async":13},"https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6","text/javascript",{"src":753,"body":13,"type":751,"async":13},"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/mathjax.js?config=tex-mml-am_chtml",{"src":755,"body":13,"type":751,"async":13},"https://d1bxh8uas1mnw7.cloudfront.net/assets/altmetric_badges-f0bc9b243ff5677d05460c1eb71834ca998946d764eb3bc244ab4b18ba50d21e.js",{"src":757,"body":13,"type":751,"async":13},"https://api.altmetric.com/v1/doi/10.3389/fpls.2025.1641677?callback=_altmetric.embed_callback&domain=www.frontiersin.org&key=3c130976ca2b8f2e88f8377633751ba1&cache_until=14-15",{"src":759,"body":13,"type":751,"async":13},"https://crossmark-cdn.crossref.org/widget/v2.0/widget.js",{"articlehubslug":19,"articlehubpage":761,"articlehubarticleslist":762,"canjournalhasarticlehub":367,"articledoilist":763},{},[],[],{"title":19,"image":-1,"breadcrumbs":765,"linkscollection":766,"metricscollection":768},[],{"total":389,"items":767},[],{"total":389,"items":769},[]] window.__nuxt__={};window.__nuxt__.config={public:{isdevmode:false,appname:"article-pages-2024",baseurl:"https://www.frontiersin.org",domain:"frontiersin.org",loopurl:"https://loop.frontiersin.org",ssmaindomain:"frontiersin.org",contentfulurl:"https://graphql.contentful.com/content/v1/spaces",spaceid:1,spacename:"frontiers",livespaceid:1,tenantlogourl:"",frontiersgraphurl:"https://apollo-federation-gateway.frontiersin.org",registrationapiurl:"https://onboarding-ui.frontiersin.org",emaildigestapiurl:"https://api-email-digest.frontiersin.org",journalfilesapiurl:"https://files-journal-api.frontiersin.org",searchapiurl:"https://search-api.frontiersin.org",productionforumapiurl:"https://production-forum-api-v2.frontiersin.org",gtmserverurl:"https://tag-manager.frontiersin.org",gtmid:"gtm-m322fv2",gtmauth:"owvbwxfajr21yqv1fe1caq",gtmpreview:"env-1",figsharepluginurl:"https://widgets.figshare.com/static/figshare.js",figshareapiurl:"https://api.figshare.com/v2/collections/search",figsharetimeout:3000,crossmarkpublisheddate:"2015/08/24",googlerecaptchasitekey:"6ldg3i0uaaaaaoc4quh35ubhgjotehp_stxhgr_v",googlerecaptchakeyname:"frontiersrecaptchav2",faviconsize16:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_16-tenantfavicon-frontiers.png",faviconsize32:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_32-tenantfavicon-frontiers.png",faviconsize180:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_180-tenantfavicon-frontiers.png",faviconsize192:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_192-tenantfavicon-frontiers.png",faviconsize512:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_512-tenantfavicon-frontiers.png",socialmediaimg:"https://brand.frontiersin.org/m/1c8bcb536c789e11/guidelines-frontiers_logo_1200x628_1-91to1.png",linkedarticlecopytext:"'{\"articletypecopytext\":[{\"articletypeid\":0,\"originalarticlecopytext\":\"part of this article's content has been mentioned in:\",\"linkedarticlecopytext\":\"this article mentions parts of:\"},{\"articletypeid\":122,\"originalarticlecopytext\":\"parts of this article's content have been modified or rectified in:\",\"linkedarticlecopytext\":\"this article is an erratum on:\"},{\"articletypeid\":129,\"originalarticlecopytext\":\"parts of this article's content have been modified or rectified in:\",\"linkedarticlecopytext\":\"this article is an addendum to:\"},{\"articletypeid\":128,\"originalarticlecopytext\":\"a correction has been applied to this article in:\",\"linkedarticlecopytext\":\"this article is a correction to:\"},{\"articletypeid\":134,\"originalarticlecopytext\":\"a retraction of this article was approved in:\",\"linkedarticlecopytext\":\"this article is a retraction of:\"},{\"articletypeid\":29,\"originalarticlecopytext\":\"a commentary has been posted on this article:\",\"linkedarticlecopytext\":\"this article is a commentary on:\"},{\"articletypeid\":30,\"originalarticlecopytext\":\"a commentary has been posted on this article:\",\"linkedarticlecopytext\":\"this article is a commentary on:\"}],\"articleidcopytext\":[]}'\n",acceptedarticleexcludedjournalids:"'[2766,979]'\n",articletypeconfigurablelabel:"\u003c\u003carticle-type:uppercase>> article",settingsfeaturesswitchers:"'{\"displaytitlepilllabels\":true,\"displayrelatedarticlesbox\":true,\"showeditors\":true,\"showreviewers\":true,\"showloopimpactlink\":true,\"enablefigshare\":false,\"usexmlimages\":true}'\n",journalswitharticlehub:"'{\"strings\":[\"science\"]}'\n",terminologysettings:"'{\"terms\":[{\"sequencenumber\":1,\"key\":\"frontiers\",\"tenantterm\":\"frontiers\",\"frontiersdefaultterm\":\"frontiers\",\"category\":\"customer\"},{\"sequencenumber\":2,\"key\":\"submission_system\",\"tenantterm\":\"submission system\",\"frontiersdefaultterm\":\"submission system\",\"category\":\"product\"},{\"sequencenumber\":3,\"key\":\"public_pages\",\"tenantterm\":\"public pages\",\"frontiersdefaultterm\":\"public pages\",\"category\":\"product\"},{\"sequencenumber\":4,\"key\":\"my_frontiers\",\"tenantterm\":\"my frontiers\",\"frontiersdefaultterm\":\"my frontiers\",\"category\":\"product\"},{\"sequencenumber\":5,\"key\":\"digital_editorial_office\",\"tenantterm\":\"digital editorial office\",\"frontiersdefaultterm\":\"digital editorial office\",\"category\":\"product\"},{\"sequencenumber\":6,\"key\":\"deo\",\"tenantterm\":\"deo\",\"frontiersdefaultterm\":\"deo\",\"category\":\"product\"},{\"sequencenumber\":7,\"key\":\"digital_editorial_office_for_chiefs\",\"tenantterm\":\"digital editorial office for chiefs\",\"frontiersdefaultterm\":\"digital editorial office for chiefs\",\"category\":\"product\"},{\"sequencenumber\":8,\"key\":\"digital_editorial_office_for_eof\",\"tenantterm\":\"digital editorial office for eof\",\"frontiersdefaultterm\":\"digital editorial office for eof\",\"category\":\"product\"},{\"sequencenumber\":9,\"key\":\"editorial_office\",\"tenantterm\":\"editorial office\",\"frontiersdefaultterm\":\"editorial office\",\"category\":\"product\"},{\"sequencenumber\":10,\"key\":\"eof\",\"tenantterm\":\"eof\",\"frontiersdefaultterm\":\"eof\",\"category\":\"product\"},{\"sequencenumber\":11,\"key\":\"research_topic_management\",\"tenantterm\":\"research topic management\",\"frontiersdefaultterm\":\"research topic management\",\"category\":\"product\"},{\"sequencenumber\":12,\"key\":\"review_forum\",\"tenantterm\":\"review forum\",\"frontiersdefaultterm\":\"review forum\",\"category\":\"product\"},{\"sequencenumber\":13,\"key\":\"accounting_office\",\"tenantterm\":\"accounting office\",\"frontiersdefaultterm\":\"accounting office\",\"category\":\"product\"},{\"sequencenumber\":14,\"key\":\"aof\",\"tenantterm\":\"aof\",\"frontiersdefaultterm\":\"aof\",\"category\":\"product\"},{\"sequencenumber\":15,\"key\":\"publishing_office\",\"tenantterm\":\"publishing office\",\"frontiersdefaultterm\":\"publishing office\",\"category\":\"product\"},{\"sequencenumber\":16,\"key\":\"production_office\",\"tenantterm\":\"production office forum\",\"frontiersdefaultterm\":\"production office forum\",\"category\":\"product\"},{\"sequencenumber\":17,\"key\":\"pof\",\"tenantterm\":\"pof\",\"frontiersdefaultterm\":\"pof\",\"category\":\"product\"},{\"sequencenumber\":18,\"key\":\"book_office_forum\",\"tenantterm\":\"book office forum\",\"frontiersdefaultterm\":\"book office forum\",\"category\":\"product\"},{\"sequencenumber\":19,\"key\":\"bof\",\"tenantterm\":\"bof\",\"frontiersdefaultterm\":\"bof\",\"category\":\"product\"},{\"sequencenumber\":20,\"key\":\"aira\",\"tenantterm\":\"aira\",\"frontiersdefaultterm\":\"aira\",\"category\":\"product\"},{\"sequencenumber\":21,\"key\":\"editorial_board_management\",\"tenantterm\":\"editorial board management\",\"frontiersdefaultterm\":\"editorial board management\",\"category\":\"product\"},{\"sequencenumber\":22,\"key\":\"ebm\",\"tenantterm\":\"ebm\",\"frontiersdefaultterm\":\"ebm\",\"category\":\"product\"},{\"sequencenumber\":23,\"key\":\"domain\",\"tenantterm\":\"domain\",\"frontiersdefaultterm\":\"domain\",\"category\":\"taxonomy\"},{\"sequencenumber\":24,\"key\":\"journal\",\"tenantterm\":\"journal\",\"frontiersdefaultterm\":\"journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":25,\"key\":\"section\",\"tenantterm\":\"section\",\"frontiersdefaultterm\":\"section\",\"category\":\"taxonomy\"},{\"sequencenumber\":26,\"key\":\"domains\",\"tenantterm\":\"domains\",\"frontiersdefaultterm\":\"domains\",\"category\":\"taxonomy\"},{\"sequencenumber\":27,\"key\":\"specialty_section\",\"tenantterm\":\"specialty section\",\"frontiersdefaultterm\":\"specialty section\",\"category\":\"taxonomy\"},{\"sequencenumber\":28,\"key\":\"specialty_journal\",\"tenantterm\":\"specialty journal\",\"frontiersdefaultterm\":\"specialty journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":29,\"key\":\"journals\",\"tenantterm\":\"journals\",\"frontiersdefaultterm\":\"journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":30,\"key\":\"sections\",\"tenantterm\":\"sections\",\"frontiersdefaultterm\":\"sections\",\"category\":\"taxonomy\"},{\"sequencenumber\":31,\"key\":\"specialty_sections\",\"tenantterm\":\"specialty sections\",\"frontiersdefaultterm\":\"specialty sections\",\"category\":\"taxonomy\"},{\"sequencenumber\":32,\"key\":\"specialty_journals\",\"tenantterm\":\"specialty journals\",\"frontiersdefaultterm\":\"specialty journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":33,\"key\":\"manuscript\",\"tenantterm\":\"manuscript\",\"frontiersdefaultterm\":\"manuscript\",\"category\":\"core\"},{\"sequencenumber\":34,\"key\":\"manuscripts\",\"tenantterm\":\"manuscripts\",\"frontiersdefaultterm\":\"manuscripts\",\"category\":\"core\"},{\"sequencenumber\":35,\"key\":\"article\",\"tenantterm\":\"article\",\"frontiersdefaultterm\":\"article\",\"category\":\"core\"},{\"sequencenumber\":36,\"key\":\"articles\",\"tenantterm\":\"articles\",\"frontiersdefaultterm\":\"articles\",\"category\":\"core\"},{\"sequencenumber\":37,\"key\":\"article_type\",\"tenantterm\":\"article type\",\"frontiersdefaultterm\":\"article type\",\"category\":\"core\"},{\"sequencenumber\":38,\"key\":\"article_types\",\"tenantterm\":\"article types\",\"frontiersdefaultterm\":\"article types\",\"category\":\"core\"},{\"sequencenumber\":39,\"key\":\"author\",\"tenantterm\":\"author\",\"frontiersdefaultterm\":\"author\",\"category\":\"label (role)\"},{\"sequencenumber\":40,\"key\":\"authors\",\"tenantterm\":\"authors\",\"frontiersdefaultterm\":\"authors\",\"category\":\"label (role)\"},{\"sequencenumber\":41,\"key\":\"authoring\",\"tenantterm\":\"authoring\",\"frontiersdefaultterm\":\"authoring\",\"category\":\"core\"},{\"sequencenumber\":42,\"key\":\"authored\",\"tenantterm\":\"authored\",\"frontiersdefaultterm\":\"authored\",\"category\":\"core\"},{\"sequencenumber\":43,\"key\":\"accept\",\"tenantterm\":\"accept\",\"frontiersdefaultterm\":\"accept\",\"category\":\"process\"},{\"sequencenumber\":44,\"key\":\"accepted\",\"tenantterm\":\"accepted\",\"frontiersdefaultterm\":\"accepted\",\"category\":\"process\"},{\"sequencenumber\":45,\"key\":\"assistant_field_chief_editor\",\"tenantterm\":\"assistant field chief editor\",\"frontiersdefaultterm\":\"assistant field chief editor\",\"description\":\"an editorial role on a field journal that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":46,\"key\":\"assistant_specialty_chief_editor\",\"tenantterm\":\"assistant specialty chief editor\",\"frontiersdefaultterm\":\"assistant specialty chief editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":47,\"key\":\"assistant_specialty_chief_editors\",\"tenantterm\":\"assistant specialty chief editors\",\"frontiersdefaultterm\":\"assistant specialty chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":48,\"key\":\"associate_editor\",\"tenantterm\":\"associate editor\",\"frontiersdefaultterm\":\"associate editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":49,\"key\":\"specialty_chief_editor\",\"tenantterm\":\"specialty chief editor\",\"frontiersdefaultterm\":\"specialty chief editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":50,\"key\":\"specialty_chief_editors\",\"tenantterm\":\"specialty chief editors\",\"frontiersdefaultterm\":\"specialty chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":51,\"key\":\"chief_editor\",\"tenantterm\":\"chief editor\",\"frontiersdefaultterm\":\"chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":52,\"key\":\"chief_editors\",\"tenantterm\":\"chief editors\",\"frontiersdefaultterm\":\"chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":53,\"key\":\"call_for_participation\",\"tenantterm\":\"call for participation\",\"frontiersdefaultterm\":\"call for participation\",\"category\":\"process\"},{\"sequencenumber\":54,\"key\":\"citation\",\"tenantterm\":\"citation\",\"frontiersdefaultterm\":\"citation\",\"category\":\"misc.\"},{\"sequencenumber\":55,\"key\":\"citations\",\"tenantterm\":\"citations\",\"frontiersdefaultterm\":\"citations\",\"category\":\"misc.\"},{\"sequencenumber\":56,\"key\":\"contributor\",\"tenantterm\":\"contributor\",\"frontiersdefaultterm\":\"contributor\",\"category\":\"label (role)\"},{\"sequencenumber\":57,\"key\":\"contributors\",\"tenantterm\":\"contributors\",\"frontiersdefaultterm\":\"contributors\",\"category\":\"label (role)\"},{\"sequencenumber\":58,\"key\":\"corresponding_author\",\"tenantterm\":\"corresponding author\",\"frontiersdefaultterm\":\"corresponding author\",\"category\":\"label (role)\"},{\"sequencenumber\":59,\"key\":\"corresponding_authors\",\"tenantterm\":\"corresponding authors\",\"frontiersdefaultterm\":\"corresponding authors\",\"category\":\"label (role)\"},{\"sequencenumber\":60,\"key\":\"decline\",\"tenantterm\":\"decline\",\"frontiersdefaultterm\":\"decline\",\"category\":\"process\"},{\"sequencenumber\":61,\"key\":\"declined\",\"tenantterm\":\"declined\",\"frontiersdefaultterm\":\"declined\",\"category\":\"process\"},{\"sequencenumber\":62,\"key\":\"reject\",\"tenantterm\":\"reject\",\"frontiersdefaultterm\":\"reject\",\"category\":\"process\"},{\"sequencenumber\":63,\"key\":\"rejected\",\"tenantterm\":\"rejected\",\"frontiersdefaultterm\":\"rejected\",\"category\":\"process\"},{\"sequencenumber\":64,\"key\":\"publish\",\"tenantterm\":\"publish\",\"frontiersdefaultterm\":\"publish\",\"category\":\"core\"},{\"sequencenumber\":65,\"key\":\"published\",\"tenantterm\":\"published\",\"frontiersdefaultterm\":\"published\",\"category\":\"core\"},{\"sequencenumber\":66,\"key\":\"publication\",\"tenantterm\":\"publication\",\"frontiersdefaultterm\":\"publication\",\"category\":\"core\"},{\"sequencenumber\":67,\"key\":\"peer_review\",\"tenantterm\":\"peer review\",\"frontiersdefaultterm\":\"peer review\",\"category\":\"peer review process\"},{\"sequencenumber\":68,\"key\":\"peer_reviewed\",\"tenantterm\":\"peer reviewed\",\"frontiersdefaultterm\":\"peer reviewed\",\"category\":\"peer review process\"},{\"sequencenumber\":69,\"key\":\"initial_validation\",\"tenantterm\":\"initial validation\",\"frontiersdefaultterm\":\"initial validation\",\"category\":\"peer review process\"},{\"sequencenumber\":70,\"key\":\"editorial_assignment\",\"tenantterm\":\"editorial assignment\",\"frontiersdefaultterm\":\"editorial assignment\",\"category\":\"peer review process\"},{\"sequencenumber\":71,\"key\":\"independent_review\",\"tenantterm\":\"independent review\",\"frontiersdefaultterm\":\"independent review\",\"category\":\"peer review process\"},{\"sequencenumber\":72,\"key\":\"interactive_review\",\"tenantterm\":\"interactive review\",\"frontiersdefaultterm\":\"interactive review\",\"category\":\"peer review process\"},{\"sequencenumber\":73,\"key\":\"review\",\"tenantterm\":\"review\",\"frontiersdefaultterm\":\"review\",\"category\":\"peer review process\"},{\"sequencenumber\":74,\"key\":\"reviewing\",\"tenantterm\":\"reviewing\",\"frontiersdefaultterm\":\"reviewing\",\"category\":\"peer review process\"},{\"sequencenumber\":75,\"key\":\"reviewer\",\"tenantterm\":\"reviewer\",\"frontiersdefaultterm\":\"reviewer\",\"category\":\"label (role)\"},{\"sequencenumber\":76,\"key\":\"reviewers\",\"tenantterm\":\"reviewers\",\"frontiersdefaultterm\":\"reviewers\",\"category\":\"label (role)\"},{\"sequencenumber\":77,\"key\":\"review_finalized\",\"tenantterm\":\"review finalized\",\"frontiersdefaultterm\":\"review finalized\",\"category\":\"peer review process\"},{\"sequencenumber\":78,\"key\":\"final_decision\",\"tenantterm\":\"final decision\",\"frontiersdefaultterm\":\"final decision\",\"category\":\"peer review process\"},{\"sequencenumber\":79,\"key\":\"final_validation\",\"tenantterm\":\"final validation\",\"frontiersdefaultterm\":\"final validation\",\"category\":\"peer review process\"},{\"sequencenumber\":80,\"key\":\"ae_accept_manuscript\",\"tenantterm\":\"recommend to accept manuscript\",\"frontiersdefaultterm\":\"accept manuscript\",\"category\":\"process\"},{\"sequencenumber\":81,\"key\":\"fee\",\"tenantterm\":\"fee\",\"frontiersdefaultterm\":\"fee\",\"category\":\"accounting\"},{\"sequencenumber\":82,\"key\":\"fees\",\"tenantterm\":\"fees\",\"frontiersdefaultterm\":\"fees\",\"category\":\"accounting\"},{\"sequencenumber\":83,\"key\":\"guest_associate_editor\",\"tenantterm\":\"guest associate editor\",\"frontiersdefaultterm\":\"guest associate editor\",\"category\":\"label (role)\"},{\"sequencenumber\":84,\"key\":\"guest_associate_editors\",\"tenantterm\":\"guest associate editors\",\"frontiersdefaultterm\":\"guest associate editors\",\"category\":\"label (role)\"},{\"sequencenumber\":85,\"key\":\"in_review\",\"tenantterm\":\"in review\",\"frontiersdefaultterm\":\"in review\",\"category\":\"peer review process\"},{\"sequencenumber\":86,\"key\":\"institutional_member\",\"tenantterm\":\"institutional partner\",\"frontiersdefaultterm\":\"institutional partner\",\"category\":\"accounting\"},{\"sequencenumber\":87,\"key\":\"institutional_membership\",\"tenantterm\":\"institutional partnership\",\"frontiersdefaultterm\":\"institutional partnership\",\"category\":\"accounting\"},{\"sequencenumber\":88,\"key\":\"article_processing_charge\",\"tenantterm\":\"article processing charge\",\"frontiersdefaultterm\":\"article processing charge\",\"category\":\"accounting\"},{\"sequencenumber\":89,\"key\":\"article_processing_charges\",\"tenantterm\":\"article processing charges\",\"frontiersdefaultterm\":\"article processing charges\",\"category\":\"accounting\"},{\"sequencenumber\":90,\"key\":\"apcs\",\"tenantterm\":\"apcs\",\"frontiersdefaultterm\":\"apcs\",\"category\":\"accounting\"},{\"sequencenumber\":91,\"key\":\"apc\",\"tenantterm\":\"apc\",\"frontiersdefaultterm\":\"apc\",\"category\":\"accounting\"},{\"sequencenumber\":92,\"key\":\"received\",\"tenantterm\":\"received\",\"frontiersdefaultterm\":\"received\",\"description\":\"date manuscript was received on.\",\"category\":\"core\"},{\"sequencenumber\":93,\"key\":\"transferred\",\"tenantterm\":\"transferred\",\"frontiersdefaultterm\":\"transferred\",\"category\":\"core\"},{\"sequencenumber\":94,\"key\":\"transfer\",\"tenantterm\":\"transfer\",\"frontiersdefaultterm\":\"transfer\",\"category\":\"core\"},{\"sequencenumber\":95,\"key\":\"research_topic\",\"tenantterm\":\"research topic\",\"frontiersdefaultterm\":\"research topic\",\"category\":\"core\"},{\"sequencenumber\":96,\"key\":\"research_topics\",\"tenantterm\":\"research topics\",\"frontiersdefaultterm\":\"research topics\",\"category\":\"core\"},{\"sequencenumber\":97,\"key\":\"topic_editor\",\"tenantterm\":\"topic editor\",\"frontiersdefaultterm\":\"topic editor\",\"category\":\"label (role)\"},{\"sequencenumber\":98,\"key\":\"review_editor\",\"tenantterm\":\"community reviewer\",\"frontiersdefaultterm\":\"review editor\",\"category\":\"label (role)\"},{\"sequencenumber\":99,\"key\":\"title\",\"tenantterm\":\"title\",\"frontiersdefaultterm\":\"title\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":100,\"key\":\"running_title\",\"tenantterm\":\"running title\",\"frontiersdefaultterm\":\"running title\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":101,\"key\":\"submit\",\"tenantterm\":\"submit\",\"frontiersdefaultterm\":\"submit\",\"category\":\"process\"},{\"sequencenumber\":102,\"key\":\"submitted\",\"tenantterm\":\"submitted\",\"frontiersdefaultterm\":\"submitted\",\"category\":\"process\"},{\"sequencenumber\":103,\"key\":\"submitting\",\"tenantterm\":\"submitting\",\"frontiersdefaultterm\":\"submitting\",\"category\":\"process\"},{\"sequencenumber\":104,\"key\":\"t_e\",\"tenantterm\":\"te\",\"frontiersdefaultterm\":\"te\",\"category\":\"label (role)\"},{\"sequencenumber\":105,\"key\":\"topic\",\"tenantterm\":\"topic\",\"frontiersdefaultterm\":\"topic\",\"category\":\"process\"},{\"sequencenumber\":106,\"key\":\"topic_summary\",\"tenantterm\":\"topic summary\",\"frontiersdefaultterm\":\"topic summary\",\"category\":\"process\"},{\"sequencenumber\":107,\"key\":\"figure\",\"tenantterm\":\"figure\",\"frontiersdefaultterm\":\"figure\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":108,\"key\":\"figures\",\"tenantterm\":\"figures\",\"frontiersdefaultterm\":\"figures\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":109,\"key\":\"editorial_file\",\"tenantterm\":\"editorial file\",\"frontiersdefaultterm\":\"editorial file\",\"category\":\"core\"},{\"sequencenumber\":110,\"key\":\"editorial_files\",\"tenantterm\":\"editorial files\",\"frontiersdefaultterm\":\"editorial files\",\"category\":\"core\"},{\"sequencenumber\":111,\"key\":\"e_book\",\"tenantterm\":\"e-book\",\"frontiersdefaultterm\":\"e-book\",\"category\":\"core\"},{\"sequencenumber\":112,\"key\":\"organization\",\"tenantterm\":\"organization\",\"frontiersdefaultterm\":\"organization\",\"category\":\"core\"},{\"sequencenumber\":113,\"key\":\"institution\",\"tenantterm\":\"institution\",\"frontiersdefaultterm\":\"institution\",\"category\":\"core\"},{\"sequencenumber\":114,\"key\":\"reference\",\"tenantterm\":\"reference\",\"frontiersdefaultterm\":\"reference\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":115,\"key\":\"references\",\"tenantterm\":\"references\",\"frontiersdefaultterm\":\"references\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":116,\"key\":\"sce\",\"tenantterm\":\"sce\",\"frontiersdefaultterm\":\"sce\",\"description\":\"abbreviation for specialty chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":117,\"key\":\"submission\",\"tenantterm\":\"submission\",\"frontiersdefaultterm\":\"submission\",\"category\":\"process\"},{\"sequencenumber\":118,\"key\":\"submissions\",\"tenantterm\":\"submissions\",\"frontiersdefaultterm\":\"submissions\",\"category\":\"process\"},{\"sequencenumber\":119,\"key\":\"editing\",\"tenantterm\":\"editing\",\"frontiersdefaultterm\":\"editing\",\"category\":\"process\"},{\"sequencenumber\":120,\"key\":\"in_preparation\",\"tenantterm\":\"in preparation\",\"frontiersdefaultterm\":\"in preparation\",\"category\":\"process\"},{\"sequencenumber\":121,\"key\":\"country_region\",\"tenantterm\":\"country/region\",\"frontiersdefaultterm\":\"country/region\",\"description\":\"because of political issues, some of the country listings are actually classified as `regions` and we need to include this. however other clients may not want to do this.\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":122,\"key\":\"countries_regions\",\"tenantterm\":\"countries/regions\",\"frontiersdefaultterm\":\"countries/regions\",\"description\":\"because of political issues, some of the country listings are actually classified as `regions` and we need to include this. however other clients may not want to do this.\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":123,\"key\":\"specialty\",\"tenantterm\":\"specialty\",\"frontiersdefaultterm\":\"specialty\",\"category\":\"core\"},{\"sequencenumber\":124,\"key\":\"specialties\",\"tenantterm\":\"specialties\",\"frontiersdefaultterm\":\"specialties\",\"category\":\"core\"},{\"sequencenumber\":125,\"key\":\"associate_editors\",\"tenantterm\":\"associate editors\",\"frontiersdefaultterm\":\"associate editors\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":126,\"key\":\"reviewed\",\"tenantterm\":\"reviewed\",\"frontiersdefaultterm\":\"reviewed\",\"category\":\"peer review process\"},{\"sequencenumber\":127,\"key\":\"institutional_members\",\"tenantterm\":\"institutional partners\",\"frontiersdefaultterm\":\"institutional partners\",\"category\":\"accounting\"},{\"sequencenumber\":128,\"key\":\"institutional_memberships\",\"tenantterm\":\"institutional partnerships\",\"frontiersdefaultterm\":\"institutional partnerships\",\"category\":\"accounting\"},{\"sequencenumber\":129,\"key\":\"assistant_field_chief_editors\",\"tenantterm\":\"assistant field chief editors\",\"frontiersdefaultterm\":\"assistant field chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":130,\"key\":\"publications\",\"tenantterm\":\"publications\",\"frontiersdefaultterm\":\"publications\",\"category\":\"process\"},{\"sequencenumber\":131,\"key\":\"ae_accepted\",\"tenantterm\":\"recommended acceptance\",\"frontiersdefaultterm\":\"accepted\",\"category\":\"process\"},{\"sequencenumber\":132,\"key\":\"field_journal\",\"tenantterm\":\"field journal\",\"frontiersdefaultterm\":\"field journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":133,\"key\":\"field_journals\",\"tenantterm\":\"field journals\",\"frontiersdefaultterm\":\"field journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":134,\"key\":\"program_manager\",\"tenantterm\":\"program manager\",\"frontiersdefaultterm\":\"program manager\",\"category\":\"label (role)\"},{\"sequencenumber\":135,\"key\":\"journal_manager\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"journal manager\",\"category\":\"label (role)\"},{\"sequencenumber\":136,\"key\":\"journal_specialist\",\"tenantterm\":\"journal specialist\",\"frontiersdefaultterm\":\"journal specialist\",\"category\":\"label (role)\"},{\"sequencenumber\":137,\"key\":\"program_managers\",\"tenantterm\":\"program managers\",\"frontiersdefaultterm\":\"program managers\",\"category\":\"label (role)\"},{\"sequencenumber\":138,\"key\":\"journal_managers\",\"tenantterm\":\"journal managers\",\"frontiersdefaultterm\":\"journal managers\",\"category\":\"label (role)\"},{\"sequencenumber\":139,\"key\":\"journal_specialists\",\"tenantterm\":\"journal specialists\",\"frontiersdefaultterm\":\"journal specialists\",\"category\":\"label (role)\"},{\"sequencenumber\":140,\"key\":\"cover_letter\",\"tenantterm\":\"manuscript contribution to the field\",\"frontiersdefaultterm\":\"manuscript contribution to the field\",\"category\":\"process\"},{\"sequencenumber\":141,\"key\":\"ae_accepted_manuscript\",\"tenantterm\":\"recommended to accept manuscript\",\"frontiersdefaultterm\":\"accepted manuscript\",\"category\":\"process\"},{\"sequencenumber\":142,\"key\":\"recommend_for_rejection\",\"tenantterm\":\"recommend for rejection\",\"frontiersdefaultterm\":\"recommend for rejection\",\"category\":\"process\"},{\"sequencenumber\":143,\"key\":\"recommended_for_rejection\",\"tenantterm\":\"recommended for rejection\",\"frontiersdefaultterm\":\"recommended for rejection\",\"category\":\"process\"},{\"sequencenumber\":144,\"key\":\"ae\",\"tenantterm\":\"ae\",\"frontiersdefaultterm\":\"ae\",\"description\":\"associate editor - board member\",\"category\":\"label (role)\"},{\"sequencenumber\":145,\"key\":\"re\",\"tenantterm\":\"re\",\"frontiersdefaultterm\":\"re\",\"description\":\"review editor\",\"category\":\"label (role)\"},{\"sequencenumber\":146,\"key\":\"rev\",\"tenantterm\":\"rev\",\"frontiersdefaultterm\":\"rev\",\"description\":\"reviewer\",\"category\":\"label (role)\"},{\"sequencenumber\":147,\"key\":\"aut\",\"tenantterm\":\"aut\",\"frontiersdefaultterm\":\"aut\",\"description\":\"author\",\"category\":\"label (role)\"},{\"sequencenumber\":148,\"key\":\"coraut\",\"tenantterm\":\"coraut\",\"frontiersdefaultterm\":\"coraut\",\"description\":\"corresponding author\",\"category\":\"label (role)\"},{\"sequencenumber\":149,\"key\":\"saut\",\"tenantterm\":\"saut\",\"frontiersdefaultterm\":\"saut\",\"description\":\"submitting author\",\"category\":\"label (role)\"},{\"sequencenumber\":150,\"key\":\"coaut\",\"tenantterm\":\"coaut\",\"frontiersdefaultterm\":\"coaut\",\"description\":\"co-author\",\"category\":\"label (role)\"},{\"sequencenumber\":151,\"key\":\"tsof\",\"tenantterm\":\"tsof\",\"frontiersdefaultterm\":\"tsof\",\"description\":\"typesetter\",\"category\":\"label (role)\"},{\"sequencenumber\":152,\"key\":\"typesetting_office\",\"tenantterm\":\"typesetting office\",\"frontiersdefaultterm\":\"typesetting office\",\"category\":\"product\"},{\"sequencenumber\":153,\"key\":\"config\",\"tenantterm\":\"config\",\"frontiersdefaultterm\":\"config\",\"description\":\"configuration office role\",\"category\":\"label (role)\"},{\"sequencenumber\":154,\"key\":\"jm\",\"tenantterm\":\"jm\",\"frontiersdefaultterm\":\"jm\",\"description\":\"journal manager\",\"category\":\"label (role)\"},{\"sequencenumber\":155,\"key\":\"rte\",\"tenantterm\":\"rte\",\"frontiersdefaultterm\":\"rte\",\"description\":\"research topic editor\",\"category\":\"label (role)\"},{\"sequencenumber\":156,\"key\":\"organizations\",\"tenantterm\":\"organizations\",\"frontiersdefaultterm\":\"organizations\",\"category\":\"core\"},{\"sequencenumber\":157,\"key\":\"publishing\",\"tenantterm\":\"publishing\",\"frontiersdefaultterm\":\"publishing\",\"category\":\"core\"},{\"sequencenumber\":158,\"key\":\"acceptance\",\"tenantterm\":\"acceptance\",\"frontiersdefaultterm\":\"acceptance\",\"category\":\"process\"},{\"sequencenumber\":159,\"key\":\"preferred_associate_editor\",\"tenantterm\":\"preferred associate editor\",\"frontiersdefaultterm\":\"preferred associate editor\",\"category\":\"label (role)\"},{\"sequencenumber\":160,\"key\":\"topic_editors\",\"tenantterm\":\"topic editors\",\"frontiersdefaultterm\":\"topic editors\",\"category\":\"label (role)\"},{\"sequencenumber\":161,\"key\":\"institutions\",\"tenantterm\":\"institutions\",\"frontiersdefaultterm\":\"institutions\",\"category\":\"core\"},{\"sequencenumber\":162,\"key\":\"author(s)\",\"tenantterm\":\"author(s)\",\"frontiersdefaultterm\":\"author(s)\",\"category\":\"label (role)\"},{\"sequencenumber\":163,\"key\":\"figure(s)\",\"tenantterm\":\"figure(s)\",\"frontiersdefaultterm\":\"figure(s)\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":164,\"key\":\"co-authors\",\"tenantterm\":\"co-authors\",\"frontiersdefaultterm\":\"co-authors\",\"description\":\"co-authors\",\"category\":\"label (role)\"},{\"sequencenumber\":165,\"key\":\"editorial_board_members\",\"tenantterm\":\"editorial board members\",\"frontiersdefaultterm\":\"editorial board members\",\"description\":\"editorial board members\",\"category\":\"label (role)\"},{\"sequencenumber\":166,\"key\":\"editorial_board\",\"tenantterm\":\"editorial board\",\"frontiersdefaultterm\":\"editorial board\",\"description\":\"editorial board\",\"category\":\"product\"},{\"sequencenumber\":167,\"key\":\"co-authorship\",\"tenantterm\":\"co-authorship\",\"frontiersdefaultterm\":\"co-authorship\",\"description\":\"co-authorship\",\"category\":\"misc.\"},{\"sequencenumber\":168,\"key\":\"role_id_1\",\"tenantterm\":\"registration office\",\"frontiersdefaultterm\":\"registration office\",\"category\":\"user role\"},{\"sequencenumber\":169,\"key\":\"role_id_2\",\"tenantterm\":\"editorial office\",\"frontiersdefaultterm\":\"editorial office\",\"category\":\"user role\"},{\"sequencenumber\":170,\"key\":\"role_id_7\",\"tenantterm\":\"field chief editor\",\"frontiersdefaultterm\":\"field chief editor\",\"category\":\"user role\"},{\"sequencenumber\":171,\"key\":\"role_id_8\",\"tenantterm\":\"assistant field chief editor\",\"frontiersdefaultterm\":\"assistant field chief editor\",\"category\":\"user role\"},{\"sequencenumber\":172,\"key\":\"role_id_9\",\"tenantterm\":\"specialty chief editor\",\"frontiersdefaultterm\":\"specialty chief editor\",\"category\":\"user role\"},{\"sequencenumber\":173,\"key\":\"role_id_10\",\"tenantterm\":\"assistant specialty chief editor\",\"frontiersdefaultterm\":\"assistant specialty chief editor\",\"category\":\"user role\"},{\"sequencenumber\":174,\"key\":\"role_id_11\",\"tenantterm\":\"associate editor\",\"frontiersdefaultterm\":\"associate editor\",\"category\":\"user role\"},{\"sequencenumber\":175,\"key\":\"role_id_12\",\"tenantterm\":\"guest associate editor\",\"frontiersdefaultterm\":\"guest associate editor\",\"category\":\"user role\"},{\"sequencenumber\":176,\"key\":\"role_id_13\",\"tenantterm\":\"review editor\",\"frontiersdefaultterm\":\"review editor\",\"category\":\"user role\"},{\"sequencenumber\":177,\"key\":\"role_id_14\",\"tenantterm\":\"reviewer\",\"frontiersdefaultterm\":\"reviewer\",\"category\":\"user role\"},{\"sequencenumber\":178,\"key\":\"role_id_15\",\"tenantterm\":\"author\",\"frontiersdefaultterm\":\"author\",\"category\":\"user role\"},{\"sequencenumber\":179,\"key\":\"role_id_16\",\"tenantterm\":\"corresponding author\",\"frontiersdefaultterm\":\"corresponding author\",\"category\":\"user role\"},{\"sequencenumber\":180,\"key\":\"role_id_17\",\"tenantterm\":\"submitting author\",\"frontiersdefaultterm\":\"submitting author\",\"category\":\"user role\"},{\"sequencenumber\":181,\"key\":\"role_id_18\",\"tenantterm\":\"co-author\",\"frontiersdefaultterm\":\"co-author\",\"category\":\"user role\"},{\"sequencenumber\":182,\"key\":\"role_id_20\",\"tenantterm\":\"production office\",\"frontiersdefaultterm\":\"production office\",\"category\":\"user role\"},{\"sequencenumber\":183,\"key\":\"role_id_22\",\"tenantterm\":\"typesetting office (typesetter)\",\"frontiersdefaultterm\":\"typesetting office (typesetter)\",\"category\":\"user role\"},{\"sequencenumber\":184,\"key\":\"role_id_24\",\"tenantterm\":\"registered user\",\"frontiersdefaultterm\":\"registered user\",\"category\":\"user role\"},{\"sequencenumber\":185,\"key\":\"role_id_35\",\"tenantterm\":\"job office\",\"frontiersdefaultterm\":\"job office\",\"category\":\"user role\"},{\"sequencenumber\":186,\"key\":\"role_id_41\",\"tenantterm\":\"special event administrator\",\"frontiersdefaultterm\":\"special event administrator\",\"category\":\"user role\"},{\"sequencenumber\":187,\"key\":\"role_id_42\",\"tenantterm\":\"special event reviewer\",\"frontiersdefaultterm\":\"special event reviewer\",\"category\":\"user role\"},{\"sequencenumber\":188,\"key\":\"role_id_43\",\"tenantterm\":\"submit abstract\",\"frontiersdefaultterm\":\"submit abstract\",\"category\":\"user role\"},{\"sequencenumber\":189,\"key\":\"role_id_52\",\"tenantterm\":\"events office\",\"frontiersdefaultterm\":\"events office\",\"category\":\"user role\"},{\"sequencenumber\":190,\"key\":\"role_id_53\",\"tenantterm\":\"event administrator\",\"frontiersdefaultterm\":\"event administrator\",\"category\":\"user role\"},{\"sequencenumber\":191,\"key\":\"role_id_89\",\"tenantterm\":\"content management office\",\"frontiersdefaultterm\":\"content management office\",\"category\":\"user role\"},{\"sequencenumber\":192,\"key\":\"role_id_98\",\"tenantterm\":\"accounting office\",\"frontiersdefaultterm\":\"accounting office\",\"category\":\"user role\"},{\"sequencenumber\":193,\"key\":\"role_id_99\",\"tenantterm\":\"projects\",\"frontiersdefaultterm\":\"projects\",\"category\":\"user role\"},{\"sequencenumber\":194,\"key\":\"role_id_103\",\"tenantterm\":\"configuration office\",\"frontiersdefaultterm\":\"configuration office\",\"category\":\"user role\"},{\"sequencenumber\":195,\"key\":\"role_id_104\",\"tenantterm\":\"beta user\",\"frontiersdefaultterm\":\"beta user\",\"category\":\"user role\"},{\"sequencenumber\":196,\"key\":\"role_id_106\",\"tenantterm\":\"wfconf\",\"frontiersdefaultterm\":\"wfconf\",\"category\":\"user role\"},{\"sequencenumber\":197,\"key\":\"role_id_107\",\"tenantterm\":\"rt management beta user\",\"frontiersdefaultterm\":\"rt management beta user\",\"category\":\"user role\"},{\"sequencenumber\":198,\"key\":\"role_id_108\",\"tenantterm\":\"deo beta user\",\"frontiersdefaultterm\":\"deo beta user\",\"category\":\"user role\"},{\"sequencenumber\":199,\"key\":\"role_id_109\",\"tenantterm\":\"search beta user\",\"frontiersdefaultterm\":\"search beta user\",\"category\":\"user role\"},{\"sequencenumber\":200,\"key\":\"role_id_110\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"journal manager\",\"category\":\"user role\"},{\"sequencenumber\":201,\"key\":\"role_id_111\",\"tenantterm\":\"myfrontiers beta user\",\"frontiersdefaultterm\":\"myfrontiers beta user\",\"category\":\"user role\"},{\"sequencenumber\":202,\"key\":\"role_id_21\",\"tenantterm\":\"copy editor\",\"frontiersdefaultterm\":\"copy editor\",\"category\":\"user role\"},{\"sequencenumber\":203,\"key\":\"role_id_1_abr\",\"tenantterm\":\"rof\",\"frontiersdefaultterm\":\"rof\",\"category\":\"user role\"},{\"sequencenumber\":204,\"key\":\"role_id_2_abr\",\"tenantterm\":\"eof\",\"frontiersdefaultterm\":\"eof\",\"category\":\"user role\"},{\"sequencenumber\":205,\"key\":\"role_id_7_abr\",\"tenantterm\":\"fce\",\"frontiersdefaultterm\":\"fce\",\"category\":\"user role\"},{\"sequencenumber\":206,\"key\":\"role_id_8_abr\",\"tenantterm\":\"afce\",\"frontiersdefaultterm\":\"afce\",\"category\":\"user role\"},{\"sequencenumber\":207,\"key\":\"role_id_9_abr\",\"tenantterm\":\"sce\",\"frontiersdefaultterm\":\"sce\",\"category\":\"user role\"},{\"sequencenumber\":208,\"key\":\"role_id_10_abr\",\"tenantterm\":\"asce\",\"frontiersdefaultterm\":\"asce\",\"category\":\"user role\"},{\"sequencenumber\":209,\"key\":\"role_id_11_abr\",\"tenantterm\":\"ae\",\"frontiersdefaultterm\":\"ae\",\"category\":\"user role\"},{\"sequencenumber\":210,\"key\":\"role_id_12_abr\",\"tenantterm\":\"gae\",\"frontiersdefaultterm\":\"gae\",\"category\":\"user role\"},{\"sequencenumber\":211,\"key\":\"role_id_13_abr\",\"tenantterm\":\"re\",\"frontiersdefaultterm\":\"re\",\"category\":\"user role\"},{\"sequencenumber\":212,\"key\":\"role_id_14_abr\",\"tenantterm\":\"rev\",\"frontiersdefaultterm\":\"rev\",\"category\":\"user role\"},{\"sequencenumber\":213,\"key\":\"role_id_15_abr\",\"tenantterm\":\"aut\",\"frontiersdefaultterm\":\"aut\",\"category\":\"user role\"},{\"sequencenumber\":214,\"key\":\"role_id_16_abr\",\"tenantterm\":\"coraut\",\"frontiersdefaultterm\":\"coraut\",\"category\":\"user role\"},{\"sequencenumber\":215,\"key\":\"role_id_17_abr\",\"tenantterm\":\"saut\",\"frontiersdefaultterm\":\"saut\",\"category\":\"user role\"},{\"sequencenumber\":216,\"key\":\"role_id_18_abr\",\"tenantterm\":\"coaut\",\"frontiersdefaultterm\":\"coaut\",\"category\":\"user role\"},{\"sequencenumber\":217,\"key\":\"role_id_20_abr\",\"tenantterm\":\"pof\",\"frontiersdefaultterm\":\"pof\",\"category\":\"user role\"},{\"sequencenumber\":218,\"key\":\"role_id_22_abr\",\"tenantterm\":\"tsof\",\"frontiersdefaultterm\":\"tsof\",\"category\":\"user role\"},{\"sequencenumber\":219,\"key\":\"role_id_24_abr\",\"tenantterm\":\"ru\",\"frontiersdefaultterm\":\"ru\",\"category\":\"user role\"},{\"sequencenumber\":220,\"key\":\"role_id_35_abr\",\"tenantterm\":\"jof\",\"frontiersdefaultterm\":\"jof\",\"category\":\"user role\"},{\"sequencenumber\":221,\"key\":\"role_id_41_abr\",\"tenantterm\":\"se-adm\",\"frontiersdefaultterm\":\"se-adm\",\"category\":\"user role\"},{\"sequencenumber\":222,\"key\":\"role_id_42_abr\",\"tenantterm\":\"se-rev\",\"frontiersdefaultterm\":\"se-rev\",\"category\":\"user role\"},{\"sequencenumber\":223,\"key\":\"role_id_43_abr\",\"tenantterm\":\"se-aut\",\"frontiersdefaultterm\":\"se-aut\",\"category\":\"user role\"},{\"sequencenumber\":224,\"key\":\"role_id_52_abr\",\"tenantterm\":\"evof\",\"frontiersdefaultterm\":\"evof\",\"category\":\"user role\"},{\"sequencenumber\":225,\"key\":\"role_id_53_abr\",\"tenantterm\":\"ev-adm\",\"frontiersdefaultterm\":\"ev-adm\",\"category\":\"user role\"},{\"sequencenumber\":226,\"key\":\"role_id_89_abr\",\"tenantterm\":\"comof\",\"frontiersdefaultterm\":\"comof\",\"category\":\"user role\"},{\"sequencenumber\":227,\"key\":\"role_id_98_abr\",\"tenantterm\":\"aof\",\"frontiersdefaultterm\":\"aof\",\"category\":\"user role\"},{\"sequencenumber\":228,\"key\":\"role_id_99_abr\",\"tenantterm\":\"projects\",\"frontiersdefaultterm\":\"projects\",\"category\":\"user role\"},{\"sequencenumber\":229,\"key\":\"role_id_103_abr\",\"tenantterm\":\"config\",\"frontiersdefaultterm\":\"config\",\"category\":\"user role\"},{\"sequencenumber\":230,\"key\":\"role_id_104_abr\",\"tenantterm\":\"beta\",\"frontiersdefaultterm\":\"beta\",\"category\":\"user role\"},{\"sequencenumber\":231,\"key\":\"role_id_106_abr\",\"tenantterm\":\"wfconf\",\"frontiersdefaultterm\":\"wfconf\",\"category\":\"user role\"},{\"sequencenumber\":232,\"key\":\"role_id_107_abr\",\"tenantterm\":\"rtbeta\",\"frontiersdefaultterm\":\"rtbeta\",\"category\":\"user role\"},{\"sequencenumber\":233,\"key\":\"role_id_108_abr\",\"tenantterm\":\"deobeta\",\"frontiersdefaultterm\":\"deobeta\",\"category\":\"user role\"},{\"sequencenumber\":234,\"key\":\"role_id_109_abr\",\"tenantterm\":\"searchbeta\",\"frontiersdefaultterm\":\"searchbeta\",\"category\":\"user role\"},{\"sequencenumber\":235,\"key\":\"role_id_110_abr\",\"tenantterm\":\"jm\",\"frontiersdefaultterm\":\"jm\",\"category\":\"user role\"},{\"sequencenumber\":236,\"key\":\"role_id_111_abr\",\"tenantterm\":\"mfbeta\",\"frontiersdefaultterm\":\"mfbeta\",\"category\":\"user role\"},{\"sequencenumber\":237,\"key\":\"role_id_21_abr\",\"tenantterm\":\"coped\",\"frontiersdefaultterm\":\"coped\",\"category\":\"user role\"},{\"sequencenumber\":238,\"key\":\"reviewer_editorial_board\",\"tenantterm\":\"editorial board\",\"frontiersdefaultterm\":\"editorial board\",\"description\":\"this is the label for the review editorial board\",\"category\":\"label\"},{\"sequencenumber\":239,\"key\":\"field_chief_editor\",\"tenantterm\":\"field chief editor\",\"frontiersdefaultterm\":\"field chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":240,\"key\":\"field_chief_editors\",\"tenantterm\":\"field chief editors\",\"frontiersdefaultterm\":\"field chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":241,\"key\":\"editor\",\"tenantterm\":\"editor\",\"frontiersdefaultterm\":\"editor\",\"category\":\"label (role)\"},{\"sequencenumber\":242,\"key\":\"editors\",\"tenantterm\":\"editors\",\"frontiersdefaultterm\":\"editors\",\"category\":\"label (role)\"},{\"sequencenumber\":243,\"key\":\"board\",\"tenantterm\":\"board\",\"frontiersdefaultterm\":\"board\",\"category\":\"label\"},{\"sequencenumber\":244,\"key\":\"boards\",\"tenantterm\":\"boards\",\"frontiersdefaultterm\":\"boards\",\"category\":\"label\"},{\"sequencenumber\":245,\"key\":\"article_collection\",\"tenantterm\":\"article collection\",\"frontiersdefaultterm\":\"article collection\",\"category\":\"label\"},{\"sequencenumber\":246,\"key\":\"article_collections\",\"tenantterm\":\"article collections\",\"frontiersdefaultterm\":\"article collections\",\"category\":\"label\"},{\"sequencenumber\":247,\"key\":\"handling_editor\",\"tenantterm\":\"handling editor\",\"frontiersdefaultterm\":\"associate editor\",\"description\":\"this terminology key is for the person assigned to edit a manuscript. it is a label for the temporary handling editor assignment.\",\"category\":\"label (role)\"},{\"sequencenumber\":248,\"key\":\"handling_editors\",\"tenantterm\":\"handling editors\",\"frontiersdefaultterm\":\"associate editors\",\"description\":\"this terminology key is for the person assigned to edit a manuscript. it is a label for the temporary handling editor assignment.\",\"category\":\"label (role)\"},{\"sequencenumber\":249,\"key\":\"ae_accept\",\"tenantterm\":\"recommend acceptance\",\"frontiersdefaultterm\":\"accept\",\"category\":\"process\"},{\"sequencenumber\":250,\"key\":\"rtm\",\"tenantterm\":\"rtm\",\"frontiersdefaultterm\":\"rtm\",\"category\":\"product\"},{\"sequencenumber\":251,\"key\":\"frontiers_media_sa\",\"tenantterm\":\"frontiers media s.a\",\"frontiersdefaultterm\":\"frontiers media s.a\",\"category\":\"customer\"},{\"sequencenumber\":252,\"key\":\"review_editors\",\"tenantterm\":\"community reviewers\",\"frontiersdefaultterm\":\"review editors\",\"category\":\"label (role)\"},{\"sequencenumber\":253,\"key\":\"journal_card_chief_editor\",\"tenantterm\":\"chief editor\",\"frontiersdefaultterm\":\"chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":254,\"key\":\"journal_card_chief_editors\",\"tenantterm\":\"chief editors\",\"frontiersdefaultterm\":\"chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":255,\"key\":\"call_for_papers\",\"tenantterm\":\"call for papers\",\"frontiersdefaultterm\":\"call for papers\",\"category\":\"label\"},{\"sequencenumber\":256,\"key\":\"calls_for_papers\",\"tenantterm\":\"calls for papers\",\"frontiersdefaultterm\":\"calls for papers\",\"category\":\"label\"},{\"sequencenumber\":257,\"key\":\"supervising_editor\",\"tenantterm\":\"supervising editor\",\"frontiersdefaultterm\":\"supervising editor\",\"description\":\"a chief or assistant chief editor who is assigned to a manuscript to supervise.\",\"category\":\"role\",\"externalkey\":\"supervising_editor\"},{\"sequencenumber\":258,\"key\":\"supervising_editors\",\"tenantterm\":\"supervising editors\",\"frontiersdefaultterm\":\"supervising editors\",\"description\":\"a chief or assistant chief editor who is assigned to a manuscript to supervise.\",\"category\":\"role\",\"externalkey\":\"supervising_editors\"},{\"sequencenumber\":259,\"key\":\"reviewer_endorse\",\"tenantterm\":\"endorse\",\"frontiersdefaultterm\":\"endorse\",\"category\":\"label\"},{\"sequencenumber\":260,\"key\":\"reviewer_endorsed\",\"tenantterm\":\"endorsed\",\"frontiersdefaultterm\":\"endorsed\",\"category\":\"label\"},{\"sequencenumber\":261,\"key\":\"reviewer_endorse_publication\",\"tenantterm\":\"endorse publication\",\"frontiersdefaultterm\":\"endorse publication\",\"category\":\"label\"},{\"sequencenumber\":262,\"key\":\"reviewer_endorsed_publication\",\"tenantterm\":\"endorsed publication\",\"frontiersdefaultterm\":\"endorsed publication\",\"category\":\"label\"},{\"sequencenumber\":263,\"key\":\"editor_role\",\"tenantterm\":\"editor role\",\"frontiersdefaultterm\":\"editor role\",\"category\":\"label\"},{\"sequencenumber\":264,\"key\":\"editor_roles\",\"tenantterm\":\"editor roles\",\"frontiersdefaultterm\":\"editor roles\",\"category\":\"label\"},{\"sequencenumber\":265,\"key\":\"editorial_role\",\"tenantterm\":\"editorial role\",\"frontiersdefaultterm\":\"editorial role\",\"category\":\"label\"},{\"sequencenumber\":266,\"key\":\"editorial_roles\",\"tenantterm\":\"editorial roles\",\"frontiersdefaultterm\":\"editorial roles\",\"category\":\"label\"},{\"sequencenumber\":267,\"key\":\"call_for_paper\",\"tenantterm\":\"call for paper\",\"frontiersdefaultterm\":\"call for paper\",\"category\":\"label\"},{\"sequencenumber\":268,\"key\":\"research_topic_abstract\",\"tenantterm\":\"manuscript summary\",\"frontiersdefaultterm\":\"manuscript summary\",\"category\":\"process\"},{\"sequencenumber\":269,\"key\":\"research_topic_abstracts\",\"tenantterm\":\"manuscript summaries\",\"frontiersdefaultterm\":\"manuscript summaries\",\"category\":\"process\"},{\"sequencenumber\":270,\"key\":\"submissions_team_manager\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"content manager\",\"category\":\"process\"},{\"sequencenumber\":271,\"key\":\"submissions_team\",\"tenantterm\":\"journal team\",\"frontiersdefaultterm\":\"content team\",\"category\":\"process\"},{\"sequencenumber\":272,\"key\":\"topic_coordinator\",\"tenantterm\":\"topic coordinator\",\"frontiersdefaultterm\":\"topic coordinator\",\"category\":\"process\"},{\"sequencenumber\":273,\"key\":\"topic_coordinators\",\"tenantterm\":\"topic coordinators\",\"frontiersdefaultterm\":\"topic coordinators\",\"category\":\"process\"},{\"sequencenumber\":274,\"key\":\"frontiers_journal_team\",\"tenantterm\":\"frontiers journal team\",\"frontiersdefaultterm\":\"frontiers journal team\",\"category\":\"label (role)\"},{\"sequencenumber\":275,\"key\":\"research_topic_ic_submission\",\"tenantterm\":\"rt:ic submission\",\"frontiersdefaultterm\":\"rt:ic submission\",\"category\":\"label (role)\"},{\"sequencenumber\":276,\"key\":\"research_topic_ic_submission_participant\",\"tenantterm\":\"rt:ic submission participant\",\"frontiersdefaultterm\":\"rt:ic submission participant\",\"category\":\"label (role)\"}]}'\n",impactgtmid:"gtm-njf2ml9b",impactgtmauth:"fyo-7nodm9w37qgfms03sa",impactgtmpreview:"env-1",impactgooglemapsapikey:"aizasyctehx2eu8pwfi86gw9fi00ftcykk6ifr8",qualtricsv4tov3:"'{\"enabled\":true,\"interceptid\":\"si_er0e2ze69oz8yn4\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",qualtricsumux:"'{\"enabled\":false,\"interceptid\":\"si_89fphy3etxqbwtu\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",qualtricscontinuousbrand:"'{\"enabled\":true,\"interceptid\":\"si_9zut94ut81fcrh4\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",defaultarticletemplate:"v3"},app:{baseurl:"/",buildid:"f2d5b6d8-d01c-4534-90b9-d659f10f17ca",buildassetsdir:"ap-2024/_nuxt",cdnurl:""}}

## Introduction
walnut diseases pose a serious threat to walnut production in xinjiang. the spread of crop diseases significantly exacerbates the security risks of the walnut industry if timely prevention and control measures are not taken ( cooke, 2006 ; khan et&#xa0;al., 2021 ). as a core component of the disease prevention and control system, early precise grading plays a critical role in agricultural production management ( adaskaveg et&#xa0;al., 2009 ; chiang et&#xa0;al., 2016 ; lamichhane, 2014 ). walnut brown spot, caused primarily by the fungus ophiognomonia leptostyla , typically forms specific symptoms on organs such as leaves, flowers, and fruits. among them, leaves, as the primary carrier of plant diseases, exhibit characteristics such as lesion morphology and color changes on their surfaces, which often serve as important bases for disease grading ( ghaiwat and arora, 2014 ; weber, 1980 ; zarei et&#xa0;al., 2019 ). traditional disease identification relies on manual experience ( moragrega et&#xa0;al., 2011 ; wang et&#xa0;al., 2020 ), a method that is not only time- color system to reduce interference from light and leaf veins, and lesion edge detection using the sobel operator, ultimately achieving fast and accurate grading based on the ratio of lesion area to leaf area. jadhav and patil (2016) developed a leaf image partitioning technique based on k-means clustering and squared euclidean distance, automatically quantifying damaged leaf area through the pixel ratio of lesions to leaves for disease grading. arivazhagan et al. (2013) proposed a four-step processing system involving color conversion, green pixel removal, segmentation, and texture feature classification for leaf disease grading. however, traditional methods for acquiring disease information suffer from insufficient segmentation accuracy for plant leaf disease images with complex textures and unclear lesion boundaries, thereby resulting in poor disease severity grading performance. with the continuous advancement of computational power, deep learning has increasingly become a key technology for addressing complex lesion segmentation, primarily due to its superior capability in automatic feature extraction ( mao et&#xa0;al., 2023 ). chen et al. (2021) proposed the blsnet method based on the unet semantic segmentation network, enhancing lesion segmentation accuracy through the introduction of attention mechanisms and multi-scale feature fusion. experiments showed that its segmentation and classification accuracy outperformed benchmark models such as deeplabv3+ and unet, preliminarily verifying the reliability of this method in automatic assessment of bls disease severity. ngugi et al. (2020) developed the kijaninet segmentation network based on fully convolutional neural networks, demonstrating excellent performance in tomato leaf segmentation under complex backgrounds; lin et al. (2019) developed a cnn semantic segmentation model that achieved high-precision pixel-level segmentation of cucumber powdery mildew on leaves. additionally, some studies have fused traditional features with deep learning: tripathi (2021) proposed a convolutional neural network method based on alexnet, achieving disease grading by fusing features extracted by the model with external leaf segmentation features; ma et al. (2017) introduced comprehensive color features (ccf) combining hyper-red index, hsv/h and lab/b components, and achieved lesion segmentation via interactive region growing, with experiments confirming that this method enables accurate grading of disease images such as cucumber downy mildew under actual field conditions. however, although such methods have improved grading accuracy to some extent, they still suffer from insufficient feature capture of blurred disease edges, making it difficult to achieve fine-grained differentiation of subtle differences between disease grades ( rastogi et&#xa0;al., 2015 ). in recent years, the application of deep learning in plant disease classification has continued to expand. parashar et al. (2024) systematically validated the capability of complex feature modeling for crop yield prediction, further substantiating the critical role of adaptive feature extraction in agricultural intelligent decision-making. concurrently, vishnoi et al. (2022) enhanced small-target detection precision in apple disease identification through a spatial attention mechanism-based cnn architecture for leaf disease diagnosis. ozturk et al. (2025) constructed an ensemble learning classification model based on resnet50, mobile net, efficientnetb0, and densenet121, enhancing generalization performance through statistical cross-validation and improving decision interpretability via grad-cam visualization. experimental results showed that the ensemble model achieved stable high-precision classification performance across multiple validation rounds. these studies provide robust and interpretable solutions for intelligent plant disease recognition through technical integration and architectural innovation. hu et al. (2021) integrated retinex enhancement, faster r-cnn detection, and vgg16 classification to improve the grading and detection accuracy of blurred diseased leaves. picon et al. (2019) proposed an adaptive deep residual neural network algorithm for the classification of three european wheat diseases (septoria leaf blotch, brown spot, and rust) in real-world scenarios, which effectively improved the classification accuracy of wheat diseases. kim and ahn (2021) employed deep cnn architectures such as resnet, xception, and densenet, combined with transfer learning and fine-tuning, to classify 9 categories of pests, diseases, and healthy states in tomatoes. although densenet combined with the rmsprop algorithm achieved an accuracy of 98.63%, single cnn architectures have clear bottlenecks in handling complex plant lesions and overlapping multiple diseases, including insufficient specificity in feature extraction and limited ability to distinguish subtle differences between lesions. shi et al. (2023) analyzed the application of cnns in evaluating the severity of plant diseases, pointing out that hybrid architectures such as classical cnns, improved cnns, and segmentation networks have limitations in handling complex plant lesions: classification errors caused by concurrent multiple diseases, as well as constraints on model generalization ability due to unbalanced datasets and insufficient annotation accuracy. these issues significantly restrict their precise application in practical agricultural scenarios. although significant advancements have been made in plant lesion segmentation and disease classification using deep learning, the grading task for walnut leaf brown spot disease&#x2014;characterized by blurred edge representation and complex small lesions&#x2014;still faces notable challenges. current mobilevit-based hybrid models fall into two categories: general architectures (mobilevitv1/v2/v3) focus on optimizing the cnn-transformer balance for natural images; domain-improved models (mobile-former/edgevit) are oriented toward medical/industrial tasks, with their task focus on localization rather than grading, which mismatches the pain points and fails to address the blurriness of agricultural lesions, tissue interference, and morphological variations. thus, the present study proposes a novel cogfuse-mobilevit model specifically designed for disease grading, with its specific contributions as follows: 1. a diverse field-collected walnut leaf dataset was constructed, with images divided into four distinct severity levels based on qualitative assessment of disease severity, ensuring the accuracy of disease severity grading. 2. the hierarchical feature selection module (hfsm) enhances the fusion of local details (such as lesion texture and color) with global context through local and global attention mechanisms and task-driven feature selection, while suppressing interference from healthy regions. 3. the edge convolution fusion module (ecfm) strengthens edge details through sobel convolution and residual connections, achieving effective integration of edge-specific features with general features, further enhancing edge details and enabling more precise capture of lesion contour details. 4. the adaptive multi-scale dilated dense inception convolution module (amsddicm) extracts differentiated features using multi-shaped convolution kernels and adaptively fuses multi-scale information via a dynamic weight mechanism, capturing lesion morphologies at different pathogenesis stages. 2 materials and methods 2.1 characteristics of walnut leaf brown spot and classification of disease severity levels in the local standard technical regulations for prevention and control of walnut brown spot ( mcgranahan and leslie, 1991 ), walnut brown spot is divided into four severity levels. the severity of plant leaf diseases is generally determined using the spot coverage method. in this study, walnut leaves with different disease severities were processed to separate disease-infected spots from healthy leaf areas, and the percentage of lesion area to total leaf area was calculated. with reference to the local standard, the disease grades specified in the standard were re-determined through calculations in this study, as shown in table&#xa0;1 . classification of different severity levels of walnut leaf brown spot was conducted in accordance with technical regulations for prevention and control of walnut brown spot ( wang et&#xa0;al., 2022 ; yang et&#xa0;al., 2021 ). table&#xa0;1 table&#xa0;1 . walnut leaf brown spot disease severity grading criteria. walnut leaf brown spot is caused by infection with the fungus ophiognomonia leptostyla ( mircetich et&#xa0;al., 1980 ). in the early infection stage, near-circular or irregular small spots appear on the leaves, with a gray-brown center and dark yellow-green to purplish-brown edges, and diseased leaves tend to fall off prematurely. in the middle stage, elongated elliptical or irregular slightly sunken dark brown lesions form, with larger spot size and light brown edges; longitudinal cracks are often present in the center of the lesions. in the late stage, lesions often coalesce to form large scorched necrotic areas, surrounded by yellow to golden-yellow zones, and small black granules (conidiomata and conidia of the pathogen) are scattered on the surface of the diseased tissue ( mircetich et&#xa0;al., 1980 ). according to the degree of color and texture feature changes in infected leaves, walnut leaf disease is divided into four stages: healthy, early, middle, and late stages, corresponding to disease severity levels: healthy (level 0), mild (level 1), moderate (level 2), and severe (level 3). the different severity levels of walnut leaf brown spot are shown in figure&#xa0;1 . figure&#xa0;1 figure&#xa0;1 . walnut leaves infected with brown spot disease at different severity levels. (a) healthy leaves; (b) mildly infected leaves; (c) moderately infected leaves; (d) severely infected leaves. table&#xa0;1 shows the ratio of walnut leaf brown spot lesion area to total leaf area. level 0 (healthy): healthy leaves without symptoms. level 1 (mild): near-circular or irregular small white spots appear on leaves, accounting for less than 5% of the leaf area. level 2 (moderate): lesions expand into irregular dark brown spots, covering 5% to 30% of the leaf area. level 3 (severe): abundant lesion coalesce to form large scorched necrotic areas, exceeding 30% of the leaf area. 2.2 calculation algorithm for walnut leaf brown spot disease severity levels after capturing walnut leaf brown spot samples using a camera, to minimize subjective bias, this study strictly adhered to the objective quantitative criteria defined in the technical regulations for the control of walnut brown leaf spot ( table&#xa0;1 ), utilizing the lesion area percentage (k) as the primary grading indicator. we re-determined the severity levels through computational analysis, with the specific determination process shown in figure&#xa0;2 . figure&#xa0;2 figure&#xa0;2 . computational method for different severity levels of brown spot infection in walnut leaves. first, a python-based color segmentation algorithm was employed to convert images from the bgr color space to the hsv color space for processing. the hsv color space offers inherent advantages in color segmentation tasks. by analyzing the hue, saturation, and value characteristics of diseased regions, the color threshold range in the hsv space was determined ( chaudhary et&#xa0;al., 2012 ). after generating an initial disease region mask based on this threshold range, morphological operations such as closing and opening were applied to optimize mask quality, effectively eliminating noise while preserving the integrity of lesion contours. subsequently, the original image was converted to grayscale mode, and the leaf region was extracted using an adaptive threshold binarization algorithm. leaf boundaries were localized via contour detection technology, and a complete leaf mask was generated through contour filling. pixel statistical analysis was performed on both the leaf mask and disease mask to calculate the total leaf area and diseased region area, respectively. when a valid leaf area (&gt;0) was detected, the disease severity index was calculated using the following formula: disease severity index ( % ) = s 1 s 2 &#xd7; 100 % &#xa0;&#xa0;&#xa0;s 1 : total area of diseased regions s 2 : total area of the complete leaf. 2.3 dataset construction data were collected from the walnut plantation base of tarim university (alar, xinjiang) between may and october 2024. leaves from three locally dominant early-fruiting cultivars (&#x2018;wen 185&#x2019;, &#x2018;xinxin 2&#x2019;, &#x2018;zha 343&#x2019;)widely cultivated in southern xinjiang were vertically photographed using an iphone 13. this genotypic diversity ensures model generalizability by encompassing varied disease phenotypes. the orchard follows standardized cultivation with 4m plant spacing, 5m row spacing (&#x2248;50 plants/mu), and conventional management practices. sampled trees were in full fruiting stage. to capture disease traits across microenvironments, samples were collected from safely accessible crown layers. the dataset includes healthy and brown spot-infected leaves three severity levels, spanning varied time periods, light conditions, and angles. after removing duplicates and invalid images,5,120 high-quality jpgs were retained for analysis. 2.3.1 development of the walnut leaf brown spot dataset disease severity grading was established through quantitative lesion area analysis ( figure&#xa0;2 ). a representative subset of 512 images (10% of the full 5,120-image dataset) was selected via stratified random sampling, accurately preserving the original severity distribution. to ensure labeling rigor, two plant protection specialists (&gt;5 years&#x2019; expertise in walnut pathology, certified in local protocols) executed standardized annotation: pre-annotation training unified diagnostic criteria for ambiguous cases, with formal annotation commencing only after achieving pre-calibration inter-rater agreement (kappa coefficient &#x2265;0.75). as validated in table&#xa0;2 , dual statistical metrics confirmed gold-standard reliability&#x2014;inter-rater cohen&#x2019;s kappa reached 0.71, meeting landis &amp; koch&#x2019;s &#x201c;substantial agreement&#x201d; threshold; algorithm-expert consensus attained a weighted fleiss&#x2019; kappa of 0.77, substantially outperforming conventional cohen&#x2019;s kappa in multi-annotator scenarios. a three-stage standardization pipeline eliminated preprocessing discrepancies. the final dataset, partitioned into training/testing sets (8:2 ratio), strictly adheres to plant disease survey protocols and provides benchmarked data for deep learning model development ( table&#xa0;3 ). table&#xa0;2 table&#xa0;2 . algorithm vs. expert consensus agreement evaluation. table&#xa0;3 table&#xa0;3 . walnut leaf brown spot disease image acquisition data. 2.4 construction of walnut leaf brown spot disease severity grading model 2.4.1 the optimized mobilevitv3 network: cogfuse-mobilevit to address the challenge of difficult feature capture for small lesions in walnut leaf brown spot severity grading, this study proposes an innovative model, cogfuse-mobilevit. mobilevitv3 was selected as the backbone network due to its suitability for walnut brown spot grading. its hybrid mobilenet-vit architecture integrates cnn-based local feature extraction essential for lesion detail capture with transformer-enabled global contextual modeling critical for analyzing lesion spatial distribution. this design aligns with pathological requirements for severity grading, necessitating concurrent attention to local lesion characteristics and global infection patterns. the lightweight architecture further supports real-time processing on embedded devices, enabling future field deployment. the model embodies a &#x201c;grading task-driven&#x201d; design principle. conceptually, it establishes a disease-specific framework of &#x201c;hierarchical screening to edge enhancement to dynamic fusion.&#x201d; this framework elevates general feature extraction to targeted solutions addressing three major agricultural challenges: suppressing interference from healthy tissues via hierarchical screening resolving feature confusion; strengthening blurred lesion contours through edge enhancement overcoming the bottleneck of edge blurriness; adaptively handling multi-stage morphological variations using dynamic fusion addressing morphological variation challenges. structurally, as illustrated in figure&#xa0;3 , the network implements a progressive feature extraction strategy. the hierarchical feature selection module (hfsm) first fuses shallow and middle-layer features. it employs a hierarchical attention mechanism to enhance semantic consistency while preserving spatial details. the edge convolution fusion module (ecfm) then processes these features. it utilizes learnable sobel operators to extract edge information, which is fused with conventional convolutional features via residual connections to augment edge perception capability. finally, the adaptive multi-scale dilated inception convolution module (amsddicm) enables adaptive processing of multi-scale edge features. this module is capable of both capturing fine edge changes in minute lesions and grasping the overall contour structure of large lesions, thereby comprehensively covering edge features across different developmental stages. based on these enhanced edge features, the network accurately outputs the final disease severity grade. figure&#xa0;3 figure&#xa0;3 . cogfuse-mobilevit architecture diagram (hfsm suppresses healthy regions through dynamic masks, ecfm explicitly extracts edges via sobel convolution, and amsddicm fuses multi-scale features through dynamic weights). 2.4.2 the hierarchical feature selection module the hfsm (hierarchical feature selection module) is an innovative architecture proposed in this study. unlike mobilevit&#x2019;s direct feature concatenation, hfsm utilizes learnable prompt vectors ( equation 1 ) to generate spatial masks, dynamically suppressing non-lesion regions. such task-driven selection is crucial for tiny objects. in the grading task of walnut leaf brown spot disease, this module utilizes a local attention mechanism to focus on micro-regions of walnut leaves, fusing shallow and middle-layer features. meanwhile, the hierarchical feature selection mechanism enables precise capture of local detail features such as lesion texture and color, effectively suppressing interference from healthy leaf regions and enhancing disease features. this provides high-discriminative feature representations for brown spot disease grading while preparing for subsequent lesion edge processing. as depicted in figure&#xa0;4 , the module processes two hierarchical feature maps. initial 1&#xd7;1 convolutions reduce both maps&#x2019; channels to half the output dimension curtailing computational load. one reduced map undergoes bilinear upsampling for spatial alignment with the other. these aligned features are summed and processed by a 3&#xd7;3 convolution to generate base-path features. simultaneously, the original reduced map and upsampled map are fed into dual local-global attention modules for parallel processing ( xu, 2024 ). each group contains local and global branches. during branch processing, the feature map is partitioned into p&#xd7;p non-overlapping patches p i , j via the unfold operation. after calculating the mean of pixel features within each patch, the result is processed using the following core formula: attention distribution generation. figure&#xa0;4 figure&#xa0;4 . architecture diagram of the hierarchical feature selection module (hfsm). z i , j = ml p 2 ( layernor ( ml p 1 ( mean ( p i , j ) ) ) ) ( 1 ) a i , j = softmax ( z i , j ) ( 2 ) equations 1 and 2 convert the mean value of patch features into a high-dimensional feature vector through multi-layer perceptrons (mlp) and layer normalization (layernorm) (wherein( mlp 1 &#xa0;and&#xa0;&#xa0;mlp 2 ) achieve&#xa0;dimensional&#xa0;transformation&#xa0;p 2 &#x2192; ouc / 2 &#x2192; ouc / 2 )), and then generates the attention distribution via the softmax function a i , j . this mechanism enables the model to focus on the key features of lesion regions, weaken the interference from healthy leaf areas, enhance the specificity of feature selection, and provide high-quality features for subsequent precise grading. after the local and global features output by the two modules are spliced along the channel dimension, they are combined with the basic path features to generate the output features of the final processing module. for leaf images with over 80% healthy tissue, dynamic masks are generated via learnable prompt vectors to suppress green texture features while enhancing the gray-brown features of lesions ( mcgranahan and leslie, 1991 ). 2.4.3 ecfm edge convolutional fusion module in the walnut leaf brown spot grading task, lesion edge features are crucial for accurately determining disease severity. the standard mobilevit relies on cnn-transformer blocks to implicitly learn edges, whereas the ecfm (edge convolution fusion module) incorporates sobel convolutions, conventional convolutions, and residual connections, effectively fusing edge features with general features and thereby enhancing edge details. as shown in figure&#xa0;5 , the sobel branch employs sobel convolution to extract image edge information, accurately capturing the contour details of brown spot lesions; the convolutional branch captures general features such as leaf color and texture through standard convolution. the two realize feature fusion via the following core formula ( equation 3 ): figure&#xa0;5 figure&#xa0;5 . architecture diagram of the edge convolutional fusion module (ecfm). s = sobelconv ( x ) &#xa0; ( 3 ) processing the input feature map ( x ) through the sobel convolution operator (sobelconv) specifically extracts edge information of lesions (such as the boundary contours between lesions and healthy tissues, and the edge textures of small lesions). for the commonly seen blurred edges in brown spot disease, sobel convolution can enhance edge gradient changes, making the originally blurred lesion contours clearer, thus providing key edge feature support for subsequent grading. conventional feature extraction and fusion ( equation 4 ): &#xa0; c = conv ( x ) , &#xa0; f concat = concat ( s , c ) ( 4 ) &#xa0;c = conv ( x ) extracting the overall features of leaves (such as the color distribution of lesions and the overall texture of leaves) through standard convolution forms a complement to edge features, preventing the model from focusing only on local edges while ignoring global lesion information. f concat = concat ( s , c ) concatenating the edge feature s and the conventional feature c along the channel dimension achieves the initial fusion of edge details and global features. this fusion enables the model to both identify the fine contours of lesions and judge the disease condition by combining the overall color and texture changes of lesions, thereby improving the accuracy of grading. the module also introduces feature addition and subsequent convolution operations: the fused features are first processed by the first convolution layer f 1 = conv 1 ( f concat ) , the corresponding operation results are added to the original input, and the final output is generated through the second convolution layer f final = conv 2 ( f 1 + x ) . in this process, the residual connection retains the original feature information, further strengthens the integration of edge features and conventional features, and ensures the effective transmission and enhancement of multi-level features. the edge gradient information extracted by sobel convolution (such as grayscale differences between diseased spots and healthy tissues) and the texture features from conventional convolution (such as the roughness of necrotic areas) are fused via residual connections. this not only preserves the blurred edges of early-stage lesions but also prevents edge features from being disconnected from the overall texture ( ghaiwat and arora, 2014 ). 2.4.4 amsddicm adaptive multi-scale dilated depthwise inseparable convolution module this module differs from mobilevit&#x2019;s single-scale convolution, leverages multi-shaped convolution kernels to accurately extract lesion features of circular, irregular, and other shapes from multiple dimensions, enabling the identification of subtle differences in lesion edges and internal colors to enrich feature dimensions. meanwhile, depth wise separable convolutions are employed to accommodate lesion scales at different stages of disease development. specifically, the input features are first split into two groups, each entering the amsddicm module to extract features via multi-scale depth wise separable convolutions. a dynamic weighting mechanism (global pooling + convolution + softmax) is used to generate weights for fusing multi-scale features. the processed features from the two groups are concatenated, and finally, cross-channel fusion is completed via 1&#xd7;1 convolution to output the final optimized features. the entire process integrates multi-scale convolution, dynamic weight allocation, and feature fusion to enhance the model&#x2019;s ability to capture complex features, as shown in figure&#xa0;6 . in response to the morphological differences between early-stage small spots (1-3mm in diameter) and late-stage fused spots (over 20mm in diameter), the dynamic weight mechanism enables the model to adaptively allocate the contributions of 3&#xd7;3 kernels for capturing local textures and 11&#xd7;1 kernels for extracting strip-shaped spreading features. this addresses the limitation of fixed weights in traditional inception architectures in adapting to multi-scale morphologies. figure&#xa0;6 figure&#xa0;6 . architecture diagram of the adaptive multi-scale dilated depth wise inseparable convolution module (amsddicm). when the input feature tensor x with the number of channels c enters the amsddicm module, it first undergoes a feature grouping operation: the input features are evenly split into two groups along the channel dimension, with each group containing half of the original number of channels (c//2). this grouping strategy not only reduces computational complexity but also creates conditions for subsequent multi-scale feature extraction. each feature group is fed into an independent dmsconv2d module, which adopts different configurations such as 3&#xd7;3, 5&#xd7;5 square convolution kernels and 1&#xd7;11, 11&#xd7;1 strip-shaped convolution kernels &#x2014; the square kernels capture local textures, while the strip-shaped kernels extract direction-sensitive long-range dependencies. this dynamic weight allocation draws inspiration from adaptive strategies in agricultural iot systems. similar to how salinity-aware ets models prioritize soil conductivity features under salt stress ( zhang et&#xa0;al., 2023 ), our mechanism selectively amplifies lesion morphological features critical for severity grading. the dynamic weighting mechanism of dmsconv2d generates weights through global average pooling and 1&#xd7;1 convolution, with the core formulas as follows: weighted basic feature generation: x dkw = rearrange ( conv 1 &#xd7; 1 ( avgpoo l2 d ( x ) ) ) ( 5 ) equation 5 compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information (such as the overall distribution characteristics of lesions at different scales), and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization: x dkw = f . softmax ( x dkw ) ( 6 ) equation 6 compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information, such as the overall distribution characteristics of lesions at different scales, and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization: x = &#x2211; i = 0 2 ( dwcon v i ( x ) &#xd7; x dkw , i ) ( 7 ) equation 7 is based on dynamic weights x dkw , i ) for different convolution kernels &#xa0;dwconv i ( x ) weighted fusion is performed on the extracted features. for walnut brown spot lesions exhibiting size heterogeneity and morphological complexity&#x2014;ranging from early-stage circular micro-lesions to late-stage coalesced irregular lesions&#x2014;this mechanism adaptively modulates multi-scale feature contributions. it prioritizes retention of morphology-discriminative features, local textures in small lesions or directional distributions in large lesions, thereby comprehensively capturing pathological characteristics across developmental stages. the processed features undergo channel-wise concatenation to generate optimized outputs ( mircetich et&#xa0;al., 1980 ). 2.5 experimental process for severity grading of walnut leaf brown spot disease figure&#xa0;7 is the overall flow chart of severity grading of walnut leaf spot disease. first is the image acquisition stage, where raw images of walnut leaves are captured and collected to construct an image database containing pictures to be classified ( singh et&#xa0;al., 2019 ). next is the image preprocessing stage, which involves sequentially calculating and classifying disease severity levels, establishing image labels, and performing image preprocessing operations. subsequently, the processed data are used to train the constructed dataset. finally, in the model training and performance evaluation stage, the preprocessed images are input into the model for classifying walnut leaf brown spot disease, after which performance evaluation is conducted on the model&#x2019;s classification results to determine the model&#x2019;s effectiveness and accuracy. figure&#xa0;7 figure&#xa0;7 . overall flow chart for severity grading of walnut leaf brown spot disease. 2.6 experimental parameters and evaluation metrics 2.6.1 test environment and hyperparameter setting the experimental setup of this study is based on deep learning technology and leverages high-performance computing resources. all experiments were conducted on the windows 10 operating system. the hardware platform consists of an amd ryzen 7 3700x processor and an nvidia rtx 2080ti graphics card. the software environment was built using python 3.8, the pytorch 1.13.0 deep learning framework, and the cuda 11.3 parallel computing platform. additionally, pytorch 1.13.0&#x2014;a widely adopted open-source deep learning library renowned for its high flexibility&#x2014;was selected, making it well-suited for research and development. during model training, multiple adjustments were made to the hyperparameters to compare test results and select the optimal hyperparameter combination. the model accepts input images sized at 224&#xd7;224 pixels, with a configured batch size of 32 during training and a maximum of 100 training epochs. the optimizer employs an initial learning rate of 0.003. 2.6.2 evaluation metrics this paper aims to evaluate the performance of the model and verify the effectiveness of the improvement measures. we selected multiple evaluation metrics, and all subsequent experimental results adopt the method of calculating averages via 5-fold cross-validation, aiming to comprehensively and reliably evaluate the model performance. including precision (p), recall (r), etc. ( equations 8 &#x2013; 16 ) these metrics can be calculated using the following formulas. the arithmetic mean of the metric values across the five folds: m &#xaf; = 1 5 &#x2211; i = 1 5 m i ( 8 ) precision = tp / ( tp + fp ) ( 9 ) recall = tp / ( tp + fn ) ( 10 ) f 1 score = 2 tp 2 tp + fp + fn ( 11 ) macro&#xa0;precision= 1 c &#x2211; i = 1 c p i ( 12 ) macro&#xa0;recall= 1 c &#x2211; i = 1 c r i ( 13 ) weighted&#xa0;avg&#xa0;precision= &#x2211; ( n i &#xb7; precisio n i ) &#x2211; n i ( 14 ) weighted&#xa0;avg&#xa0;recall= &#x2211; ( n i &#xb7; precisio n i ) &#x2211; n i ( 15 ) weighted&#xa0;avg&#xa0;recall= &#x2211; i = 1 c t p i n ( 16 ) 3 experiments and results analysis 3.1 core module design and validity experimental verification 3.1.1 comparative test of necessity of hfsm module to validate the necessity of the hierarchical feature selection module (hfsm) within the cogfuse-mobilevit framework, this study conducted comparative tests against two mainstream lightweight attention modules: se (squeeze-and-excitation) and cbam (convolutional block attention module). as illustrated in table&#xa0;4 , the hfsm module achieves a significantly higher accuracy of 86.61%, outperforming the se module and cbam module by 7.38% and 4.46%, respectively. this demonstrates that its hierarchical feature selection mechanism more effectively captures discriminative features. in terms of computational efficiency, the parameters and flops of hfsm remain comparable with those of the se module and cbam module, indicating that its performance breakthrough stems from innovative structural design under equivalent lightweight constraints. comprehensive results confirm that replacing hfsm with se or cbam modules would incur a performance degradation exceeding 4%, thereby validating the indispensable value of hfsm in enabling efficient feature selection within the cogfuse-mobilevit framework. table&#xa0;4 table&#xa0;4 . performance comparison of attention modules within the cogfuse-mobilevit framework. 3.1.2 convolutional kernel selection in amsddicm rigorous validation via kernel combination ablation studies ( table&#xa0;5 ) demonstrates. the hybrid configuration (3&#xd7;3 + 5&#xd7;5 + 1&#xd7;11) significantly outperformed square-only kernels (3&#xd7;3 + 5&#xd7;5) accuracy 86.61% and 82.15% stage-specific recall gains. mid-stage lesions (level 2) raise 9.4 percentage points. late-stage lesions (level 3) raise 6.1 percentage points. this empirically validates the necessity of 1&#xd7;11 rectangular kernels for capturing linear pathological features, overcoming the limitation of isotropic kernels in detecting anisotropic structures. table&#xa0;5 table&#xa0;5 . comparison of different convolution kernels in amsddicm. 3.1.3 the impact of new modules on computational complexity figure&#xa0;8 shows that the hfsm module incurs a 169% flops increase to achieve a breakthrough improvement in early-stage lesion detection&#x2014;reducing level 0/1 misclassification by 24%, thereby establishing the pathological foundation for severity grading. the ecfm module contributes a mere 3% flops increment yet drives a 3.68 percentage point accuracy gain through enhanced edge feature representation. with only a 0.028g flops overhead 1.3%, the amsddicm module enables adaptive fusion of multiscale pathological deformations, culminating in a 7.80-pp accuracy leap. these modules form a cascaded optimization paradigm: hfsm&#x2019;s substantial cost resolves the core pathological bottleneck, while subsequent modules deliver superlinear returns&#x2014;harvesting 6.85-pp accuracy gain with just 14% additional flops&#x2014;collectively establishing the globally optimal computation-performance equilibrium. figure&#xa0;8 figure&#xa0;8 . comparison of the impact of each newly added module on computational complexity (a) accuracy (%); (b) flops (g); (c) params (m). measured on nvidia rtx 2080ti gpu (pytorch 1.13.0, cuda 11.3) with 224&#xd7;224 input. 3.1.4 comparison of the influence of different module fusion on model performance to validate the effect of module fusion, table&#xa0;6 compares model performances with different combinations. when only hfsm (hierarchical feature selection module) is introduced, precision increases from the baseline of 82.23% to 83.77%, but recall decreases by 3 percentage points to 72.31%, causing the f1 score to slightly decline to 78.04%. accuracy rises to 82.15%, indicating improved overall classification correctness of the model, but with potential risk of missed detections. table&#xa0;6 table&#xa0;6 . impact of fusion of different modules on model performance. when hfsm and ecfm (edge-context feature module) are synergistically introduced, precision increases to 86.34%, recall recovers to 74.69%, and both f1 score and accuracy are significantly optimized. hfsm fuses shallow and middle-layer features through a hierarchical attention mechanism, laying the foundation for subsequent edge feature processing; ecfm enhances lesion edge features. the combination of the two effectively improves the integrity of feature representation and the clarity of lesion boundaries. the combination of hfsm and amsddicm (adaptive multi-scale dilated depthwise inseparable convolution module) further pushes recall to 76.24%, accuracy to 83.94%, and f1 score to 80.79%, outperforming the hfsm+ecfm combination. amsddicm compensates for hfsm&#x2019;s deficiency in local feature refinement through attention-guided multi-scale detail fusion, which integrates multi-layer detail feature weights, especially suitable for scenarios with small targets or blurred features. when the three modules work synergistically, all indicators reach optimal levels: precision, recall, f1 score, and accuracy increase by 12.19%, 7.67%, 9.62%, and 10.00% respectively compared with the baseline. among them, hfsm lays the foundation for cross-layer feature fusion, ecfm effectively integrates edge-specific features with general features to enhance image edge information, and amsddicm adaptively fuses multi-scale and multi-type features through an attention mechanism, forming a progressive optimization chain from &#x201c;hierarchical feature extraction&#x201d; to &#x201c;edge semantic enhancement&#x201d; and then to &#x201c;multi-layer detail fusion&#x201d;. the experimental results show a balanced improvement in both precision and recall, indicating that the fusion of the three modules enables the model to focus more on learning the features of small brown spot lesions, thereby improving its classification performance. this synergistic task-specific design, combining hierarchical selection, explicit edge enhancement, and adaptive multi-scale fusion, fundamentally distinguishes cogfuse-mobilevit from prior mobilevit hybrids designed for general vision or localization tasks. 3.1.5 influence of different module combinations on f1-score of level (0-3) in order to verify the targeted improvement of each module for a specific disease level, we conducted the following comparative experiments as shown in the table&#xa0;6 , when hfsm is introduced alone, the f1-score of level 0 increases from 83.10% to 85.60%, effectively reducing feature confusion between healthy leaves and early-stage lesions. after adding ecfm, the f1-score of level 1 rises from 73.20% (with hfsm alone) to 79.80%, significantly mitigating the recognition bias caused by blurred edges of small lesions. amsddicm increases the f1-score of level 3 from 80.37% to 85.54%, enhancing the adaptability to the morphology of large-scale fused scorched areas. with the collaboration of the three modules, the f1-scores of level 0&#x2013;3 reach 93.99%, 82.57%, 84.28%, and 85.54% respectively, achieving balanced optimization of performance across all levels. 3.2 results comparison of different algorithms and statistical significance verification 3.2.1 comparison of grading results for different classification models to verify the effectiveness of the cogfuse-mobilevit model, we selected 9 commonly used classification models to compare with the optimized cogfuse-mobilevit model, and the results are the average of 5-fold cross-validation over 120 epochs. as shown in table&#xa0;7 , the proposed cogfuse-mobilevit model achieved the highest grading performance in terms of precision, recall, and f1-score for identifying walnut leaf brown spot disease at different severity levels among all compared models. the improved cogfuse-mobilevit model exhibits an accuracy of 86.61%, representing a 7.80-percentage-point improvement over the original model. overall, the experimental results highlight that the enhanced cogfuse-mobilevit model is more conducive to focusing on lesion edge details and accurately learning the features of different severity levels of walnut leaf brown spot disease, thereby improving the model&#x2019;s classification performance. table&#xa0;7 table&#xa0;7 . comparison of grading results for different classification models. 3.2.2 comparison of performance and reliability validation of different algorithms in model training, to quantify the reliability of results and avoid random biases from single experiments, this study employs 5-fold cross-validation to generate 95% confidence intervals, as shown in the figure&#xa0;9 . the results demonstrate that cogfuse-mobilevit takes a significant lead with an accuracy of 86.61% (95% ci: [85.24%, 87.89%]). its narrowest confidence interval range indicates the strongest generalization capability. among the comparative models, mobilevitv3 has a 95% ci of [77.20%, 80.42%]; its lower bound is significantly lower than that of cogfuse-mobilevit, confirming that the 7.8% performance improvement is not due to random fluctuations. furthermore, the confidence intervals constructed through 5-fold cross-validation further highlight the reliability of the experimental results. figure&#xa0;9 figure&#xa0;9 . confidence intervals of accuracy for different classification models in 5-fold cross-validation. 3.2.3 statistical significance verification of model improvement to statistically evaluate the performance improvement of cogfuse-mobilevit over the baseline mobilevitv3, an independent samples t-test was conducted ( table&#xa0;8 ). for each model architecture, five independent training runs. the null hypothesis stated that the mean accuracy of cogfuse-mobilevit was equal to that of mobilevitv3 h 0 : &#x3bc; c og = &#x3bc; m o b ), while the alternative hypothesis stated that cogfuse-mobilevit had a higher mean accuracy ( h 1 : &#x3bc; c og &gt; &#x3bc; m o b )cogfuse-mobilevit achieved a mean accuracy, significantly outperforming mobilevitv3. the independent samples t-test confirmed this improvement (t(8)=18.92,p=3.7&#xd7;10 &#x2212;8 ). table&#xa0;8 table&#xa0;8 . comparison of accuracy results between mobilevitv3 and cogfuse-mobilevit using independent samples t-test. under 5-fold cross-validation ( figure&#xa0;10 ), cogfuse-mobilevit achieves rapid convergence within the first 20 epochs, demonstrating more efficient feature learning capability. upon entering the steady-state phase, its validation loss is reduced by over 5-fold compared to the baseline mobilevitv3, directly confirming smaller prediction errors and superior generalization capability. meanwhile, the smooth and minimally fluctuating loss curve of cogfuse-mobilevit reflects strong robustness against data noise and distribution variations. combined with the previous statistical findings from accuracy confidence intervals and independent t-tests, these results collectively validate the statistical significance of the improved model. figure&#xa0;10 figure&#xa0;10 . comparison of loss curves between the cogfuse-mobilevit model and the original model in 5-fold cross-validation within 120 epochs. 3.3 model result analysis performance comparison of different models figure&#xa0;11a shows that in the walnut leaf brown spot disease severity grading task, the classification accuracy of all models gradually improved and stabilized with the increase of training epochs, indicating that the models continuously optimized during learning until convergence. cogfuse-mobilevit stood out prominently: it achieved rapid accuracy improvement, reached a high level in relatively early training epochs with minimal subsequent fluctuations, and stably maintained the highest accuracy, demonstrating fast convergence and strong generalization ability to efficiently extract features distinguishing different disease levels. densenet also maintained high accuracy in the late training stage, but its accuracy improvement was slower, with slightly inferior convergence speed and final stability compared to cogfuse-mobilevit. models like resnet and regnet showed limited accuracy gains with gentle upward trends, and their final stable accuracy values were significantly lower, reflecting insufficient feature extraction capabilities possibly due to network architecture or parameter optimization efficiency. figure&#xa0;11 figure&#xa0;11 . comparison of accuracy and loss across different models (a) accuracy line chart; (b) loss line chart. in figure&#xa0;11b , all models showed high initial loss values that dropped rapidly and then stabilized, following the typical learning process of iterative optimization. cogfuse-mobilevit achieved significant early loss decline and stabilized near the minimum, indicating efficient feature learning and strong fitting capability. while densenet also reached a relatively low loss level, it remained slightly higher than cogfuse-mobilevit. other models had higher late-stage loss values: some showed fast initial decline but converged to higher levels, indicating shortcomings in capturing key disease features. 3.4 analysis of detection results for different classification models the confusion matrix is a key indicator for evaluating classification models: the higher the diagonal values, the higher the classification accuracy for the corresponding category, and the lower the off-diagonal values, the fewer misjudgments. figure&#xa0;12 shows the classification results of different models for the four disease grades (0&#x2013;3) of walnut leaf brown spot disease. it is clearly evident that the overall classification performance, particularly the accuracy of cogfuse-mobilevit across all categories, is remarkably high with minimal misjudgments, demonstrating that the model has enhanced discrimination ability for disease grades and performs optimally in distinguishing the four disease levels. figure&#xa0;12 figure&#xa0;12 . confusion matrices of the improved cogfuse-mobilevit model and traditional classification models (a) densenet; (b) efficientnet; (c) efficientnetv2; (d) swin transformer; (e) mobilevitv3; (f) cogfuse-mobilevit. the edge-enhanced feature module (ecfm) effectively addressed level 0 and level 1 misclassification by capturing incipient lesion features. baseline analysis revealed substantial false negatives for early-stage lesions, with level 1 and level 0 misclassification reaching 32%. ecfm reduced this rate to 8%, demonstrating its capacity to resolve texture confusion through enhanced edge contour delineation. similarly, the adaptive multi-scale dilated convolution module (amsddicm) significantly mitigated level 2 and level 3 mutual misclassification. amsddicm drastically reduced both errors by enhancing local fine-grained features in mid-stage lesions (level 2) while strengthening global fusion features in late-stage coalesced lesions (level 3). this resolves grading ambiguity arising from the morphological continuum of lesion progression. to establish a comprehensive performance evaluation framework and quantify the model&#x2019;s overall discriminative capability, figure&#xa0;13 presents the receiver operating characteristic (roc) curves of cogfuse-mobilevit across four severity levels. these curves compare the true positive rate (tpr) against the false positive rate (fpr) by dynamically adjusting the classification threshold. the area under the curve (auc) serves as the primary performance metric, where a higher value indicates stronger classification ability. notably, the auc values for all categories significantly exceed the level of random guessing (auc=0.5), fully demonstrating that the model possesses robust discriminative capability across different severity levels. figure&#xa0;13 figure&#xa0;13 . roc curves of the four different severity levels for cogfuse-mobilevit. 3.5 tsne visualization of features extracted by different models the tsne visualization of features extracted from the models is shown in figures&#xa0;14a&#x2013;d . this study analyzed the features of the original model at the 20th, 50th, and 100th training epochs, as well as the improved model at the 100th epoch. after 20 epochs of training, the original model showed a highly discrete feature distribution. limited by the number of training epochs, the model failed to fully learn discriminative features, resulting in insufficient distinction between categories and significant overlap among different classes. this indicates that under this training intensity, the original model&#x2019;s ability to capture meaningful patterns was relatively limited. when the original model was trained to 50 epochs, compared with (a), the feature aggregation significantly improved. however, inter-class overlap still existed, suggesting that although prolonged training aided feature learning, the original model&#x2019;s architecture had inherent defects in achieving clear feature separation. at 100 epochs of training, the original model exhibited more prominent feature clustering. nevertheless, some regions still lacked clear separation between different categories, indicating that even after prolonged training, the original model faced challenges in maximizing inter-class distance and intra-class compactness. in figure (d), the improved model after 100 epochs of training demonstrated a more superior feature distribution: each category was tightly clustered, achieving high intra-class compactness, while distinct boundaries between different categories were established, resulting in significant inter-class distance. this suggests that the model improvements effectively enhanced its ability to extract discriminative features, greatly reduced feature ambiguity, and improved feature discriminative power. compared with the original model, it showed stronger feature discrimination and optimization potential. figure&#xa0;14 figure&#xa0;14 . tsne visualization of features extracted by different models. after the three modules work synergistically, the boundaries of feature clustering between level 0 (healthy) and level 1 (early-stage) are significantly clearer, which verifies the effectiveness of edge-texture collaborative learning (a) the original model was trained for 20 epochs; (b) the original model was trained for 50 epochs; (c) the original model was trained for 100 epochs; (d) the improved model was trained for 100 epochs. 3.6 radar chart for comparison of classification performance between original and improved models as shown in figure&#xa0;15 , a radar chart compares the performance of the original model and the improved cogfuse-mobilevit model across multiple evaluation metrics. in terms of accuracy, cogfuse-mobilevit exhibits significantly higher values than the original mobilevitv3 model, indicating that the improved model achieves overall higher classification accuracy. macro-average precision, which considers the average precision of each category, clearly shows that cogfuse-mobilevit also performs better, meaning it has superior precision across all categories. meanwhile, cogfuse-mobilevit also demonstrates better macro-average recall. when examining weighted-average precision and weighted-average recall&#x2014;metrics that account for class imbalance&#x2014;cogfuse-mobilevit outperforms mobilevitv3 in both, indicating that in practical applications, even with uneven class distribution, the cogfuse-mobilevit model can deliver better performance. these results further demonstrate the model&#x2019;s reliability and practicality. figure&#xa0;15 figure&#xa0;15 . multi-metric comparison between mobilevitv3 and improved cogfuse-mobilevit model. 3.7 public data set experiment to further validate the efficacy and generalizability of the proposed cogfuse-mobilevit model, experiments were conducted on the public dataset appleleaf9 ( yang et&#xa0;al., 2022 ). this dataset comprises healthy apple leaves and eight categories of apple leaf diseases captured in field environments without restrictions on imaging angles or noise interference. the dataset was partitioned into training and test sets at an 8:2 ratio. all images were resized to 224&#xd7;224 pixels to optimize deep learning model training efficiency. following the hyperparameter configurations specified in &#x201c;experimental parameters and evaluation metrics&#x201d;, both the baseline mobilevitv3 and cogfuse-mobilevit models were trained and evaluated. cross-species validation demonstrates that the cogfuse-mobilevit model delivers exceptional performance on the appleleaf9 dataset. as presented in table&#xa0;9 , the model comprehensively surpasses the baseline mobilevitv3 across all four core metrics: precision increases by 0.16 percentage points, recall achieves a breakthrough improvement of 3.45 percentage points, f1-score rises by 1.18 percentage points, and accuracy elevates by 1.14 percentage points. the synergistic enhancement in both precision and recall signifies that performance gains stem from strengthened feature discriminability rather than metric trade-off compromises. the remarkable percentage points recall gain substantially mitigates leaf disease omission rates, while the holistic advance in f1-score further attests to the model&#x2019;s robustness. these results collectively validate the generalizability of cogfuse-mobilevit&#x2019;s core innovations in agricultural fine-grained disease grading, establishing a transferable paradigm for small-target pathology identification across plant species. table&#xa0;9 table&#xa0;9 . experimental results of the original model and cogfuse-mobilevit model on appleleaf9 data set. 4 conclusion this study addresses the challenges in precise grading of walnut leaf brown spot disease. by adopting dynamic feature filtering, edge gradient reinforcement, and multi-scale morphological adaptation, it effectively resolves three key limitations of existing hybrid architectures and the baseline model mobilevitv3 in plant disease grading weak capability in capturing blurred edges, poor multi-scale adaptability, and interference from healthy tissues. experimental results show that the model achieves an 86.61% grading accuracy on a dataset encompassing diverse lighting conditions, cultivars, and lesion stages, representing a 7.80% improvement over the baseline mobilevitv3 and outperforming nine state-of-the-art models. at the same time, this study confirmed that the performance improvement of cogfuse-mobilevit was statistically significant and stable through strict statistical tests, providing a reliable method for accurate classification of walnut leaf spot disease. the constructed image dataset and proposed grading re-measurement method lay the foundation for accuracy. this approach provides a new paradigm for small-target disease grading, with core modules transferable to multi-crop disease recognition. beyond disease classification, context-aware ai frameworks demonstrate significant efficacy in agricultural management ( acharya et&#xa0;al., 2022 ). iot systems dynamically adjust fertilization recommendations by integrating real-time soil-crop data, while salinity-corrected evapotranspiration (ets) models optimize irrigation strategies for saline-alkali soils ( li et&#xa0;al., 2023 ; su et&#xa0;al., 2023 ). similarly, multimodal fusion techniques enhance reference evapotranspiration (eto) prediction accuracy, facilitating precision water allocation ( jo et&#xa0;al., 2021 ; rustia et&#xa0;al., 2023 ). these breakthroughs collectively validate the robust capability of adaptive feature processing in complex agricultural environments&#x2014;a core principle that resonates with our dynamic weighting strategy for disease feature extraction. future research should therefore integrate cross-modal and cross-domain capabilities to establish multidimensional assessment systems and regional dynamic monitoring frameworks, thereby delivering comprehensive technical solutions for small-target disease control. 5 discussion and future work compared with existing methods, this study overcomes the limitation of traditional deep learning models relying on fixed convolution kernels, enabling adaptive capture of lesion features with multi-shaped convolution kernels to accurately extract edge textures of circular micro-lesions and regional contours of irregular lesions. although the constructed multi-source dataset covers diverse lighting conditions, cultivars, and lesion development stages, the homogeneous internal features of severe diseases lead to limited recall rate. meanwhile, when lesions are severely overlapped or mixed with mechanical damage, insect damage, or other types of injuries, the discriminative accuracy of the model is affected. for ultra-small lesions, the local feature information is too weak and easily overlooked, and the computational efficiency still needs optimization on some hardware platforms. furthermore, the adaptability of the current model in extreme field scenarios, such as high-density occlusion and compound damage from pests and diseases, remains to be further verified. in these scenarios, the coupling of complex interference factors exacerbates feature confusion, affecting grading reliability. to address these issues, future work will focus on the following research directions. future research will focus on enhancing the model&#x2019;s performance in complex field environments through multiple synergistic strategies. this includes constructing multi-interference factor coupled datasets that incorporating insect holes, lesions, and soil adhesion to strengthen robustness against extreme field disturbances; introducing attention-based multi-damage feature decoupling modules and dedicated micro-lesion enhancement modules combining super-resolution and feature interpolation to improve discriminability in complex scenarios and for tiny lesions;&#xa0;optimizing dynamic weight calculations via approximate computation or hardware-friendly reconstruction, while developing lightweight real-time deployment frameworks integrated with uav near-ground sensing technology to boost efficiency and practical monitoring coverage; and establishing cross-crop pathological transfer learning mechanisms, extending the model to multi-crop disease recognition tasks with self-supervised pre-training to enhance algorithm universality and low-contrast lesion feature mining capabilities. through the above research, it is expected to further improve the applicability and practicality of the model in complex field environments, promoting the development of intelligent plant disease grading technology towards more accurate, efficient, and universal directions. data availability statement the raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. author contributions yw: writing &#x2013; original draft. dz: writing &#x2013; original draft. lz:&#xa0;writing &#x2013; review &amp; editing. funding the author(s) declare financial support was received for the research and/or publication of this article. this work was supported in part by the science and technology key project of the xinjiang production and construction corps (2023ab063) development and application demonstration of green technology for online monitoring of fresh fruits and cold chain logistics in xinjiang. conflict of interest the authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. generative ai statement the author(s) declare that no generative ai was used in the creation of this manuscript. any alternative text (alt text) provided alongside figures in this article has been generated by frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. if you identify any issues, please contact us. publisher&#x2019;s note all claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. references acharya, p., burgers, t., and nguyen, k.-d. (2022). ai-enabled droplet detection and tracking for agricultural spraying systems. computers and electronics in agriculture , 202, 107325. doi:&#xa0;10.1016/j.compag.2022.107325 crossref full text | google scholar adaskaveg, j., f&#xf6;rster, h., thompson, d., driever, g., connell, j., buchner, r., et al. (2009). epidemiology and management of walnut blight. walnut res. rep , 94, 225&#x2013;256. doi:&#xa0;10.1016/j.inpa.2016.10.005 crossref full text | google scholar arivazhagan, s., shebiah, r. n., ananthi, s., and varthini, s. v. (2013). detection of unhealthy region of plant leaves and classification of plant leaf diseases using texture features. agricultural engineering international: cigr journal , 15, 211&#x2013;217. google scholar chaudhary, p., chaudhari, a. k., cheeran, a., and godara, s. (2012). color transform based approach for disease spot detection on plant leaf. international journal of computer science and telecommunications , 3, 65&#x2013;70. google scholar chen, s., zhang, k., zhao, y., sun, y., ban, w., chen, y., et al. (2021). an approach for rice bacterial leaf streak disease segmentation and disease severity estimation. agriculture , 11, 420. doi:&#xa0;10.3390/agriculture11050420 crossref full text | google scholar chiang, k.-s., bock, c., el jarroudi, m., delfosse, p., lee, i., and liu, h. (2016). effects of rater bias and assessment method on disease severity estimation with regard to hypothesis testing. phytopathology , 65, 523&#x2013;535. doi:&#xa0;10.1111/ppa.12435 crossref full text | google scholar cooke, b. (2006). &#x201c;disease assessment and yield loss,&#x201d; in the epidemiology of plant diseases (dordrecht: springer), 43&#x2013;80. google scholar ghaiwat, s. n. and arora, p. (2014). detection and classification of plant leaf diseases using image processing techniques: a review. international journal of recent advances in engineering &amp; technology , 2, 1&#x2013;7. google scholar hu, g., wang, h., zhang, y., and wan, m. (2021). detection and severity analysis of tea leaf blight based on deep learning. computers &amp; electrical engineering , 90, 107023. doi:&#xa0;10.1016/j.compeleceng.2021.107023 crossref full text | google scholar jadhav, s. b. and patil, s. b. (2016). grading of soybean leaf disease based on segmented image using k-means clustering. iaes international journal of artificial intelligence , 5, 13&#x2013;13. doi:&#xa0;10.11591/ijai.v5.i1.pp13-21 crossref full text | google scholar jo, w. j., kim, d. s., sim, h. s., ahn, s. r., lee, h. j., moon, y. h., et al. (2021). estimation of evapotranspiration and water requirements of strawberry plants in greenhouses using environmental data. frontiers in sustainable food systems , 5, 684808. doi:&#xa0;10.3389/fsufs.2021.684808 crossref full text | google scholar khan, m. a., ali, m., shah, m., mahmood, t., ahmad, m., jhanjhi, n., et al. (2021). machine learning-based detection and classification of walnut fungi diseases. intelligent automation &amp; soft computing , 30, 771&#x2013;785. doi:&#xa0;10.32604/iasc.2021.018039 crossref full text | google scholar kim, s.-k. and ahn, j.-g. (2021). tomato crop diseases classification models using deep cnn-based architectures. j korea acad-ind coop soc , 22, 7&#x2013;14. doi:&#xa0;10.5762/kais.2021.22.5.7 crossref full text | google scholar lamichhane, j. r. (2014). xanthomonas arboricola diseases of stone fruit, almond, and walnut trees: progress toward understanding and management. plant disease , 98, 1600&#x2013;1610. doi:&#xa0;10.1094/pdis-08-14-0831-fe pubmed abstract | crossref full text | google scholar li, x., zha, t., liu, p., bourque, c. p.-a., jia, x., and tian, y. (2023). interannual variation in gross ecosystem production and evapotranspiration in a temperate semiarid grassland undergoing vegetation recovery. agricultural and forest meteorology , 341, 109672. doi:&#xa0;10.1016/j.agrformet.2023.109672 crossref full text | google scholar lin, k., gong, l., huang, y., liu, c., and pan, j. (2019). deep learning-based segmentation and quantification of cucumber powdery mildew using convolutional neural network. frontiers in plant science , 10, 155. doi:&#xa0;10.3389/fpls.2019.00155 pubmed abstract | crossref full text | google scholar ma, j., du, k., zhang, l., zheng, f., chu, j., and sun, z. (2017). a segmentation method for greenhouse vegetable foliar disease spots images using color information and region growing. computers and electronics in agriculture , 142, 110&#x2013;117. doi:&#xa0;10.1016/j.compag.2017.08.023 crossref full text | google scholar mao, r., wang, z., li, f., zhou, j., chen, y., and hu, x. (2023). gseyolox-s: an improved lightweight network for identifying the severity of wheat fusarium head blight. agronomy , 13, 242. doi:&#xa0;10.3390/agronomy13010242 crossref full text | google scholar mcgranahan, g. and leslie, c. (1991). walnuts (juglans). molecules , 907&#x2013;924. doi:&#xa0;10.17660/actahortic.1991.290.20 crossref full text | google scholar mircetich, s. m., sanborn, r., and ramos, d. (1980). natural spread, graft-transmission, and possible etiology of walnut blackline disease. phytopathology , 70, 962&#x2013;968. doi:&#xa0;10.1094/phyto-70-962 crossref full text | google scholar moragrega, c., matias, j., alet&#xe0;, n., montesinos, e., and rovira, m. (2011). apical necrosis and premature drop of persian (english) walnut fruit caused by xanthomonas arboricola pv. juglandis. plant disease , 95, 1565&#x2013;1570. doi:&#xa0;10.1094/pdis-03-11-0259 pubmed abstract | crossref full text | google scholar ngugi, l. c., abdelwahab, m., and abo-zahhad, m. (2020). tomato leaf segmentation algorithms for mobile phone applications using deep learning. computers and electronics in agriculture , 178, 105788. doi:&#xa0;10.1016/j.compag.2020.105788 crossref full text | google scholar ozturk, o., sarica, b., and seker, d. z. (2025). interpretable and robust ensemble deep learning framework for tea leaf disease classification. horticulturae , 11, 437. doi:&#xa0;10.3390/horticulturae11040437 crossref full text | google scholar parashar, n., johri, p., khan, a. a., gaur, n., and kadry, s. (2024). an integrated analysis of yield prediction models: a comprehensive review of advancements and challenges. computers, materials &amp; continua , 80 (1). doi:&#xa0;10.32604/cmc.2024.050240 crossref full text | google scholar picon, a., alvarez-gila, a., seitz, m., ortiz-barredo, a., echazarra, j., and johannes, a. (2019). deep convolutional neural networks for mobile capture device-based crop disease classification in the wild. computers and electronics in agriculture , 161, 280&#x2013;290. doi:&#xa0;10.1016/j.compag.2018.04.002 crossref full text | google scholar rastogi, a., arora, r., and sharma, s. (2015). &#x201c;leaf disease detection and grading using computer vision technology &amp; fuzzy logic,&#x201d; in paper presented at the 2015 2nd international conference on signal processing and integrated networks (spin). google scholar rustia, d. j. a., lee, w.-c., lu, c.-y., wu, y.-f., shih, p.-y., and chen, s.-k. (2023). edge-based wireless imaging system for continuous monitoring of insect pests in a remote outdoor mango orchard. computers and electronics in agriculture , 211, 108019. doi:&#xa0;10.1016/j.compag.2023.108019 crossref full text | google scholar shi, t., liu, y., zheng, x., hu, k., huang, h., liu, h., et al. (2023). recent advances in plant disease severity assessment using convolutional neural networks. scientific reports , 13, 2336. doi:&#xa0;10.1038/s41598-023-29230-7 pubmed abstract | crossref full text | google scholar singh, u. p., chouhan, s. s., jain, s., and jain, s. (2019). multilayer convolution neural network for the classification of mango leaves infected by anthracnose disease. ieee access , 7, 43721&#x2013;43729. doi:&#xa0;10.1109/access.2019.2907383 crossref full text | google scholar su, y., wang, j., li, j., wang, l., wang, k., li, a., et al. (2023). spatiotemporal changes and driving factors of reference evapotranspiration and crop evapotranspiration for cotton production in china from 1960 to 2019. frontiers in environmental science , 11, 1251789. doi:&#xa0;10.3389/fenvs.2023.1251789 crossref full text | google scholar tripathi, r. (2021). &#x201c;a deep learning approach for plant material disease identification,&#x201d; in paper presented at the iop conference series: materials science and engineering. google scholar vishnoi, v. k., kumar, k., kumar, b., mohan, s., and khan, a. a. (2022). detection of apple plant diseases using leaf images through convolutional neural network . ieee access , 11, 6594&#x2013;6609. doi:&#xa0;10.1109/access.2022.3232917 crossref full text | google scholar wang, f., dun, c., tang, t., duan, y., guo, x., and you, j. (2022). boeremia exigua causes leaf spot of walnut trees (juglans regia) in china. plant disease , 106, 1993. doi:&#xa0;10.1094/pdis-10-21-2304-pdn crossref full text | google scholar wang, q.-h., fan, k., li, d.-w., han, c.-m., qu, y.-y., qi, y.-k., et al. (2020). identification, virulence and fungicide sensitivity of colletotrichum gloeosporioides ss responsible for walnut anthracnose disease in china. plant disease , 104, 1358&#x2013;1368. doi:&#xa0;10.1094/pdis-12-19-2569-re pubmed abstract | crossref full text | google scholar weber, b. c. (1980). how to diagnose black walnut damage vol. 57 (north central forest experiment station, forest service, us department of agriculture). google scholar xu, w. (2024). hcf-net: hierarchicalcontextfusion network forinfrared small object detection. arxiv . arxiv, 2403.10778. doi:&#xa0;10.1109/icme57554.2024.10687431 crossref full text | google scholar yang, c., deng, y., wang, f., yang, h., xu, x., zeng, q., et al. (2021). brown leaf spot on juglans sigillata caused by ophiognomonia leptostyla in sichuan, china. plant disease , 105, 4160. doi:&#xa0;10.1094/pdis-02-21-0344-pdn crossref full text | google scholar yang, q., duan, s., and wang, l. (2022). efficient identification of apple leaf diseases in the wild using convolutional neural networks. agronomy , 12, 2784. doi:&#xa0;10.3390/agronomy12112784 crossref full text | google scholar zarei, s., taghavi, s. m., banihashemi, z., hamzehzarghani, h., and osdaghi, e. (2019). etiology of leaf spot and fruit canker symptoms on stone fruits and nut trees in iran. journal of plant pathology , 101, 1133&#x2013;1142. doi:&#xa0;10.1007/s42161-019-00283-w crossref full text | google scholar zhang, y., li, x., &#x160;im&#x16f;nek, j., shi, h., chen, n., and hu, q. (2023). quantifying water and salt movement in a soil-plant system of a corn field using hydrus (2d/3d) and the stable isotope method. agricultural water management , 288, 108492. doi:&#xa0;10.1016/j.agwat.2023.108492 crossref full text | google scholar keywords: walnut, brown spot disease ( ophiognomonia leptostyla ), hierarchical feature selection, edge features perception, adaptive multi-scale dilated convolution, disease grading citation: wei y, zeng d and zheng l (2025) a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution. front. plant sci. 16:1641677. doi: 10.3389/fpls.2025.1641677 received: 05 june 2025; accepted: 09 september 2025; published: 03 october 2025. edited by: ravinder kumar , indian agricultural research institute (icar), india reviewed by: geza bujdoso , hungarian university of agricultural and life sciences, hungary vasudha vedula , university of texas of the permian basin, united states copyright &#xa9; 2025 wei, zeng and zheng. this is an open-access article distributed under the terms of the creative commons attribution license (cc by) . the use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. no use, distribution or reproduction is permitted which does not comply with these terms. *correspondence: liangfang zheng, mtgxnjaynta0othamtyzlmnvbq== &#x2020; these authors have contributed equally to this work disclaimer: all claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. download article download pdf readcube epub xml share on export citation endnote reference manager simple text file bibtex 1,063 total views 103 downloads citation numbers are available from dimensions view article impact view altmetric score share on edited by r k ravinder kumar reviewed by g b geza bujdoso v v vasudha vedula table of contents abstract 1 introduction 2 materials and methods 3 experiments and results analysis 4 conclusion 5 discussion and future work data availability statement author contributions funding conflict of interest generative ai statement publisher&#x2019;s note references export citation endnote reference manager simple text file bibtex check for updates frontiers&#39; impact articles published with frontiers have received 12 million total citations your research is the real superpower - learn how we maximise its impact through our leading community journals explore our impact metrics supplementary material download article download download pdf readcube epub xml guidelines author guidelines services for authors policies and publication ethics editor guidelines fee policy explore articles research topics journals how we publish outreach frontiers forum frontiers policy labs frontiers for young minds frontiers planet prize connect help center emails and alerts contact us submit career opportunities follow us © 2025 frontiers media s.a. all rights reserved privacy policy | terms and conditions [["shallowreactive",1],{"data":2,"state":4,"once":10,"_errors":11,"serverrendered":13,"path":14,"pinia":15},["shallowreactive",3],{},["reactive",5],{"$ssite-config":6},{"env":7,"name":8,"url":9},"production","frontiers articles","https://article-pages-2024.frontiersin.org/",["set"],["shallowreactive",12],{},true,"/journals/plant-science/articles/10.3389/fpls.2025.1641677/full",["reactive",16],{"main":17,"user":534,"article":535,"articlehub":760,"mainheader":764},{"ibar":18,"footer":284,"newslettercomponent":-1,"snackbaritem":366,"toggleshowsnackbar":367,"contentfuljournal":368,"graphjournal":434,"settingsfeaturesswitchers":438,"templatetogglebanner":439,"tenantconfig":499},{"tenantlogo":19,"journallogo":19,"aboutus":20,"submiturl":113,"showsubmitbutton":13,"journal":114,"sectionterm":215,"aboutjournal":216,"mainlinks":265,"journallinks":272,"helpcenterlink":281},"",[21,38,47,71,87],{"title":22,"links":23},"who we are",[24,29,32,35],{"text":25,"url":26,"target":27,"arialabel":28},"mission and values","https://www.frontiersin.org/about/mission","_self",null,{"text":30,"url":31,"target":27,"arialabel":28},"history","https://www.frontiersin.org/about/history",{"text":33,"url":34,"target":27,"arialabel":28},"leadership","https://www.frontiersin.org/about/leadership",{"text":36,"url":37,"target":27,"arialabel":28},"awards","https://www.frontiersin.org/about/awards",{"title":39,"links":40},"impact and progress",[41,44],{"text":42,"url":43,"target":27,"arialabel":28},"frontiers' impact","https://www.frontiersin.org/about/impact",{"text":45,"url":46,"target":27,"arialabel":28},"our annual reports","https://www.frontiersin.org/about/annual-reports",{"title":48,"links":49},"publishing model",[50,53,56,59,62,65,68],{"text":51,"url":52,"target":27,"arialabel":28},"how we publish","https://www.frontiersin.org/about/how-we-publish",{"text":54,"url":55,"target":27,"arialabel":28},"open access","https://www.frontiersin.org/about/open-access",{"text":57,"url":58,"target":27,"arialabel":28},"peer review","https://www.frontiersin.org/about/peer-review",{"text":60,"url":61,"target":27,"arialabel":28},"research integrity","https://www.frontiersin.org/about/research-integrity",{"text":63,"url":64,"target":27,"arialabel":28},"research topics","https://www.frontiersin.org/about/research-topics",{"text":66,"url":67,"target":27,"arialabel":28},"fair² data management","https://www.frontiersin.org/about/fair-data-management",{"text":69,"url":70,"target":27,"arialabel":28},"fee policy","https://www.frontiersin.org/about/fee-policy",{"title":72,"links":73},"services",[74,78,81,84],{"text":75,"url":76,"target":77,"arialabel":28},"societies","https://publishingpartnerships.frontiersin.org/","_blank",{"text":79,"url":80,"target":27,"arialabel":28},"national consortia","https://www.frontiersin.org/open-access-agreements/consortia",{"text":82,"url":83,"target":27,"arialabel":28},"institutional partnerships","https://www.frontiersin.org/about/open-access-agreements",{"text":85,"url":86,"target":27,"arialabel":28},"collaborators","https://www.frontiersin.org/about/collaborators",{"title":88,"links":89},"more from frontiers",[90,94,97,101,105,109],{"text":91,"url":92,"target":77,"arialabel":93},"frontiers forum","https://forum.frontiersin.org/","this link will take you to the frontiers forum website",{"text":95,"url":96,"target":27,"arialabel":28},"frontiers planet prize","https://www.frontiersin.org/about/frontiers-planet-prize",{"text":98,"url":99,"target":77,"arialabel":100},"press office","https://pressoffice.frontiersin.org/","this link will take you to the frontiers press office website",{"text":102,"url":103,"target":27,"arialabel":104},"sustainability","https://www.frontiersin.org/about/sustainability","link to information about frontiers' sustainability",{"text":106,"url":107,"target":77,"arialabel":108},"career opportunities","https://careers.frontiersin.org/","this link will take you to the frontiers careers website",{"text":110,"url":111,"target":27,"arialabel":112},"contact us","https://www.frontiersin.org/about/contact","this link will take you to the help pages to contact our support team","https://www.frontiersin.org/submission/submit?domainid=1&fieldid=66&specialtyid=0&entitytype=2&entityid=373",{"id":115,"name":116,"slug":117,"sections":118},373,"frontiers in plant science","plant-science",[119,123,127,131,135,139,143,147,151,155,159,163,167,171,175,179,183,187,191,195,199,203,207,211],{"id":120,"name":121,"slug":122},1553,"aquatic photosynthetic organisms","aquatic-photosynthetic-organisms",{"id":124,"name":125,"slug":126},1356,"crop and product physiology","crop-and-product-physiology",{"id":128,"name":129,"slug":130},467,"functional plant ecology","functional-plant-ecology",{"id":132,"name":133,"slug":134},2844,"functional and applied plant genomics","functional-and-applied-plant-genomics",{"id":136,"name":137,"slug":138},2843,"photosynthesis and photobiology","photosynthesis-and-photobiology",{"id":140,"name":141,"slug":142},1312,"plant abiotic stress","plant-abiotic-stress",{"id":144,"name":145,"slug":146},2183,"plant bioinformatics","plant-bioinformatics",{"id":148,"name":149,"slug":150},589,"plant biophysics and modeling","plant-biophysics-and-modeling",{"id":152,"name":153,"slug":154},560,"plant biotechnology","plant-biotechnology",{"id":156,"name":157,"slug":158},468,"plant breeding","plant-breeding",{"id":160,"name":161,"slug":162},577,"plant cell biology","plant-cell-biology",{"id":164,"name":165,"slug":166},474,"plant development and evodevo","plant-development-and-evodevo",{"id":168,"name":169,"slug":170},2845,"plant genetics, epigenetics and chromosome biology","plant-genetics-epigenetics-and-chromosome-biology",{"id":172,"name":173,"slug":174},479,"plant membrane traffic and transport","plant-membrane-traffic-and-transport",{"id":176,"name":177,"slug":178},481,"plant metabolism and chemodiversity","plant-metabolism-and-chemodiversity",{"id":180,"name":181,"slug":182},486,"plant nutrition","plant-nutrition",{"id":184,"name":185,"slug":186},485,"plant pathogen interactions","plant-pathogen-interactions",{"id":188,"name":189,"slug":190},226,"plant physiology","plant-physiology",{"id":192,"name":193,"slug":194},580,"plant proteomics and protein structural biology","plant-proteomics-and-protein-structural-biology",{"id":196,"name":197,"slug":198},1570,"plant symbiotic interactions","plant-symbiotic-interactions",{"id":200,"name":201,"slug":202},1428,"plant systematics and evolution","plant-systematics-and-evolution",{"id":204,"name":205,"slug":206},483,"plant systems and synthetic biology","plant-systems-and-synthetic-biology",{"id":208,"name":209,"slug":210},2277,"sustainable and intelligent phytoprotection","sustainable-and-intelligent-phytoprotection",{"id":212,"name":213,"slug":214},484,"technical advances in plant science","technical-advances-in-plant-science","sections",[217,241],{"title":218,"links":219},"scope",[220,223,226,229,232,235,238],{"text":221,"url":222,"target":27,"arialabel":28},"field chief editors","https://www.frontiersin.org/journals/plant-science/about#about-editors",{"text":224,"url":225,"target":27,"arialabel":28},"mission & scope","https://www.frontiersin.org/journals/plant-science/about#about-scope",{"text":227,"url":228,"target":27,"arialabel":28},"facts","https://www.frontiersin.org/journals/plant-science/about#about-facts",{"text":230,"url":231,"target":27,"arialabel":28},"journal sections","https://www.frontiersin.org/journals/plant-science/about#about-submission",{"text":233,"url":234,"target":27,"arialabel":28},"open access statement","https://www.frontiersin.org/journals/plant-science/about#about-open",{"text":236,"url":237,"target":27,"arialabel":28},"copyright statement","https://www.frontiersin.org/journals/plant-science/about#copyright-statement",{"text":239,"url":240,"target":27,"arialabel":28},"quality","https://www.frontiersin.org/journals/plant-science/about#about-quality",{"title":242,"links":243},"for authors",[244,247,250,253,256,259,262],{"text":245,"url":246,"target":27,"arialabel":28},"why submit?","https://www.frontiersin.org/journals/plant-science/for-authors/why-submit",{"text":248,"url":249,"target":27,"arialabel":28},"article types","https://www.frontiersin.org/journals/plant-science/for-authors/article-types",{"text":251,"url":252,"target":27,"arialabel":28},"author guidelines","https://www.frontiersin.org/journals/plant-science/for-authors/author-guidelines",{"text":254,"url":255,"target":27,"arialabel":28},"editor guidelines","https://www.frontiersin.org/journals/plant-science/for-authors/editor-guidelines",{"text":257,"url":258,"target":27,"arialabel":28},"publishing fees","https://www.frontiersin.org/journals/plant-science/for-authors/publishing-fees",{"text":260,"url":261,"target":27,"arialabel":28},"submission checklist","https://www.frontiersin.org/journals/plant-science/for-authors/submission-checklist",{"text":263,"url":264,"target":27,"arialabel":28},"contact editorial office","https://www.frontiersin.org/journals/plant-science/for-authors/contact-editorial-office",[266,269],{"text":267,"url":268,"target":27,"arialabel":28},"all journals","https://www.frontiersin.org/journals",{"text":270,"url":271,"target":27,"arialabel":28},"all articles","https://www.frontiersin.org/articles",[273,276,278],{"text":274,"url":275,"target":27,"arialabel":28},"articles","articles",{"text":63,"url":277,"target":27,"arialabel":28},"research-topics",{"text":279,"url":280,"target":27,"arialabel":28},"editorial board","editors",{"text":282,"url":283,"target":77,"arialabel":282},"help center","https://helpcenter.frontiersin.org",{"blocks":285,"sociallinks":339,"copyright":363,"termsandconditionsurl":364,"privacypolicyurl":365},[286,300,310,324],{"title":287,"links":288},"guidelines",[289,291,294,297,299],{"text":251,"url":290,"target":27,"arialabel":28},"https://www.frontiersin.org/guidelines/author-guidelines",{"text":292,"url":293,"target":27,"arialabel":28},"services for authors","https://www.frontiersin.org/for-authors/author-services",{"text":295,"url":296,"target":27,"arialabel":28},"policies and publication ethics","https://www.frontiersin.org/guidelines/policies-and-publication-ethics",{"text":254,"url":298,"target":27,"arialabel":28},"https://www.frontiersin.org/guidelines/editor-guidelines",{"text":69,"url":70,"target":27,"arialabel":28},{"title":301,"links":302},"explore",[303,304,307,309],{"text":274,"url":271,"target":27,"arialabel":28},{"text":305,"url":306,"target":27,"arialabel":28},"research topics ","https://www.frontiersin.org/research-topics",{"text":308,"url":268,"target":27,"arialabel":28},"journals",{"text":51,"url":52,"target":27,"arialabel":28},{"title":311,"links":312},"outreach",[313,316,319,323],{"text":314,"url":92,"target":77,"arialabel":315},"frontiers forum ","frontiers forum website",{"text":317,"url":318,"target":77,"arialabel":28},"frontiers policy labs ","https://policylabs.frontiersin.org/",{"text":320,"url":321,"target":77,"arialabel":322},"frontiers for young minds","https://kids.frontiersin.org/","frontiers for young minds journal",{"text":95,"url":96,"target":27,"arialabel":28},{"title":325,"links":326},"connect",[327,328,332,335,338],{"text":282,"url":283,"target":77,"arialabel":282},{"text":329,"url":330,"target":77,"arialabel":331},"emails and alerts ","https://subscription-management.frontiersin.org/emails/preferences#block0","subscribe to frontiers emails",{"text":333,"url":111,"target":27,"arialabel":334},"contact us ","subscribe to newsletter",{"text":336,"url":337,"target":27,"arialabel":28},"submit","https://www.frontiersin.org/submission/submit",{"text":106,"url":107,"target":77,"arialabel":28},[340,348,353,358],{"link":341,"type":344,"color":345,"icon":346,"size":347,"hiddentext":13},{"text":342,"url":343,"target":77,"arialabel":342},"frontiers facebook","https://www.facebook.com/frontiersin","link","grey","facebook","medium",{"link":349,"type":344,"color":345,"icon":352,"size":347,"hiddentext":13},{"text":350,"url":351,"target":77,"arialabel":28},"frontiers twitter","https://twitter.com/frontiersin","twitter",{"link":354,"type":344,"color":345,"icon":357,"size":347,"hiddentext":13},{"text":355,"url":356,"target":77,"arialabel":28},"frontiers linkedin","https://www.linkedin.com/company/frontiers","linkedin",{"link":359,"type":344,"color":345,"icon":362,"size":347,"hiddentext":13},{"text":360,"url":361,"target":77,"arialabel":28},"frontiers instagram","https://www.instagram.com/frontiersin_","instagram","frontiers media s.a. all rights reserved","https://www.frontiersin.org/legal/terms-and-conditions","https://www.frontiersin.org/legal/privacy-policy",{},false,{"__typename":369,"identifier":115,"name":116,"slug":117,"banner":370,"description":427,"mission":428,"palette":429,"impactfactor":430,"citescore":431,"citations":432,"showtagline":28,"twitter":433},"journal",[371],{"id":372,"src":373,"name":374,"tags":375,"type":385,"width":386,"height":387,"idhash":388,"archive":389,"brandid":390,"limited":389,"filesize":391,"ispublic":392,"original":393,"copyright":394,"extension":395,"thumbnails":397,"datecreated":405,"description":406,"orientation":407,"usercreated":408,"watermarked":389,"datemodified":405,"datepublished":409,"ecsarchivefiles":410,"propertyoptions":411,"property_channel":416,"property_sub-type":418,"property_asset_type":420,"activeoriginalfocuspoint":422,"property_office_department":425},"450e9326-0272-405c-b8d614c72bed9f89","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/webimage-00a7f7fd-61fc-4329-b3bfcc0119b4b276.jpg","fpls_main visual_green_website",[376,377,378,379,380,381,382,383,384],"medical","vitality","decoration","five","cosmetic","r","cure","aloe vera","spike","image",4928,3264,"720507a162925515",0,"22c10171-81b3-4da6-99342f272a32e8bb",11359471,1,"https://brand.frontiersin.org/m/720507a162925515/original/fpls_main-visual_green_website.jpeg","copyright (c) 2017 sabine hortebusch/shutterstock. no use without permission.",[396],"jpeg",{"mini":398,"thul":399,"webimage":373,"guidelines":400,"websitejpg_xl":401,"websitewebp_l":402,"websitewebp_m":403,"websitewebp_xl":404},"https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/mini-006feee0-2d70-4630-b9abfb0a0fa410aa.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/thul-c817db1b-00e8-4b29-b71c4e5a6058947f.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/guidelines-fpls_main visual_green_website.png","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitejpg_xl-fpls_main visual_green_website.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_l-fpls_main visual_green_website.webp","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_m-fpls_main visual_green_website.webp","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_xl-fpls_main visual_green_website.webp","2022-06-27t10:00:48z","spiral aloe vera with water drops, closeup","landscape","caroline sutter","2022-06-27t09:27:09z",[],[412,413,414,415],"414fb2d4-2283-43fd-be14e534eca67928","6c18119b-14bd-4951-b437696f4357bd33","7c692885-db25-4858-b1fb4ff47b241e9b","d88c0047-ec30-4506-a7df28a4d765e1cf",[417],"frontiersin_org",[419],"main_visual",[421],"photography",{"x":423,"y":424},2464,1632,[426],"publishing","the most cited plant science journal, advancing our understanding of plant biology for sustainable food security, functional ecosystems and human health.","\u003cp>frontiers in plant science is a leading, multidisciplinary journal that seeks to advance our understanding of fundamental processes in plant biology.\u003c/p>\n\n\u003cp>led by field chief editor prof. chun-ming liu (institute of botany, chinese academy of sciences) and indexed in pubmed, pubmed central, and scopus, among others, the journal seeks original and significant contributions that cultivate plant biology and its applications. the journal has the long-term goal of supporting sustainable development, food security, functional ecosystems, biotechnology (including biofuels and biomaterials), and human health.\u003c/p>\n\u003cp>frontiers in plant science welcomes original research, review, opinion, and perspective articles, among other submission types, covering the journal’s specialty sections:\u003c/p>\n\n\u003cdiv> &bull; aquatic photosynthetic organisms\u003c/div>\n\u003cdiv> &bull; crop and product physiology\u003c/div>\n\u003cdiv> &bull; functional plant ecology\u003c/div>\n\u003cdiv> &bull; functional and applied plant genomics\u003c/div>\n\u003cdiv> &bull; photosynthesis and photobiology\u003c/div>\n\u003cdiv> &bull; plant abiotic stress\u003c/div>\n\u003cdiv> &bull; plant bioinformatics\u003c/div>\n\u003cdiv> &bull; plant biophysics and modeling\u003c/div>\n\u003cdiv> &bull; plant biotechnology\u003c/div>\n\u003cdiv> &bull; plant breeding\u003c/div>\n\u003cdiv> &bull; plant cell biology\u003c/div>\n\u003cdiv> &bull; plant development and evodevo\u003c/div>\n\u003cdiv> &bull; plant genetics, epigenetics and chromosome biology\u003c/div>\n\u003cdiv> &bull; plant membrane traffic and transport\u003c/div>\n\u003cdiv> &bull; plant metabolism and chemodiversity\u003c/div>\n\u003cdiv> &bull; plant nutrition\u003c/div>\n\u003cdiv> &bull; plant pathogen interactions\u003c/div>\n\u003cdiv> &bull; plant physiology\u003c/div>\n\u003cdiv> &bull; plant proteomics and protein structural biology\u003c/div>\n\u003cdiv> &bull; plant symbiotic interactions\u003c/div>\n\u003cdiv> &bull; plant systematics and evolution\u003c/div>\n\u003cdiv> &bull; plant systems and synthetic biology\u003c/div>\n\u003cdiv> &bull; sustainable and intelligent phytoprotection\u003c/div>\n\u003cdiv> &bull; technical advances in plant science.\u003c/div>\n\u003cbr>\n\u003cp>furthermore, the journal welcomes submissions that support and advance the un's sustainable development goals (sdgs), notably sdg 13: climate action and sdg 15: life on land.\u003c/p> \n\n\u003cp>frontiers in plant science is committed to advancing developments in the field of plant biology by allowing unrestricted access to articles and communicating scientific knowledge to researchers and the public alike, to enable the scientific breakthroughs of the future.\u003c/p> \n \n\u003cp>requirements\u003cp>\n\u003cp>manuscripts that focus on non-plant-related microbiology, human or animal genetics, and medical and pharmacological research are not suitable for publication in this journal. pure field agriculture studies such as those focusing on fertilizer application or yield optimization, without relevance to plant science, are also not within the scope of this journal.\u003c/p>\nstudies falling into the categories below will not be considered for review in this journal unless they are expanded and provide insight into the biological process being studied:\u003c/p>\n\n\u003cp>i) descriptive collections of transcripts, proteins, or metabolites, including comparative sets as a result of different conditions or treatments;\u003c/p>\n\u003cp>ii) descriptive studies that define gene families using pure phylogenetics and the assignment of cursory functional attributions (e.g. expression profiles, promoter analysis, and bioinformatic parameters).\u003c/p>\n\n\u003cp>quantitative analysis needs to be performed on a minimum of three biological replicates in order to enable an assessment of significance. this includes quantitative omics studies (transcriptomics, proteomics, metabolomics) as well as phenotypic measurements, quantitative assays, and qpcr expression analysis. studies that do not comply with these replication requirements will not be considered for review.\u003cp>\n\n\u003cp>studies using transgenic or mutant lines (plants and algae), for example, t-dna, transposon, rnai, crispr/cas9, chemically induced, overexpressors and reporter fusions (gus, gfps, luc), should be based on data from multiple alleles (minimum of two) displaying a common and stable phenotype. qualitative data can be presented from a single allele but should be indicative of observations from multiple alleles which should be explicitly stated in the text. quantitative data should be derived from multiple alleles (at least two) and should be displayed separately for each allele (with at least three independent replications for each allele). studies reporting single alleles may be considered acceptable when:\u003c/p>\n\n\u003cp>i) complementation via transformation is used for confirmation;\u003c/p> \n\u003cp>ii) the allele has been previously characterized and published, and is representative of multiple independent lines;\u003c/p>\n\u003cp>iii) in situations where genetic transformation is difficult or not yet possible, alternative evidence should be presented\u003c/p>\n\n","green","5.6","7.1","927148","@frontplantsci",{"id":115,"name":116,"slug":117,"abbreviation":435,"isonline":13,"isopenforsubmissions":13,"citescore":436,"impactfactor":437},"fpls",8.8,4.8,{"displaytitlepilllabels":13,"displayrelatedarticlesbox":13,"showeditors":13,"showreviewers":13,"showloopimpactlink":13,"enablefigshare":367,"usexmlimages":13},{"ispublic":13,"allowcompanyusers":367,"whitelistemails":440,"enablealljournals":13,"whitelistjournals":462},[441,442,443,444,445,446,447,448,449,446,450,451,452,453,454,455,456,457,458,459,460,461],"y2fybg9zlmxvcmnhqgzyb250awvyc2lulm9yzw==","zgfuawvslmx1ekbmcm9udgllcnnpbi5vcmc=","bgf1cmeudgvuyubmcm9udgllcnnpbi5vcmc=","cmfmywvslnjpyw5jag9aznjvbnrpzxjzaw4ub3jn","amftzxmuc21hbgxib25lqgzyb250awvyc2lulm9yzw==","znjhbi5tb3jlbm9aznjvbnrpzxjzaw4ub3jn","c2ftdwvslmzlcm5hbmrlekbmcm9udgllcnnpbi5vcmc=","ymfyymfyys5jyxjjyw5naxvaznjvbnrpzxjzaw4ub3jn","axzhbi5zyw50yw1hcmlhqgzyb250awvyc2lulm9yzw==","ahvllnryyw5aznjvbnrpzxjzaw4ub3jn","z29uy2fsby52yxjnyxnaznjvbnrpzxjzaw4ub3jn","bhvjawuuc2vubkbmcm9udgllcnnpbi5vcmc=","bg9ybi5mcmfzzxjaznjvbnrpzxjzaw4ub3jn","zwxzys5jyxjyb25aznjvbnrpzxjzaw4ub3jn","bwfhcnrlbi52yw5kawpja0bmcm9udgllcnnpbi5vcmc=","ymluzgh1lmtyaxnobmfuqgzyb250awvyc2lulm9yzw==","bwf0dghldy5hdhr3yxrlcnnaznjvbnrpzxjzaw4ub3jn","z2l1bglhlnzhbhnly2noaubmcm9udgllcnnpbi5vcmc=","zmfiawfulmrlbgxhbw9ydgvaznjvbnrpzxjzaw4ub3jn","dmlqyxlhbi5wcebwaxrzb2x1dglvbnmuy29t","z3vpbgxhdw1llnnldxjhdebmcm9udgllcnnpbi5vcmc=",[463,464,465,466,467,468,392,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498],2232,1729,2357,2456,2176,2333,1843,602,106,616,310,657,3123,1468,2137,628,1566,2726,2086,1490,451,141,1335,2655,1238,604,698,1547,176,1440,403,1239,755,2136,609,1534,{"spaceid":392,"name":392,"availablejournalpages":500,"announcement":504},[275,280,277,501,502,503],"volumes","about","community-reviewers",{"__typename":505,"sys":506,"preheader":42,"title":508,"description":509,"image":510,"link":532},"announcement",{"id":507},"5ittnttqgazppgh6rx0alm","articles published with frontiers have received 12 million total citations","your research is the real superpower - learn how we maximise its impact through our leading community journals",[511],{"archive":389,"brandid":390,"copyright":28,"datecreated":512,"datemodified":513,"datepublished":514,"description":28,"extension":515,"filesize":517,"height":518,"id":519,"ispublic":389,"limited":389,"name":520,"orientation":407,"original":28,"thumbnails":521,"type":385,"watermarked":389,"width":528,"videopreviewurls":529,"tags":530,"textmetaproperties":531,"src":522},"2025-06-18t12:58:01z","2025-06-18t14:15:34z","2025-06-18t12:57:32z",[516],"png",1365026,920,"7c5269c4-3c83-4591-951a74d15b95daea","impact metrics banner for article pages",{"webimage":522,"thul":523,"mini":524,"websitewebp_l":525,"websitewebp_m":526,"guidelines":527},"https://brand.frontiersin.org/m/59ee6275cb9a849c/webimage-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/m/59ee6275cb9a849c/thul-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/m/59ee6275cb9a849c/mini-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/websitewebp_l/impact-metrics-banner-for-article-pages.webp","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/websitewebp_m/impact-metrics-banner-for-article-pages.webp","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/guidelines/impact-metrics-banner-for-article-pages.png",1600,[],[],[],{"text":533,"url":43,"target":27,"arialabel":28},"explore our impact metrics",{"loggeduserinfo":-1},{"currentarticle":536,"ispreviewpage":367,"hassupplementaldata":367,"showcrossmarkwidget":13,"articletemplate":648,"currentarticlepagemetainfo":649,"xmlparsedarticlecontent":-1,"xmlparsingerror":-1},{"id":537,"doi":538,"title":539,"acceptancedate":540,"receptiondate":541,"publicationdate":542,"lastmodifieddate":543,"ispublished":13,"abstract":544,"researchtopic":545,"articletype":551,"stage":554,"keywords":556,"authors":563,"editors":587,"reviewers":595,"journal":610,"section":617,"impactmetrics":619,"volume":622,"articlevolume":623,"relatedarticles":624,"ispublishedv2":13,"contents":625,"files":628},1641677,"10.3389/fpls.2025.1641677","a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution","2025-09-09t14:48:37.000z","2025-06-05t09:39:42.000z","2025-10-03t00:00:00.000z","2025-10-21t02:25:15.907z","walnut leaf brown spot disease, caused by ophiognomonia leptostyla, is among the most destructive fungal diseases in walnut cultivation. in the development of smart agriculture, precision grading of plant diseases remains a core technical challenge; specifically, this disease is plagued by blurred lesion edges and inefficient extraction of complex features, which directly limits the accurate grading of the disease. to address these issues, this study proposes a disease grading method integrating hierarchical feature selection and adaptive multi-scale dilated convolution, and develops the cogfuse-mobilevit model. this model overcomes the limitations of the standard mobilevitv3 model in capturing blurred edges of tiny lesions via three innovative modules: specifically, the hierarchical feature screening module (hfsm) enables hierarchical screening of disease-related features; the edge feature focus module (ecfm) works in synergy with the hfsm to enhance the focus on lesion edge features; and the adaptive multi-scale dilated convolution fusion module (amsdidcm) achieves dynamic multi-scale fusion of lesion textures and global structures. experimental results demonstrate that the proposed model achieves an accuracy of 86.61% on the test set, representing an improvement of 7.8 percentage points compared with the original mobilevitv3 model and significantly outperforming other mainstream disease grading models. this study confirms that the cogfuse-mobilevit model can effectively resolve the issues of blurred edges and inefficient feature extraction in this disease, provides a reliable technical solution for its precision grading, and holds practical application value for the intelligent diagnosis of plant diseases in smart agriculture.",{"id":546,"title":547,"articlescount":548,"ismagazinepage":367,"slug":549,"isopenforsubmission":13,"views":550},66487,"innovative field diagnostics for real-time plant pathogen detection and management",9,"innovative-field-diagnostics-for-real-time-plant-pathogen-detection-and-management",14940,{"id":552,"name":553},24,"original research",{"id":555,"name":19},18,[557,558,559,560,561,562],"walnut","brown spot disease (ophiognomonia leptostyla)","hierarchical feature selection","edge features perception","adaptive multi-scale dilated convolution","disease grading",[564,575,581],{"id":565,"firstname":566,"middlename":19,"lastname":567,"givennames":568,"iscorresponding":367,"isprofilepublic":13,"userid":565,"email":-1,"affiliations":569},3089871,"yuting","wei","yuting ",[570,573],{"organizationname":571,"countryname":572,"cityname":19,"statename":19,"zipcode":19},"college of information engineering, tarim university","china",{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},"key laboratory of tarim oasis agriculture, ministry of education, tarim university",{"id":389,"firstname":576,"middlename":19,"lastname":577,"givennames":578,"iscorresponding":367,"isprofilepublic":367,"userid":389,"email":-1,"affiliations":579},"debin","zeng","debin ",[580],{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},{"id":389,"firstname":582,"middlename":19,"lastname":583,"givennames":584,"iscorresponding":13,"isprofilepublic":367,"userid":389,"email":19,"affiliations":585},"liangfang","zheng","liangfang ",[586],{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},[588],{"id":589,"firstname":590,"middlename":19,"lastname":591,"givennames":592,"iscorresponding":367,"isprofilepublic":13,"userid":589,"email":-1,"affiliations":593},1037951,"ravinder","kumar","ravinder ",[594],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},[596,603],{"id":597,"firstname":598,"middlename":19,"lastname":599,"givennames":600,"iscorresponding":367,"isprofilepublic":13,"userid":597,"email":-1,"affiliations":601},2082015,"geza","bujdoso","geza ",[602],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},{"id":604,"firstname":605,"middlename":19,"lastname":606,"givennames":607,"iscorresponding":367,"isprofilepublic":13,"userid":604,"email":-1,"affiliations":608},3078480,"vasudha","vedula","vasudha ",[609],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},{"id":115,"slug":117,"name":116,"shortname":611,"electronicissn":612,"field":613,"specialtyid":28,"journalsectionpaths":615},"front. plant sci.","1664-462x",{"id":614,"domainid":392},66,[616],{"section":617},{"id":208,"name":209,"slug":210,"specialtyid":618},2685,{"views":620,"downloads":621,"citations":389},1063,103,16,"volume 16 - 2025",[],{"titlehtml":539,"fulltexthtml":626,"menuhtml":627},"\u003cdiv class=\"journalabstract\">\u003ca id=\"h1\" name=\"h1\">\u003c/a>\u003cdiv class=\"authors\">\u003cspan class=\"author-wrapper notranslate\">\u003ca href=\"https://loop.frontiersin.org/people/3089871/overview\" class=\"user-id-3089871/overview\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/images/profile/3089871/overview/74\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\" alt=\"yuting wei,&#x;\">yuting wei\u003c/a>\u003csup>1,2&#x2020;\u003c/sup>\u003c/span>\u003cspan class=\"author-wrapper notranslate\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/cdn/images/profile/default_32.jpg\" alt=\"debin zeng&#x;\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\">debin zeng\u003csup>2&#x2020;\u003c/sup>\u003c/span>\u003cspan class=\"author-wrapper notranslate\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/cdn/images/profile/default_32.jpg\" alt=\"liangfang zheng*\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\">liangfang zheng\u003csup>2*\u003c/sup>\u003c/span>\u003c/div>\u003cul class=\"notes\">\u003cli>\u003cspan>\u003csup>1\u003c/sup>\u003c/span>college of information engineering, tarim university, alaer, china\u003c/li>\u003cli>\u003cspan>\u003csup>2\u003c/sup>\u003c/span>key laboratory of tarim oasis agriculture, ministry of education, tarim university, alaer, china\u003c/li>\u003c/ul>\u003cp>walnut leaf brown spot disease, caused by \u003ci>ophiognomonia leptostyla\u003c/i>, is among the most destructive fungal diseases in walnut cultivation. in the development of smart agriculture, precision grading of plant diseases remains a core technical challenge; specifically, this disease is plagued by blurred lesion edges and inefficient extraction of complex features, which directly limits the accurate grading of the disease. to address these issues, this study proposes a disease grading method integrating hierarchical feature selection and adaptive multi-scale dilated convolution, and develops the cogfuse-mobilevit model. this model overcomes the limitations of the standard mobilevitv3 model in capturing blurred edges of tiny lesions via three innovative modules: specifically, the hierarchical feature screening module (hfsm) enables hierarchical screening of disease-related features; the edge feature focus module (ecfm) works in synergy with the hfsm to enhance the focus on lesion edge features; and the adaptive multi-scale dilated convolution fusion module (amsdidcm) achieves dynamic multi-scale fusion of lesion textures and global structures. experimental results demonstrate that the proposed model achieves an accuracy of 86.61% on the test set, representing an improvement of 7.8 percentage points compared with the original mobilevitv3 model and significantly outperforming other mainstream disease grading models. this study confirms that the cogfuse-mobilevit model can effectively resolve the issues of blurred edges and inefficient feature extraction in this disease, provides a reliable technical solution for its precision grading, and holds practical application value for the intelligent diagnosis of plant diseases in smart agriculture.\u003c/p>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"journalfulltext\">\u003ca id=\"h2\" name=\"h2\">\u003c/a>\u003ch2>1 introduction\u003c/h2>\u003cp class=\"mb15\">walnut diseases pose a serious threat to walnut production in xinjiang. the spread of crop diseases significantly exacerbates the security risks of the walnut industry if timely prevention and control measures are not taken (\u003ca href=\"#b7\">cooke, 2006\u003c/a>; \u003ca href=\"#b12\">khan et&#xa0;al., 2021\u003c/a>). as a core component of the disease prevention and control system, early precise grading plays a critical role in agricultural production management (\u003ca href=\"#b2\">adaskaveg et&#xa0;al., 2009\u003c/a>; \u003ca href=\"#b6\">chiang et&#xa0;al., 2016\u003c/a>; \u003ca href=\"#b14\">lamichhane, 2014\u003c/a>). walnut brown spot, caused primarily by the fungus \u003ci>ophiognomonia leptostyla\u003c/i>, typically forms specific symptoms on organs such as leaves, flowers, and fruits. among them, leaves, as the primary carrier of plant diseases, exhibit characteristics such as lesion morphology and color changes on their surfaces, which often serve as important bases for disease grading (\u003ca href=\"#b8\">ghaiwat and arora, 2014\u003c/a>; \u003ca href=\"#b35\">weber, 1980\u003c/a>; \u003ca href=\"#b39\">zarei et&#xa0;al., 2019\u003c/a>). traditional disease identification relies on manual experience (\u003ca href=\"#b21\">moragrega et&#xa0;al., 2011\u003c/a>; \u003ca href=\"#b34\">wang et&#xa0;al., 2020\u003c/a>), a method that is not only time- color system to reduce interference from light and leaf veins, and lesion edge detection using the sobel operator, ultimately achieving fast and accurate grading based on the ratio of lesion area to leaf area. \u003ca href=\"#b10\">jadhav and patil (2016)\u003c/a> developed a leaf image partitioning technique based on k-means clustering and squared euclidean distance, automatically quantifying damaged leaf area through the pixel ratio of lesions to leaves for disease grading. \u003ca href=\"#b3\">arivazhagan et al. (2013)\u003c/a> proposed a four-step processing system involving color conversion, green pixel removal, segmentation, and texture feature classification for leaf disease grading. however, traditional methods for acquiring disease information suffer from insufficient segmentation accuracy for plant leaf disease images with complex textures and unclear lesion boundaries, thereby resulting in poor disease severity grading performance.\u003c/p>\u003cp class=\"mb15\">with the continuous advancement of computational power, deep learning has increasingly become a key technology for addressing complex lesion segmentation, primarily due to its superior capability in automatic feature extraction (\u003ca href=\"#b18\">mao et&#xa0;al., 2023\u003c/a>). \u003ca href=\"#b5\">chen et al. (2021)\u003c/a> proposed the blsnet method based on the unet semantic segmentation network, enhancing lesion segmentation accuracy through the introduction of attention mechanisms and multi-scale feature fusion. experiments showed that its segmentation and classification accuracy outperformed benchmark models such as deeplabv3+ and unet, preliminarily verifying the reliability of this method in automatic assessment of bls disease severity. \u003ca href=\"#b22\">ngugi et al. (2020)\u003c/a> developed the kijaninet segmentation network based on fully convolutional neural networks, demonstrating excellent performance in tomato leaf segmentation under complex backgrounds; \u003ca href=\"#b16\">lin et al. (2019)\u003c/a> developed a cnn semantic segmentation model that achieved high-precision pixel-level segmentation of cucumber powdery mildew on leaves. additionally, some studies have fused traditional features with deep learning: \u003ca href=\"#b31\">tripathi (2021)\u003c/a> proposed a convolutional neural network method based on alexnet, achieving disease grading by fusing features extracted by the model with external leaf segmentation features; \u003ca href=\"#b17\">ma et al. (2017)\u003c/a> introduced comprehensive color features (ccf) combining hyper-red index, hsv/h and lab/b components, and achieved lesion segmentation via interactive region growing, with experiments confirming that this method enables accurate grading of disease images such as cucumber downy mildew under actual field conditions. however, although such methods have improved grading accuracy to some extent, they still suffer from insufficient feature capture of blurred disease edges, making it difficult to achieve fine-grained differentiation of subtle differences between disease grades (\u003ca href=\"#b26\">rastogi et&#xa0;al., 2015\u003c/a>).\u003c/p>\u003cp class=\"mb15\">in recent years, the application of deep learning in plant disease classification has continued to expand. \u003ca href=\"#b24\">parashar et al. (2024)\u003c/a> systematically validated the capability of complex feature modeling for crop yield prediction, further substantiating the critical role of adaptive feature extraction in agricultural intelligent decision-making. concurrently, \u003ca href=\"#b32\">vishnoi et al. (2022)\u003c/a> enhanced small-target detection precision in apple disease identification through a spatial attention mechanism-based cnn architecture for leaf disease diagnosis. \u003ca href=\"#b23\">ozturk et al. (2025)\u003c/a> constructed an ensemble learning classification model based on resnet50, mobile net, efficientnetb0, and densenet121, enhancing generalization performance through statistical cross-validation and improving decision interpretability via grad-cam visualization. experimental results showed that the ensemble model achieved stable high-precision classification performance across multiple validation rounds. these studies provide robust and interpretable solutions for intelligent plant disease recognition through technical integration and architectural innovation. \u003ca href=\"#b9\">hu et al. (2021)\u003c/a> integrated retinex enhancement, faster r-cnn detection, and vgg16 classification to improve the grading and detection accuracy of blurred diseased leaves. \u003ca href=\"#b25\">picon et al. (2019)\u003c/a> proposed an adaptive deep residual neural network algorithm for the classification of three european wheat diseases (septoria leaf blotch, brown spot, and rust) in real-world scenarios, which effectively improved the classification accuracy of wheat diseases. \u003ca href=\"#b13\">kim and ahn (2021)\u003c/a> employed deep cnn architectures such as resnet, xception, and densenet, combined with transfer learning and fine-tuning, to classify 9 categories of pests, diseases, and healthy states in tomatoes. although densenet combined with the rmsprop algorithm achieved an accuracy of 98.63%, single cnn architectures have clear bottlenecks in handling complex plant lesions and overlapping multiple diseases, including insufficient specificity in feature extraction and limited ability to distinguish subtle differences between lesions. \u003ca href=\"#b28\">shi et al. (2023)\u003c/a> analyzed the application of cnns in evaluating the severity of plant diseases, pointing out that hybrid architectures such as classical cnns, improved cnns, and segmentation networks have limitations in handling complex plant lesions: classification errors caused by concurrent multiple diseases, as well as constraints on model generalization ability due to unbalanced datasets and insufficient annotation accuracy. these issues significantly restrict their precise application in practical agricultural scenarios.\u003c/p>\u003cp class=\"mb15\">although significant advancements have been made in plant lesion segmentation and disease classification using deep learning, the grading task for walnut leaf brown spot disease&#x2014;characterized by blurred edge representation and complex small lesions&#x2014;still faces notable challenges. current mobilevit-based hybrid models fall into two categories: general architectures (mobilevitv1/v2/v3) focus on optimizing the cnn-transformer balance for natural images; domain-improved models (mobile-former/edgevit) are oriented toward medical/industrial tasks, with their task focus on localization rather than grading, which mismatches the pain points and fails to address the blurriness of agricultural lesions, tissue interference, and morphological variations. thus, the present study proposes a novel cogfuse-mobilevit model specifically designed for disease grading, with its specific contributions as follows:\u003c/p>\u003cp style=\"margin-top:1em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">1. a diverse field-collected walnut leaf dataset was constructed, with images divided into four distinct severity levels based on qualitative assessment of disease severity, ensuring the accuracy of disease severity grading.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">2. the hierarchical feature selection module (hfsm) enhances the fusion of local details (such as lesion texture and color) with global context through local and global attention mechanisms and task-driven feature selection, while suppressing interference from healthy regions.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">3. the edge convolution fusion module (ecfm) strengthens edge details through sobel convolution and residual connections, achieving effective integration of edge-specific features with general features, further enhancing edge details and enabling more precise capture of lesion contour details.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:1em;margin-left:1em;text-indent:-1em;text-align:left\">4. the adaptive multi-scale dilated dense inception convolution module (amsddicm) extracts differentiated features using multi-shaped convolution kernels and adaptively fuses multi-scale information via a dynamic weight mechanism, capturing lesion morphologies at different pathogenesis stages.\u003c/p>\u003ca id=\"h3\" name=\"h3\">\u003c/a>\u003ch2>2 materials and methods\u003c/h2>\u003ch3>2.1 characteristics of walnut leaf brown spot and classification of disease severity levels\u003c/h3>\u003cp class=\"mb0\">in the local standard technical regulations for prevention and control of walnut brown spot (\u003ca href=\"#b19\">mcgranahan and leslie, 1991\u003c/a>), walnut brown spot is divided into four severity levels. the severity of plant leaf diseases is generally determined using the spot coverage method. in this study, walnut leaves with different disease severities were processed to separate disease-infected spots from healthy leaf areas, and the percentage of lesion area to total leaf area was calculated. with reference to the local standard, the disease grades specified in the standard were re-determined through calculations in this study, as shown in \u003ca href=\"#t1\">table&#xa0;1\u003c/a>. classification of different severity levels of walnut leaf brown spot was conducted in accordance with technical regulations for prevention and control of walnut brown spot (\u003ca href=\"#b33\">wang et&#xa0;al., 2022\u003c/a>; \u003ca href=\"#b37\">yang et&#xa0;al., 2021\u003c/a>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;1\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" name=\"table&#xa0;1\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" alt=\"www.frontiersin.org\" id=\"t1\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;1\u003c/b>. walnut leaf brown spot disease severity grading criteria.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">walnut leaf brown spot is caused by infection with the fungus \u003ci>ophiognomonia leptostyla\u003c/i> (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>). in the early infection stage, near-circular or irregular small spots appear on the leaves, with a gray-brown center and dark yellow-green to purplish-brown edges, and diseased leaves tend to fall off prematurely. in the middle stage, elongated elliptical or irregular slightly sunken dark brown lesions form, with larger spot size and light brown edges; longitudinal cracks are often present in the center of the lesions. in the late stage, lesions often coalesce to form large scorched necrotic areas, surrounded by yellow to golden-yellow zones, and small black granules (conidiomata and conidia of the pathogen) are scattered on the surface of the diseased tissue (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>). according to the degree of color and texture feature changes in infected leaves, walnut leaf disease is divided into four stages: healthy, early, middle, and late stages, corresponding to disease severity levels: healthy (level 0), mild (level 1), moderate (level 2), and severe (level 3). the different severity levels of walnut leaf brown spot are shown in \u003ca href=\"#f1\">figure&#xa0;1\u003c/a>.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;1\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" alt=\"four leaves are displayed, labeled a to d. a shows a healthy leaf with no spots. b features a mildly infected leaf with a few small spots. c presents a moderately infected leaf with numerous spots. d shows a severely infected leaf covered with dark spots.\" id=\"f1\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;1\u003c/b>. walnut leaves infected with brown spot disease at different severity levels. \u003cb>(a)\u003c/b> healthy leaves; \u003cb>(b)\u003c/b> mildly infected leaves; \u003cb>(c)\u003c/b> moderately infected leaves; \u003cb>(d)\u003c/b> severely infected leaves.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">\u003ca href=\"#t1\">table&#xa0;1\u003c/a> shows the ratio of walnut leaf brown spot lesion area to total leaf area. level 0 (healthy): healthy leaves without symptoms. level 1 (mild): near-circular or irregular small white spots appear on leaves, accounting for less than 5% of the leaf area. level 2 (moderate): lesions expand into irregular dark brown spots, covering 5% to 30% of the leaf area. level 3 (severe): abundant lesion coalesce to form large scorched necrotic areas, exceeding 30% of the leaf area.\u003c/p>\u003ch3>2.2 calculation algorithm for walnut leaf brown spot disease severity levels\u003c/h3>\u003cp class=\"mb0\">after capturing walnut leaf brown spot samples using a camera, to minimize subjective bias, this study strictly adhered to the objective quantitative criteria defined in the technical regulations for the control of walnut brown leaf spot (\u003ca href=\"#t1\">table&#xa0;1\u003c/a>), utilizing the lesion area percentage (k) as the primary grading indicator. we re-determined the severity levels through computational analysis, with the specific determination process shown in \u003ca href=\"#f2\">figure&#xa0;2\u003c/a>.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;2\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" alt=\"flowchart illustrating a process for identifying disease areas on leaves. images of leaves are converted through morphological color space to grayscale. the grayscale is used to highlight disease area s1 and leaf area s2. the ratio s1/s2 is applied, producing a final analysis image showing leaf disease marked in red.\" id=\"f2\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;2\u003c/b>. computational method for different severity levels of brown spot infection in walnut leaves.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">first, a python-based color segmentation algorithm was employed to convert images from the bgr color space to the hsv color space for processing. the hsv color space offers inherent advantages in color segmentation tasks. by analyzing the hue, saturation, and value characteristics of diseased regions, the color threshold range in the hsv space was determined (\u003ca href=\"#b4\">chaudhary et&#xa0;al., 2012\u003c/a>). after generating an initial disease region mask based on this threshold range, morphological operations such as closing and opening were applied to optimize mask quality, effectively eliminating noise while preserving the integrity of lesion contours. subsequently, the original image was converted to grayscale mode, and the leaf region was extracted using an adaptive threshold binarization algorithm. leaf boundaries were localized via contour detection technology, and a complete leaf mask was generated through contour filling. pixel statistical analysis was performed on both the leaf mask and disease mask to calculate the total leaf area and diseased region area, respectively. when a valid leaf area (&gt;0) was detected, the disease severity index was calculated using the following formula: disease severity index \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im1\">\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmo mathsize=\"10.5pt\">%\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">s\u003c/mi>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003c/mrow>\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">s\u003c/mi>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmn mathsize=\"10.5pt\">100\u003c/mn>\u003cmo mathsize=\"10.5pt\">%\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;&#xa0;&#xa0;s\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>: total area of diseased regions \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im2\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>: total area of the complete leaf.\u003c/p>\u003ch3>2.3 dataset construction\u003c/h3>\u003cp class=\"mb0\">data were collected from the walnut plantation base of tarim university (alar, xinjiang) between may and october 2024. leaves from three locally dominant early-fruiting cultivars (&#x2018;wen 185&#x2019;, &#x2018;xinxin 2&#x2019;, &#x2018;zha 343&#x2019;)widely cultivated in southern xinjiang were vertically photographed using an iphone 13. this genotypic diversity ensures model generalizability by encompassing varied disease phenotypes. the orchard follows standardized cultivation with 4m plant spacing, 5m row spacing (&#x2248;50 plants/mu), and conventional management practices. sampled trees were in full fruiting stage. to capture disease traits across microenvironments, samples were collected from safely accessible crown layers. the dataset includes healthy and brown spot-infected leaves three severity levels, spanning varied time periods, light conditions, and angles. after removing duplicates and invalid images,5,120 high-quality jpgs were retained for analysis.\u003c/p>\u003ch4>2.3.1 development of the walnut leaf brown spot dataset\u003c/h4>\u003cp class=\"mb0\">disease severity grading was established through quantitative lesion area analysis (\u003ca href=\"#f2\">figure&#xa0;2\u003c/a>). a representative subset of 512 images (10% of the full 5,120-image dataset) was selected via stratified random sampling, accurately preserving the original severity distribution. to ensure labeling rigor, two plant protection specialists (&gt;5 years&#x2019; expertise in walnut pathology, certified in local protocols) executed standardized annotation: pre-annotation training unified diagnostic criteria for ambiguous cases, with formal annotation commencing only after achieving pre-calibration inter-rater agreement (kappa coefficient &#x2265;0.75). as validated in \u003ca href=\"#t2\">table&#xa0;2\u003c/a>, dual statistical metrics confirmed gold-standard reliability&#x2014;inter-rater cohen&#x2019;s kappa reached 0.71, meeting landis &amp; koch&#x2019;s &#x201c;substantial agreement&#x201d; threshold; algorithm-expert consensus attained a weighted fleiss&#x2019; kappa of 0.77, substantially outperforming conventional cohen&#x2019;s kappa in multi-annotator scenarios. a three-stage standardization pipeline eliminated preprocessing discrepancies. the final dataset, partitioned into training/testing sets (8:2 ratio), strictly adheres to plant disease survey protocols and provides benchmarked data for deep learning model development (\u003ca href=\"#t3\">table&#xa0;3\u003c/a>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;2\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" name=\"table&#xa0;2\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" alt=\"www.frontiersin.org\" id=\"t2\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;2\u003c/b>. algorithm vs. expert consensus agreement evaluation.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;3\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" name=\"table&#xa0;3\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" alt=\"www.frontiersin.org\" id=\"t3\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;3\u003c/b>. walnut leaf brown spot disease image acquisition data.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>2.4 construction of walnut leaf brown spot disease severity grading model\u003c/h3>\u003ch4>2.4.1 the optimized mobilevitv3 network: cogfuse-mobilevit\u003c/h4>\u003cp class=\"mb15\">to address the challenge of difficult feature capture for small lesions in walnut leaf brown spot severity grading, this study proposes an innovative model, cogfuse-mobilevit. mobilevitv3 was selected as the backbone network due to its suitability for walnut brown spot grading. its hybrid mobilenet-vit architecture integrates cnn-based local feature extraction essential for lesion detail capture with transformer-enabled global contextual modeling critical for analyzing lesion spatial distribution. this design aligns with pathological requirements for severity grading, necessitating concurrent attention to local lesion characteristics and global infection patterns. the lightweight architecture further supports real-time processing on embedded devices, enabling future field deployment.\u003c/p>\u003cp class=\"mb0\">the model embodies a &#x201c;grading task-driven&#x201d; design principle. conceptually, it establishes a disease-specific framework of &#x201c;hierarchical screening to edge enhancement to dynamic fusion.&#x201d; this framework elevates general feature extraction to targeted solutions addressing three major agricultural challenges: suppressing interference from healthy tissues via hierarchical screening resolving feature confusion; strengthening blurred lesion contours through edge enhancement overcoming the bottleneck of edge blurriness; adaptively handling multi-stage morphological variations using dynamic fusion addressing morphological variation challenges. structurally, as illustrated in \u003ca href=\"#f3\">figure&#xa0;3\u003c/a>, the network implements a progressive feature extraction strategy. the hierarchical feature selection module (hfsm) first fuses shallow and middle-layer features. it employs a hierarchical attention mechanism to enhance semantic consistency while preserving spatial details. the edge convolution fusion module (ecfm) then processes these features. it utilizes learnable sobel operators to extract edge information, which is fused with conventional convolutional features via residual connections to augment edge perception capability. finally, the adaptive multi-scale dilated inception convolution module (amsddicm) enables adaptive processing of multi-scale edge features. this module is capable of both capturing fine edge changes in minute lesions and grasping the overall contour structure of large lesions, thereby comprehensively covering edge features across different developmental stages. based on these enhanced edge features, the network accurately outputs the final disease severity grade.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;3\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" alt=\"diagram illustrating a deep learning process for leaf classification. input images of leaves undergo convolution and mobilevit blocks, reducing dimensions and processing through hfsm, ecfm, and amsddicm modules. outputs are pooled and linearly transformed to logits.\" id=\"f3\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;3\u003c/b>. cogfuse-mobilevit architecture diagram (hfsm suppresses healthy regions through dynamic masks, ecfm explicitly extracts edges via sobel convolution, and amsddicm fuses multi-scale features through dynamic weights).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>2.4.2 the hierarchical feature selection module\u003c/h4>\u003cp class=\"mb0\">the hfsm (hierarchical feature selection module) is an innovative architecture proposed in this study. unlike mobilevit&#x2019;s direct feature concatenation, hfsm utilizes learnable prompt vectors (\u003ca href=\"#eq1\">equation 1\u003c/a>) to generate spatial masks, dynamically suppressing non-lesion regions. such task-driven selection is crucial for tiny objects. in the grading task of walnut leaf brown spot disease, this module utilizes a local attention mechanism to focus on micro-regions of walnut leaves, fusing shallow and middle-layer features. meanwhile, the hierarchical feature selection mechanism enables precise capture of local detail features such as lesion texture and color, effectively suppressing interference from healthy leaf regions and enhancing disease features. this provides high-discriminative feature representations for brown spot disease grading while preparing for subsequent lesion edge processing. as depicted in \u003ca href=\"#f4\">figure&#xa0;4\u003c/a>, the module processes two hierarchical feature maps. initial 1&#xd7;1 convolutions reduce both maps&#x2019; channels to half the output dimension curtailing computational load. one reduced map undergoes bilinear upsampling for spatial alignment with the other. these aligned features are summed and processed by a 3&#xd7;3 convolution to generate base-path features. simultaneously, the original reduced map and upsampled map are fed into dual local-global attention modules for parallel processing (\u003ca href=\"#b36\">xu, 2024\u003c/a>). each group contains local and global branches. during branch processing, the feature map is partitioned into p&#xd7;p non-overlapping patches \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im3\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">p\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmi mathsize=\"10.5pt\">j\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math> via the unfold operation. after calculating the mean of pixel features within each patch, the result is processed using the following core formula: attention distribution generation.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;4\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" alt=\"diagram illustrating a neural network architecture with multiple components. the upper section shows two convolution layers, lga modules, concatenation, and rep blocks. the lower section features processes like unfold, mean calculation, softmax, and feature selection, highlighting token and channel selection. various operations and components like multiplication, addition, and dropout are labeled, with symbols explaining their functions.\" id=\"f4\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;4\u003c/b>. architecture diagram of the hierarchical feature selection module (hfsm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">z\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ml\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">layernor\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ml\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">mean\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>1\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">a\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">softmax\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">z\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>2\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq1\">equations 1\u003c/a> and \u003ca href=\"#eq2\">2\u003c/a> convert the mean value of patch features into a high-dimensional feature vector through multi-layer perceptrons (mlp) and layer normalization (layernorm) (wherein(\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im4\">\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">mlp\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;and&#xa0;&#xa0;mlp\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>) \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im5\">\u003cmrow>\u003cmsup>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">achieve&#xa0;dimensional&#xa0;transformation&#xa0;p\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msup>\u003cmo mathsize=\"10.5pt\">&#x2192;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ouc\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmo mathsize=\"10.5pt\">&#x2192;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ouc\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/mrow>\u003c/math>)), and then generates the attention distribution via the softmax function \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im6\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">a\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmi mathsize=\"10.5pt\">j\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>. this mechanism enables the model to focus on the key features of lesion regions, weaken the interference from healthy leaf areas, enhance the specificity of feature selection, and provide high-quality features for subsequent precise grading. after the local and global features output by the two modules are spliced along the channel dimension, they are combined with the basic path features to generate the output features of the final processing module.\u003c/p>\u003cp class=\"mb0\">for leaf images with over 80% healthy tissue, dynamic masks are generated via learnable prompt vectors to suppress green texture features while enhancing the gray-brown features of lesions (\u003ca href=\"#b19\">mcgranahan and leslie, 1991\u003c/a>).\u003c/p>\u003ch4>2.4.3 ecfm edge convolutional fusion module\u003c/h4>\u003cp class=\"mb0\">in the walnut leaf brown spot grading task, lesion edge features are crucial for accurately determining disease severity. the standard mobilevit relies on cnn-transformer blocks to implicitly learn edges, whereas the ecfm (edge convolution fusion module) incorporates sobel convolutions, conventional convolutions, and residual connections, effectively fusing edge features with general features and thereby enhancing edge details. as shown in \u003ca href=\"#f5\">figure&#xa0;5\u003c/a>, the sobel branch employs sobel convolution to extract image edge information, accurately capturing the contour details of brown spot lesions; the convolutional branch captures general features such as leaf color and texture through standard convolution. the two realize feature fusion via the following core formula (\u003ca href=\"#eq3\">equation 3\u003c/a>):\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;5\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" alt=\"diagram showing a network flow. input x is split into two paths: one goes through a sobel filter and the other through a convolution (conv) layer. outputs are concatenated (concat) and followed by another conv layer. the result is summed with a bypass connection and passed through a final conv layer.\" id=\"f5\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;5\u003c/b>. architecture diagram of the edge convolutional fusion module (ecfm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">sobelconv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>3\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">processing the input feature map \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im7\">\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmi mathsize=\"10.5pt\">x\u003c/mi>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> through the sobel convolution operator (sobelconv) specifically extracts edge information of lesions (such as the boundary contours between lesions and healthy tissues, and the edge textures of small lesions). for the commonly seen blurred edges in brown spot disease, sobel convolution can enhance edge gradient changes, making the originally blurred lesion contours clearer, thus providing key edge feature support for subsequent grading. conventional feature extraction and fusion (\u003ca href=\"#eq4\">equation 4\u003c/a>):\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>4\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im8\">\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;c\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> extracting the overall features of leaves (such as the color distribution of lesions and the overall texture of leaves) through standard convolution forms a complement to edge features, preventing the model from focusing only on local edges while ignoring global lesion information.\u003c/p>\u003cp class=\"mb15\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im9\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> concatenating the edge feature s and the conventional feature c along the channel dimension achieves the initial fusion of edge details and global features. this fusion enables the model to both identify the fine contours of lesions and judge the disease condition by combining the overall color and texture changes of lesions, thereby improving the accuracy of grading. the module also introduces feature addition and subsequent convolution operations: the fused features are first processed by the first convolution layer \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im10\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math>, the corresponding operation results are added to the original input, and the final output is generated through the second convolution layer \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im11\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">final\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math>. in this process, the residual connection retains the original feature information, further strengthens the integration of edge features and conventional features, and ensures the effective transmission and enhancement of multi-level features.\u003c/p>\u003cp class=\"mb0\">the edge gradient information extracted by sobel convolution (such as grayscale differences between diseased spots and healthy tissues) and the texture features from conventional convolution (such as the roughness of necrotic areas) are fused via residual connections. this not only preserves the blurred edges of early-stage lesions but also prevents edge features from being disconnected from the overall texture (\u003ca href=\"#b8\">ghaiwat and arora, 2014\u003c/a>).\u003c/p>\u003ch4>2.4.4 amsddicm adaptive multi-scale dilated depthwise inseparable convolution module\u003c/h4>\u003cp class=\"mb0\">this module differs from mobilevit&#x2019;s single-scale convolution, leverages multi-shaped convolution kernels to accurately extract lesion features of circular, irregular, and other shapes from multiple dimensions, enabling the identification of subtle differences in lesion edges and internal colors to enrich feature dimensions. meanwhile, depth wise separable convolutions are employed to accommodate lesion scales at different stages of disease development. specifically, the input features are first split into two groups, each entering the amsddicm module to extract features via multi-scale depth wise separable convolutions. a dynamic weighting mechanism (global pooling + convolution + softmax) is used to generate weights for fusing multi-scale features. the processed features from the two groups are concatenated, and finally, cross-channel fusion is completed via 1&#xd7;1 convolution to output the final optimized features. the entire process integrates multi-scale convolution, dynamic weight allocation, and feature fusion to enhance the model&#x2019;s ability to capture complex features, as shown in \u003ca href=\"#f6\">figure&#xa0;6\u003c/a>. in response to the morphological differences between early-stage small spots (1-3mm in diameter) and late-stage fused spots (over 20mm in diameter), the dynamic weight mechanism enables the model to adaptively allocate the contributions of 3&#xd7;3 kernels for capturing local textures and 11&#xd7;1 kernels for extracting strip-shaped spreading features. this addresses the limitation of fixed weights in traditional inception architectures in adapting to multi-scale morphologies.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;6\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" alt=\"diagram showing a neural network process. the top section involves splitting input channels, applying a dms 2d convolution, concatenation, and another convolution. the bottom section includes a pooling layer, convolution, rearrangement, softmax, summation, batch normalization, and activation. the process results in output \\(x^{''}\\).\" id=\"f6\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;6\u003c/b>. architecture diagram of the adaptive multi-scale dilated depth wise inseparable convolution module (amsddicm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">when the input feature tensor x with the number of channels c enters the amsddicm module, it first undergoes a feature grouping operation: the input features are evenly split into two groups along the channel dimension, with each group containing half of the original number of channels (c//2). this grouping strategy not only reduces computational complexity but also creates conditions for subsequent multi-scale feature extraction. each feature group is fed into an independent dmsconv2d module, which adopts different configurations such as 3&#xd7;3, 5&#xd7;5 square convolution kernels and 1&#xd7;11, 11&#xd7;1 strip-shaped convolution kernels &#x2014; the square kernels capture local textures, while the strip-shaped kernels extract direction-sensitive long-range dependencies. this dynamic weight allocation draws inspiration from adaptive strategies in agricultural iot systems. similar to how salinity-aware ets models prioritize soil conductivity features under salt stress (\u003ca href=\"#b40\">zhang et&#xa0;al., 2023\u003c/a>), our mechanism selectively amplifies lesion morphological features critical for severity grading. the dynamic weighting mechanism of dmsconv2d generates weights through global average pooling and 1&#xd7;1 convolution, with the core formulas as follows:\u003c/p>\u003cp class=\"mb15\">weighted basic feature generation:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">rearrange\u003c/mtext>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003c/msub>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">avgpoo\u003c/mtext>\u003cmn mathsize=\"10.5pt\">l2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">d\u003c/mtext>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>5\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq5\">equation 5\u003c/a> compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information (such as the overall distribution characteristics of lesions at different scales), and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmo mathsize=\"10.5pt\">.\u003c/mo>\u003cmtext mathsize=\"10.5pt\">softmax\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>6\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq6\">equation 6\u003c/a> compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information, such as the overall distribution characteristics of lesions at different scales, and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">0\u003c/mn>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msubsup>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">dwcon\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">v\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>7\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb0\">\u003ca href=\"#eq7\">equation 7\u003c/a> is based on dynamic weights \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im12\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>) for different convolution kernels \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im13\">\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;dwconv\u003c/mtext>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> weighted fusion is performed on the extracted features. for walnut brown spot lesions exhibiting size heterogeneity and morphological complexity&#x2014;ranging from early-stage circular micro-lesions to late-stage coalesced irregular lesions&#x2014;this mechanism adaptively modulates multi-scale feature contributions. it prioritizes retention of morphology-discriminative features, local textures in small lesions or directional distributions in large lesions, thereby comprehensively capturing pathological characteristics across developmental stages. the processed features undergo channel-wise concatenation to generate optimized outputs (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>).\u003c/p>\u003ch3>2.5 experimental process for severity grading of walnut leaf brown spot disease\u003c/h3>\u003cp class=\"mb0\">\u003ca href=\"#f7\">figure&#xa0;7\u003c/a> is the overall flow chart of severity grading of walnut leaf spot disease. first is the image acquisition stage, where raw images of walnut leaves are captured and collected to construct an image database containing pictures to be classified (\u003ca href=\"#b29\">singh et&#xa0;al., 2019\u003c/a>). next is the image preprocessing stage, which involves sequentially calculating and classifying disease severity levels, establishing image labels, and performing image preprocessing operations. subsequently, the processed data are used to train the constructed dataset. finally, in the model training and performance evaluation stage, the preprocessed images are input into the model for classifying walnut leaf brown spot disease, after which performance evaluation is conducted on the model&#x2019;s classification results to determine the model&#x2019;s effectiveness and accuracy.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;7\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" alt=\"flowchart illustrating a process for walnut leaf brown spot disease classification. it consists of three main phases: image acquisition, preprocessing, and model training and evaluation. the image acquisition phase involves collecting walnut leaf images and storing them in a database. the preprocessing phase includes classification of disease grades, establishing image labels, and training the dataset. finally, the model classifies the disease, resulting in output images and a performance evaluation matrix for accuracy verification. the chart uses labeled boxes, arrows, and image samples for illustration.\" id=\"f7\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;7\u003c/b>. overall flow chart for severity grading of walnut leaf brown spot disease.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>2.6 experimental parameters and evaluation metrics\u003c/h3>\u003ch4>2.6.1 test environment and hyperparameter setting\u003c/h4>\u003cp class=\"mb15\">the experimental setup of this study is based on deep learning technology and leverages high-performance computing resources. all experiments were conducted on the windows 10 operating system. the hardware platform consists of an amd ryzen 7 3700x processor and an nvidia rtx 2080ti graphics card. the software environment was built using python 3.8, the pytorch 1.13.0 deep learning framework, and the cuda 11.3 parallel computing platform. additionally, pytorch 1.13.0&#x2014;a widely adopted open-source deep learning library renowned for its high flexibility&#x2014;was selected, making it well-suited for research and development.\u003c/p>\u003cp class=\"mb0\">during model training, multiple adjustments were made to the hyperparameters to compare test results and select the optimal hyperparameter combination. the model accepts input images sized at 224&#xd7;224 pixels, with a configured batch size of 32 during training and a maximum of 100 training epochs. the optimizer employs an initial learning rate of 0.003.\u003c/p>\u003ch4>2.6.2 evaluation metrics\u003c/h4>\u003cp class=\"mb15\">this paper aims to evaluate the performance of the model and verify the effectiveness of the improvement measures. we selected multiple evaluation metrics, and all subsequent experimental results adopt the method of calculating averages via 5-fold cross-validation, aiming to comprehensively and reliably evaluate the model performance. including precision (p), recall (r), etc. (\u003ca href=\"#eq8\">equations 8\u003c/a>&#x2013;\u003ca href=\"#eq16\">16\u003c/a>) these metrics can be calculated using the following formulas.\u003c/p>\u003cp class=\"mb15\">the arithmetic mean of the metric values across the five folds:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmover accent=\"true\">\u003cmtext mathsize=\"10.5pt\">m\u003c/mtext>\u003cmo mathsize=\"10.5pt\">&#xaf;\u003c/mo>\u003c/mover>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmn mathsize=\"10.5pt\">5\u003c/mn>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">5\u003c/mn>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">m\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>8\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">precision\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>9\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">recall\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fn\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>10\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">score\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003c/mrow>\u003cmrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fn\u003c/mtext>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>11\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">macro&#xa0;precision=\u003c/mtext>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>12\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">macro&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">r\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>13\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;precision=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&#xb7;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">precisio\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>14\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&#xb7;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">precisio\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>15\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmsubsup>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003cmtext mathsize=\"10.5pt\">t\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>16\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003ca id=\"h4\" name=\"h4\">\u003c/a>\u003ch2>3 experiments and results analysis\u003c/h2>\u003ch3>3.1 core module design and validity experimental verification\u003c/h3>\u003ch4>3.1.1 comparative test of necessity of hfsm module\u003c/h4>\u003cp class=\"mb0\">to validate the necessity of the hierarchical feature selection module (hfsm) within the cogfuse-mobilevit framework, this study conducted comparative tests against two mainstream lightweight attention modules: se (squeeze-and-excitation) and cbam (convolutional block attention module). as illustrated in \u003ca href=\"#t4\">table&#xa0;4\u003c/a>, the hfsm module achieves a significantly higher accuracy of 86.61%, outperforming the se module and cbam module by 7.38% and 4.46%, respectively. this demonstrates that its hierarchical feature selection mechanism more effectively captures discriminative features. in terms of computational efficiency, the parameters and flops of hfsm remain comparable with those of the se module and cbam module, indicating that its performance breakthrough stems from innovative structural design under equivalent lightweight constraints. comprehensive results confirm that replacing hfsm with se or cbam modules would incur a performance degradation exceeding 4%, thereby validating the indispensable value of hfsm in enabling efficient feature selection within the cogfuse-mobilevit framework.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;4\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" name=\"table&#xa0;4\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" alt=\"www.frontiersin.org\" id=\"t4\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;4\u003c/b>. performance comparison of attention modules within the cogfuse-mobilevit framework.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.2 convolutional kernel selection in amsddicm\u003c/h4>\u003cp class=\"mb0\">rigorous validation via kernel combination ablation studies (\u003ca href=\"#t5\">table&#xa0;5\u003c/a>) demonstrates. the hybrid configuration (3&#xd7;3 \u003cb>+\u003c/b> 5&#xd7;5 \u003cb>+\u003c/b> 1&#xd7;11) significantly outperformed square-only kernels (3&#xd7;3 \u003cb>+\u003c/b> 5&#xd7;5) accuracy 86.61% and 82.15% stage-specific recall gains. mid-stage lesions (level 2) raise 9.4 percentage points. late-stage lesions (level 3) raise 6.1 percentage points. this empirically validates the necessity of 1&#xd7;11 rectangular kernels for capturing linear pathological features, overcoming the limitation of isotropic kernels in detecting anisotropic structures.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;5\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" name=\"table&#xa0;5\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" alt=\"www.frontiersin.org\" id=\"t5\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;5\u003c/b>. comparison of different convolution kernels in amsddicm.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.3 the impact of new modules on computational complexity\u003c/h4>\u003cp class=\"mb0\">\u003ca href=\"#f8\">figure&#xa0;8\u003c/a> shows that the hfsm module incurs a 169% flops increase to achieve a breakthrough improvement in early-stage lesion detection&#x2014;reducing level 0/1 misclassification by 24%, thereby establishing the pathological foundation for severity grading. the ecfm module contributes a mere 3% flops increment yet drives a 3.68 percentage point accuracy gain through enhanced edge feature representation. with only a 0.028g flops overhead 1.3%, the amsddicm module enables adaptive fusion of multiscale pathological deformations, culminating in a 7.80-pp accuracy leap. these modules form a cascaded optimization paradigm: hfsm&#x2019;s substantial cost resolves the core pathological bottleneck, while subsequent modules deliver superlinear returns&#x2014;harvesting 6.85-pp accuracy gain with just 14% additional flops&#x2014;collectively establishing the globally optimal computation-performance equilibrium.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;8\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" alt=\"bar graphs comparing different configurations of mobilevit models across three metrics. panel a shows accuracy percentages, with values from 78.81% to 86.61%. panel b displays flops, ranging from 0.74g to 2.1g. panel c illustrates parameters in millions, with values between 1.94m and 2.02m. each graph compares mobilevit, mobilevit plus hfsm, mobilevit plus hfsm plus ecfm, and cogfuse-mobilevit.\" id=\"f8\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;8\u003c/b>. comparison of the impact of each newly added module on computational complexity \u003cb>(a)\u003c/b> accuracy (%); \u003cb>(b)\u003c/b> flops (g); \u003cb>(c)\u003c/b> params (m). measured on nvidia rtx 2080ti gpu (pytorch 1.13.0, cuda 11.3) with 224&#xd7;224 input.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.4 comparison of the influence of different module fusion on model performance\u003c/h4>\u003cp class=\"mb0\">to validate the effect of module fusion, \u003ca href=\"#t6\">table&#xa0;6\u003c/a> compares model performances with different combinations. when only hfsm (hierarchical feature selection module) is introduced, precision increases from the baseline of 82.23% to 83.77%, but recall decreases by 3 percentage points to 72.31%, causing the f1 score to slightly decline to 78.04%. accuracy rises to 82.15%, indicating improved overall classification correctness of the model, but with potential risk of missed detections.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;6\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" name=\"table&#xa0;6\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" alt=\"www.frontiersin.org\" id=\"t6\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;6\u003c/b>. impact of fusion of different modules on model performance.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">when hfsm and ecfm (edge-context feature module) are synergistically introduced, precision increases to 86.34%, recall recovers to 74.69%, and both f1 score and accuracy are significantly optimized. hfsm fuses shallow and middle-layer features through a hierarchical attention mechanism, laying the foundation for subsequent edge feature processing; ecfm enhances lesion edge features. the combination of the two effectively improves the integrity of feature representation and the clarity of lesion boundaries.\u003c/p>\u003cp class=\"mb15\">the combination of hfsm and amsddicm (adaptive multi-scale dilated depthwise inseparable convolution module) further pushes recall to 76.24%, accuracy to 83.94%, and f1 score to 80.79%, outperforming the hfsm+ecfm combination. amsddicm compensates for hfsm&#x2019;s deficiency in local feature refinement through attention-guided multi-scale detail fusion, which integrates multi-layer detail feature weights, especially suitable for scenarios with small targets or blurred features.\u003c/p>\u003cp class=\"mb0\">when the three modules work synergistically, all indicators reach optimal levels: precision, recall, f1 score, and accuracy increase by 12.19%, 7.67%, 9.62%, and 10.00% respectively compared with the baseline. among them, hfsm lays the foundation for cross-layer feature fusion, ecfm effectively integrates edge-specific features with general features to enhance image edge information, and amsddicm adaptively fuses multi-scale and multi-type features through an attention mechanism, forming a progressive optimization chain from &#x201c;hierarchical feature extraction&#x201d; to &#x201c;edge semantic enhancement&#x201d; and then to &#x201c;multi-layer detail fusion&#x201d;. the experimental results show a balanced improvement in both precision and recall, indicating that the fusion of the three modules enables the model to focus more on learning the features of small brown spot lesions, thereby improving its classification performance. this synergistic task-specific design, combining hierarchical selection, explicit edge enhancement, and adaptive multi-scale fusion, fundamentally distinguishes cogfuse-mobilevit from prior mobilevit hybrids designed for general vision or localization tasks.\u003c/p>\u003ch4>3.1.5 influence of different module combinations on f1-score of level (0-3)\u003c/h4>\u003cp class=\"mb0\">in order to verify the targeted improvement of each module for a specific disease level, we conducted the following comparative experiments as shown in the \u003ca href=\"#t6\">table&#xa0;6\u003c/a>, when hfsm is introduced alone, the f1-score of level 0 increases from 83.10% to 85.60%, effectively reducing feature confusion between healthy leaves and early-stage lesions. after adding ecfm, the f1-score of level 1 rises from 73.20% (with hfsm alone) to 79.80%, significantly mitigating the recognition bias caused by blurred edges of small lesions. amsddicm increases the f1-score of level 3 from 80.37% to 85.54%, enhancing the adaptability to the morphology of large-scale fused scorched areas. with the collaboration of the three modules, the f1-scores of level 0&#x2013;3 reach 93.99%, 82.57%, 84.28%, and 85.54% respectively, achieving balanced optimization of performance across all levels.\u003c/p>\u003ch3>3.2 results comparison of different algorithms and statistical significance verification\u003c/h3>\u003ch4>3.2.1 comparison of grading results for different classification models\u003c/h4>\u003cp class=\"mb0\">to verify the effectiveness of the cogfuse-mobilevit model, we selected 9 commonly used classification models to compare with the optimized cogfuse-mobilevit model, and the results are the average of 5-fold cross-validation over 120 epochs. as shown in \u003ca href=\"#t7\">table&#xa0;7\u003c/a>, the proposed cogfuse-mobilevit model achieved the highest grading performance in terms of precision, recall, and f1-score for identifying walnut leaf brown spot disease at different severity levels among all compared models. the improved cogfuse-mobilevit model exhibits an accuracy of 86.61%, representing a 7.80-percentage-point improvement over the original model. overall, the experimental results highlight that the enhanced cogfuse-mobilevit model is more conducive to focusing on lesion edge details and accurately learning the features of different severity levels of walnut leaf brown spot disease, thereby improving the model&#x2019;s classification performance.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;7\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" name=\"table&#xa0;7\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" alt=\"www.frontiersin.org\" id=\"t7\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;7\u003c/b>. comparison of grading results for different classification models.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.2.2 comparison of performance and reliability validation of different algorithms\u003c/h4>\u003cp class=\"mb0\">in model training, to quantify the reliability of results and avoid random biases from single experiments, this study employs 5-fold cross-validation to generate 95% confidence intervals, as shown in the \u003ca href=\"#f9\">figure&#xa0;9\u003c/a>. the results demonstrate that cogfuse-mobilevit takes a significant lead with an accuracy of 86.61% (95% ci: [85.24%, 87.89%]). its narrowest confidence interval range indicates the strongest generalization capability. among the comparative models, mobilevitv3 has a 95% ci of [77.20%, 80.42%]; its lower bound is significantly lower than that of cogfuse-mobilevit, confirming that the 7.8% performance improvement is not due to random fluctuations. furthermore, the confidence intervals constructed through 5-fold cross-validation further highlight the reliability of the experimental results.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;9\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" alt=\"bar chart showing confidence intervals for accuracy rates of different classification models. models with higher accuracy include cogfuse-mobilevit at 86.61 and mobilevitv3 at 78.81. other models range from 73.63 to 68.72. each model is color-coded, with a corresponding legend on the right.\" id=\"f9\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;9\u003c/b>. confidence intervals of accuracy for different classification models in 5-fold cross-validation.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.2.3 statistical significance verification of model improvement\u003c/h4>\u003cp class=\"mb0\">to statistically evaluate the performance improvement of cogfuse-mobilevit over the baseline mobilevitv3, an independent samples t-test was conducted (\u003ca href=\"#t8\">table&#xa0;8\u003c/a>). for each model architecture, five independent training runs. the null hypothesis stated that the mean accuracy of cogfuse-mobilevit was equal to that of mobilevitv3 \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im14\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">h\u003c/mi>\u003cmn mathsize=\"10.5pt\">0\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">:\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">c\u003c/mi>\u003cmtext mathsize=\"10.5pt\">og\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">m\u003c/mi>\u003cmi mathsize=\"10.5pt\">o\u003c/mi>\u003cmi mathsize=\"10.5pt\">b\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>), while the alternative hypothesis stated that cogfuse-mobilevit had a higher mean accuracy (\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im15\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">h\u003c/mi>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">:\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">c\u003c/mi>\u003cmtext mathsize=\"10.5pt\">og\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&gt;\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">m\u003c/mi>\u003cmi mathsize=\"10.5pt\">o\u003c/mi>\u003cmi mathsize=\"10.5pt\">b\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>)cogfuse-mobilevit achieved a mean accuracy, significantly outperforming mobilevitv3. the independent samples t-test confirmed this improvement (t(8)=18.92,p=3.7&#xd7;10 \u003csup>&#x2212;8\u003c/sup>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;8\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" name=\"table&#xa0;8\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" alt=\"www.frontiersin.org\" id=\"t8\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;8\u003c/b>. comparison of accuracy results between mobilevitv3 and cogfuse-mobilevit using independent samples t-test.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">under 5-fold cross-validation (\u003ca href=\"#f10\">figure&#xa0;10\u003c/a>), cogfuse-mobilevit achieves rapid convergence within the first 20 epochs, demonstrating more efficient feature learning capability. upon entering the steady-state phase, its validation loss is reduced by over 5-fold compared to the baseline mobilevitv3, directly confirming smaller prediction errors and superior generalization capability. meanwhile, the smooth and minimally fluctuating loss curve of cogfuse-mobilevit reflects strong robustness against data noise and distribution variations. combined with the previous statistical findings from accuracy confidence intervals and independent t-tests, these results collectively validate the statistical significance of the improved model.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;10\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" alt=\"line graph showing validation loss over epochs for two models: cogfuse-mobilevit (blue) and mobilevitv3 (orange). loss decreases sharply initially, then stabilizes, with cogfuse-mobilevit exhibiting a consistently lower loss.\" id=\"f10\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;10\u003c/b>. comparison of loss curves between the cogfuse-mobilevit model and the original model in 5-fold cross-validation within 120 epochs.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.3 model result analysis performance comparison of different models\u003c/h3>\u003cp class=\"mb0\">\u003ca href=\"#f11\">figure&#xa0;11a\u003c/a> shows that in the walnut leaf brown spot disease severity grading task, the classification accuracy of all models gradually improved and stabilized with the increase of training epochs, indicating that the models continuously optimized during learning until convergence. cogfuse-mobilevit stood out prominently: it achieved rapid accuracy improvement, reached a high level in relatively early training epochs with minimal subsequent fluctuations, and stably maintained the highest accuracy, demonstrating fast convergence and strong generalization ability to efficiently extract features distinguishing different disease levels. densenet also maintained high accuracy in the late training stage, but its accuracy improvement was slower, with slightly inferior convergence speed and final stability compared to cogfuse-mobilevit. models like resnet and regnet showed limited accuracy gains with gentle upward trends, and their final stable accuracy values were significantly lower, reflecting insufficient feature extraction capabilities possibly due to network architecture or parameter optimization efficiency.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;11\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" alt=\"panel a shows a line graph comparing the accuracy of different neural networks over 100 epochs. cogfuse-mobilevit reaches the highest accuracy, achieving over 90 percent. panel b displays a line graph comparing loss over epochs. cogfuse-mobilevit has the lowest loss, declining swiftly to under 0.2. the legend lists networks like densenet, efficientnet, and mobilevitv3.\" id=\"f11\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;11\u003c/b>. comparison of accuracy and loss across different models \u003cb>(a)\u003c/b> accuracy line chart; \u003cb>(b)\u003c/b> loss line chart.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">in \u003ca href=\"#f11\">figure&#xa0;11b\u003c/a>, all models showed high initial loss values that dropped rapidly and then stabilized, following the typical learning process of iterative optimization. cogfuse-mobilevit achieved significant early loss decline and stabilized near the minimum, indicating efficient feature learning and strong fitting capability. while densenet also reached a relatively low loss level, it remained slightly higher than cogfuse-mobilevit. other models had higher late-stage loss values: some showed fast initial decline but converged to higher levels, indicating shortcomings in capturing key disease features.\u003c/p>\u003ch3>3.4 analysis of detection results for different classification models\u003c/h3>\u003cp class=\"mb0\">the confusion matrix is a key indicator for evaluating classification models: the higher the diagonal values, the higher the classification accuracy for the corresponding category, and the lower the off-diagonal values, the fewer misjudgments. \u003ca href=\"#f12\">figure&#xa0;12\u003c/a> shows the classification results of different models for the four disease grades (0&#x2013;3) of walnut leaf brown spot disease. it is clearly evident that the overall classification performance, particularly the accuracy of cogfuse-mobilevit across all categories, is remarkably high with minimal misjudgments, demonstrating that the model has enhanced discrimination ability for disease grades and performs optimally in distinguishing the four disease levels.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;12\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" alt=\"six normalized confusion matrices labeled a to f show predicted versus true values ranging from zero to three. each matrix illustrates classification performance with varying accuracy per class, indicated by different shades of red. brighter reds denote higher accuracy, while lighter shades indicate lower accuracy or misclassification.\" id=\"f12\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;12\u003c/b>. confusion matrices of the improved cogfuse-mobilevit model and traditional classification models \u003cb>(a)\u003c/b> densenet; \u003cb>(b)\u003c/b> efficientnet; \u003cb>(c)\u003c/b> efficientnetv2; \u003cb>(d)\u003c/b> swin transformer; \u003cb>(e)\u003c/b> mobilevitv3; \u003cb>(f)\u003c/b> cogfuse-mobilevit.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">the edge-enhanced feature module (ecfm) effectively addressed level 0 and level 1 misclassification by capturing incipient lesion features. baseline analysis revealed substantial false negatives for early-stage lesions, with level 1 and level 0 misclassification reaching 32%. ecfm reduced this rate to 8%, demonstrating its capacity to resolve texture confusion through enhanced edge contour delineation. similarly, the adaptive multi-scale dilated convolution module (amsddicm) significantly mitigated level 2 and level 3 mutual misclassification. amsddicm drastically reduced both errors by enhancing local fine-grained features in mid-stage lesions (level 2) while strengthening global fusion features in late-stage coalesced lesions (level 3). this resolves grading ambiguity arising from the morphological continuum of lesion progression.\u003c/p>\u003cp class=\"mb0\">to establish a comprehensive performance evaluation framework and quantify the model&#x2019;s overall discriminative capability, \u003ca href=\"#f13\">figure&#xa0;13\u003c/a> presents the receiver operating characteristic (roc) curves of cogfuse-mobilevit across four severity levels. these curves compare the true positive rate (tpr) against the false positive rate (fpr) by dynamically adjusting the classification threshold. the area under the curve (auc) serves as the primary performance metric, where a higher value indicates stronger classification ability. notably, the auc values for all categories significantly exceed the level of random guessing (auc=0.5), fully demonstrating that the model possesses robust discriminative capability across different severity levels.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;13\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" alt=\"receiver operating characteristic (roc) curve chart showing true positive rate versus false positive rate for four classes: class 0 (blue), class 1 (orange), class 2 (green), and class 3 (red), along with an average roc curve (dashed purple). the curves are above the diagonal, indicating good model performance.\" id=\"f13\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;13\u003c/b>. roc curves of the four different severity levels for cogfuse-mobilevit.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.5 tsne visualization of features extracted by different models\u003c/h3>\u003cp class=\"mb0\">the tsne visualization of features extracted from the models is shown in \u003ca href=\"#f14\">figures&#xa0;14a&#x2013;d\u003c/a>. this study analyzed the features of the original model at the 20th, 50th, and 100th training epochs, as well as the improved model at the 100th epoch. after 20 epochs of training, the original model showed a highly discrete feature distribution. limited by the number of training epochs, the model failed to fully learn discriminative features, resulting in insufficient distinction between categories and significant overlap among different classes. this indicates that under this training intensity, the original model&#x2019;s ability to capture meaningful patterns was relatively limited. when the original model was trained to 50 epochs, compared with (a), the feature aggregation significantly improved. however, inter-class overlap still existed, suggesting that although prolonged training aided feature learning, the original model&#x2019;s architecture had inherent defects in achieving clear feature separation. at 100 epochs of training, the original model exhibited more prominent feature clustering. nevertheless, some regions still lacked clear separation between different categories, indicating that even after prolonged training, the original model faced challenges in maximizing inter-class distance and intra-class compactness. in figure (d), the improved model after 100 epochs of training demonstrated a more superior feature distribution: each category was tightly clustered, achieving high intra-class compactness, while distinct boundaries between different categories were established, resulting in significant inter-class distance. this suggests that the model improvements effectively enhanced its ability to extract discriminative features, greatly reduced feature ambiguity, and improved feature discriminative power. compared with the original model, it showed stronger feature discrimination and optimization potential.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;14\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" alt=\"four scatter plots labeled a, b, c, and d illustrate data points grouped by colors representing levels zero to three. each plot displays varying cluster formations. plot a shows two distinct clusters. plot b has dispersed clusters, with one elongated group. plot c features overlapping clusters, and plot d displays four well-separated clusters.\" id=\"f14\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;14\u003c/b>. tsne visualization of features extracted by different models. after the three modules work synergistically, the boundaries of feature clustering between level 0 (healthy) and level 1 (early-stage) are significantly clearer, which verifies the effectiveness of edge-texture collaborative learning \u003cb>(a)\u003c/b> the original model was trained for 20 epochs; \u003cb>(b)\u003c/b> the original model was trained for 50 epochs; \u003cb>(c)\u003c/b> the original model was trained for 100 epochs; \u003cb>(d)\u003c/b> the improved model was trained for 100 epochs.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.6 radar chart for comparison of classification performance between original and improved models\u003c/h3>\u003cp class=\"mb0\">as shown in \u003ca href=\"#f15\">figure&#xa0;15\u003c/a>, a radar chart compares the performance of the original model and the improved cogfuse-mobilevit model across multiple evaluation metrics. in terms of accuracy, cogfuse-mobilevit exhibits significantly higher values than the original mobilevitv3 model, indicating that the improved model achieves overall higher classification accuracy. macro-average precision, which considers the average precision of each category, clearly shows that cogfuse-mobilevit also performs better, meaning it has superior precision across all categories. meanwhile, cogfuse-mobilevit also demonstrates better macro-average recall. when examining weighted-average precision and weighted-average recall&#x2014;metrics that account for class imbalance&#x2014;cogfuse-mobilevit outperforms mobilevitv3 in both, indicating that in practical applications, even with uneven class distribution, the cogfuse-mobilevit model can deliver better performance. these results further demonstrate the model&#x2019;s reliability and practicality.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;15\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" alt=\"radar chart comparing two models: mobilevitv3 (red) and cogfuse-mobilevit (blue). metrics include accuracy, macro average precision, macro average recall, weighted average precision, and weighted average recall. each axis ranges from fifty to one hundred.\" id=\"f15\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;15\u003c/b>. multi-metric comparison between mobilevitv3 and improved cogfuse-mobilevit model.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.7 public data set experiment\u003c/h3>\u003cp class=\"mb15\">to further validate the efficacy and generalizability of the proposed cogfuse-mobilevit model, experiments were conducted on the public dataset appleleaf9 (\u003ca href=\"#b38\">yang et&#xa0;al., 2022\u003c/a>). this dataset comprises healthy apple leaves and eight categories of apple leaf diseases captured in field environments without restrictions on imaging angles or noise interference. the dataset was partitioned into training and test sets at an 8:2 ratio. all images were resized to 224&#xd7;224 pixels to optimize deep learning model training efficiency. following the hyperparameter configurations specified in &#x201c;experimental parameters and evaluation metrics&#x201d;, both the baseline mobilevitv3 and cogfuse-mobilevit models were trained and evaluated.\u003c/p>\u003cp class=\"mb0\">cross-species validation demonstrates that the cogfuse-mobilevit model delivers exceptional performance on the appleleaf9 dataset. as presented in \u003ca href=\"#t9\">table&#xa0;9\u003c/a>, the model comprehensively surpasses the baseline mobilevitv3 across all four core metrics: precision increases by 0.16 percentage points, recall achieves a breakthrough improvement of 3.45 percentage points, f1-score rises by 1.18 percentage points, and accuracy elevates by 1.14 percentage points. the synergistic enhancement in both precision and recall signifies that performance gains stem from strengthened feature discriminability rather than metric trade-off compromises. the remarkable percentage points recall gain substantially mitigates leaf disease omission rates, while the holistic advance in f1-score further attests to the model&#x2019;s robustness. these results collectively validate the generalizability of cogfuse-mobilevit&#x2019;s core innovations in agricultural fine-grained disease grading, establishing a transferable paradigm for small-target pathology identification across plant species.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;9\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" name=\"table&#xa0;9\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" alt=\"www.frontiersin.org\" id=\"t9\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;9\u003c/b>. experimental results of the original model and cogfuse-mobilevit model on appleleaf9 data set.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ca id=\"h5\" name=\"h5\">\u003c/a>\u003ch2>4 conclusion\u003c/h2>\u003cp class=\"mb0\">this study addresses the challenges in precise grading of walnut leaf brown spot disease. by adopting dynamic feature filtering, edge gradient reinforcement, and multi-scale morphological adaptation, it effectively resolves three key limitations of existing hybrid architectures and the baseline model mobilevitv3 in plant disease grading weak capability in capturing blurred edges, poor multi-scale adaptability, and interference from healthy tissues. experimental results show that the model achieves an 86.61% grading accuracy on a dataset encompassing diverse lighting conditions, cultivars, and lesion stages, representing a 7.80% improvement over the baseline mobilevitv3 and outperforming nine state-of-the-art models. at the same time, this study confirmed that the performance improvement of cogfuse-mobilevit was statistically significant and stable through strict statistical tests, providing a reliable method for accurate classification of walnut leaf spot disease. the constructed image dataset and proposed grading re-measurement method lay the foundation for accuracy. this approach provides a new paradigm for small-target disease grading, with core modules transferable to multi-crop disease recognition. beyond disease classification, context-aware ai frameworks demonstrate significant efficacy in agricultural management (\u003ca href=\"#b1\">acharya et&#xa0;al., 2022\u003c/a>). iot systems dynamically adjust fertilization recommendations by integrating real-time soil-crop data, while salinity-corrected evapotranspiration (ets) models optimize irrigation strategies for saline-alkali soils (\u003ca href=\"#b15\">li et&#xa0;al., 2023\u003c/a>; \u003ca href=\"#b30\">su et&#xa0;al., 2023\u003c/a>). similarly, multimodal fusion techniques enhance reference evapotranspiration (eto) prediction accuracy, facilitating precision water allocation (\u003ca href=\"#b11\">jo et&#xa0;al., 2021\u003c/a>; \u003ca href=\"#b27\">rustia et&#xa0;al., 2023\u003c/a>). these breakthroughs collectively validate the robust capability of adaptive feature processing in complex agricultural environments&#x2014;a core principle that resonates with our dynamic weighting strategy for disease feature extraction. future research should therefore integrate cross-modal and cross-domain capabilities to establish multidimensional assessment systems and regional dynamic monitoring frameworks, thereby delivering comprehensive technical solutions for small-target disease control.\u003c/p>\u003ca id=\"h6\" name=\"h6\">\u003c/a>\u003ch2>5 discussion and future work\u003c/h2>\u003cp class=\"mb15\">compared with existing methods, this study overcomes the limitation of traditional deep learning models relying on fixed convolution kernels, enabling adaptive capture of lesion features with multi-shaped convolution kernels to accurately extract edge textures of circular micro-lesions and regional contours of irregular lesions. although the constructed multi-source dataset covers diverse lighting conditions, cultivars, and lesion development stages, the homogeneous internal features of severe diseases lead to limited recall rate. meanwhile, when lesions are severely overlapped or mixed with mechanical damage, insect damage, or other types of injuries, the discriminative accuracy of the model is affected. for ultra-small lesions, the local feature information is too weak and easily overlooked, and the computational efficiency still needs optimization on some hardware platforms. furthermore, the adaptability of the current model in extreme field scenarios, such as high-density occlusion and compound damage from pests and diseases, remains to be further verified. in these scenarios, the coupling of complex interference factors exacerbates feature confusion, affecting grading reliability.\u003c/p>\u003cp class=\"mb15\">to address these issues, future work will focus on the following research directions. future research will focus on enhancing the model&#x2019;s performance in complex field environments through multiple synergistic strategies. this includes constructing multi-interference factor coupled datasets that incorporating insect holes, lesions, and soil adhesion to strengthen robustness against extreme field disturbances; introducing attention-based multi-damage feature decoupling modules and dedicated micro-lesion enhancement modules combining super-resolution and feature interpolation to improve discriminability in complex scenarios and for tiny lesions;&#xa0;optimizing dynamic weight calculations via approximate computation or hardware-friendly reconstruction, while developing lightweight real-time deployment frameworks integrated with uav near-ground sensing technology to boost efficiency and practical monitoring coverage; and establishing cross-crop pathological transfer learning mechanisms, extending the model to multi-crop disease recognition tasks with self-supervised pre-training to enhance algorithm universality and low-contrast lesion feature mining capabilities. through the above research, it is expected to further improve the applicability and practicality of the model in complex field environments, promoting the development of intelligent plant disease grading technology towards more accurate, efficient, and universal directions.\u003c/p>\u003ca id=\"h7\" name=\"h7\">\u003c/a>\u003ch2>data availability statement\u003c/h2>\u003cp class=\"mb0\">the raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.\u003c/p>\u003ca id=\"h8\" name=\"h8\">\u003c/a>\u003ch2>author contributions\u003c/h2>\u003cp class=\"mb0\">yw: writing &#x2013; original draft. dz: writing &#x2013; original draft. lz:&#xa0;writing &#x2013; review &amp; editing.\u003c/p>\u003ca id=\"h9\" name=\"h9\">\u003c/a>\u003ch2>funding\u003c/h2>\u003cp class=\"mb0\">the author(s) declare financial support was received for the research and/or publication of this article. this work was supported in part by the science and technology key project of the xinjiang production and construction corps (2023ab063) development and application demonstration of green technology for online monitoring of fresh fruits and cold chain logistics in xinjiang.\u003c/p>\u003ca id=\"h10\" name=\"h10\">\u003c/a>\u003ch2>conflict of interest\u003c/h2>\u003cp class=\"mb0\">the authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\u003c/p>\u003ca id=\"h11\" name=\"h11\">\u003c/a>\u003ch2>generative ai statement\u003c/h2>\u003cp class=\"mb15\">the author(s) declare that no generative ai was used in the creation of this manuscript.\u003c/p>\u003cp class=\"mb0\">any alternative text (alt text) provided alongside figures in this article has been generated by frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. if you identify any issues, please contact us.\u003c/p>\u003ca id=\"h12\" name=\"h12\">\u003c/a>\u003ch2>publisher&#x2019;s note\u003c/h2>\u003cp class=\"mb15\">all claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.\u003c/p>\u003ca id=\"h13\" name=\"h13\">\u003c/a>\u003ch2>references\u003c/h2>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b1\" id=\"b1\">\u003c/a> acharya, p., burgers, t., and nguyen, k.-d. (2022). ai-enabled droplet detection and tracking for agricultural spraying systems. \u003ci>computers and electronics in agriculture\u003c/i>, 202, 107325. doi:&#xa0;10.1016/j.compag.2022.107325\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2022.107325\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=p.+acharya&amp;author=t.+burgers&amp;author=k.-d.+nguyen&amp;publication_year=2022&amp;title=ai-enabled%20droplet%20detection%20and%20tracking%20for%20agricultural%20spraying%20systems&amp;journal=computers+and+electronics+in+agriculture&amp;volume=202&amp;pages=107325\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b2\" id=\"b2\">\u003c/a> adaskaveg, j., f&#xf6;rster, h., thompson, d., driever, g., connell, j., buchner, r., et al. (2009). epidemiology and management of walnut blight. \u003ci>walnut res. rep\u003c/i>, 94, 225&#x2013;256. doi:&#xa0;10.1016/j.inpa.2016.10.005\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.inpa.2016.10.005\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.+adaskaveg&amp;author=h.+f%c3%b6rster&amp;author=d.+thompson&amp;author=g.+driever&amp;author=j.+connell&amp;author=r.+buchner&amp;publication_year=2009&amp;title=epidemiology%20and%20management%20of%20walnut%20blight&amp;journal=walnut+res.+rep&amp;volume=94&amp;pages=225\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b3\" id=\"b3\">\u003c/a> arivazhagan, s., shebiah, r. n., ananthi, s., and varthini, s. v. (2013). detection of unhealthy region of plant leaves and classification of plant leaf diseases using texture features. \u003ci>agricultural engineering international: cigr journal\u003c/i>, 15, 211&#x2013;217.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+arivazhagan&amp;author=r.%20n.+shebiah&amp;author=s.+ananthi&amp;author=s.%20v.+varthini&amp;publication_year=2013&amp;title=detection%20of%20unhealthy%20region%20of%20plant%20leaves%20and%20classification%20of%20plant%20leaf%20diseases%20using%20texture%20features&amp;journal=agricultural+engineering+international:+cigr+journal&amp;volume=15&amp;pages=211\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b4\" id=\"b4\">\u003c/a> chaudhary, p., chaudhari, a. k., cheeran, a., and godara, s. (2012). color transform based approach for disease spot detection on plant leaf. \u003ci>international journal of computer science and telecommunications\u003c/i>, 3, 65&#x2013;70.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=p.+chaudhary&amp;author=a.%20k.+chaudhari&amp;author=a.+cheeran&amp;author=s.+godara&amp;publication_year=2012&amp;title=color%20transform%20based%20approach%20for%20disease%20spot%20detection%20on%20plant%20leaf&amp;journal=international+journal+of+computer+science+and+telecommunications&amp;volume=3&amp;pages=65\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b5\" id=\"b5\">\u003c/a> chen, s., zhang, k., zhao, y., sun, y., ban, w., chen, y., et al. (2021). an approach for rice bacterial leaf streak disease segmentation and disease severity estimation. \u003ci>agriculture\u003c/i>, 11, 420. doi:&#xa0;10.3390/agriculture11050420\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agriculture11050420\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+chen&amp;author=k.+zhang&amp;author=y.+zhao&amp;author=y.+sun&amp;author=w.+ban&amp;author=y.+chen&amp;publication_year=2021&amp;title=an%20approach%20for%20rice%20bacterial%20leaf%20streak%20disease%20segmentation%20and%20disease%20severity%20estimation&amp;journal=agriculture&amp;volume=11&amp;pages=420\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b6\" id=\"b6\">\u003c/a> chiang, k.-s., bock, c., el jarroudi, m., delfosse, p., lee, i., and liu, h. (2016). effects of rater bias and assessment method on disease severity estimation with regard to hypothesis testing. \u003ci>phytopathology\u003c/i>, 65, 523&#x2013;535. doi:&#xa0;10.1111/ppa.12435\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1111/ppa.12435\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=k.-s.+chiang&amp;author=c.+bock&amp;author=m.+el%20jarroudi&amp;author=p.+delfosse&amp;author=i.+lee&amp;author=h.+liu&amp;publication_year=2016&amp;title=effects%20of%20rater%20bias%20and%20assessment%20method%20on%20disease%20severity%20estimation%20with%20regard%20to%20hypothesis%20testing&amp;journal=phytopathology&amp;volume=65&amp;pages=523\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b7\" id=\"b7\">\u003c/a> cooke, b. (2006). &#x201c;disease assessment and yield loss,&#x201d; in \u003ci>the epidemiology of plant diseases\u003c/i> (dordrecht: springer), 43&#x2013;80.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=b.+cooke&amp;publication_year=2006&amp;title=disease%20assessment%20and%20yield%20loss&amp;book=the+epidemiology+of+plant+diseases&amp;pages=43\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b8\" id=\"b8\">\u003c/a> ghaiwat, s. n. and arora, p. (2014). detection and classification of plant leaf diseases using image processing techniques: a review. \u003ci>international journal of recent advances in engineering &amp; technology\u003c/i>, 2, 1&#x2013;7.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20n.+ghaiwat&amp;author=p.+arora&amp;publication_year=2014&amp;title=detection%20and%20classification%20of%20plant%20leaf%20diseases%20using%20image%20processing%20techniques%3a%20a%20review&amp;journal=international+journal+of+recent+advances+in+engineering+&amp;+technology&amp;volume=2&amp;pages=1\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b9\" id=\"b9\">\u003c/a> hu, g., wang, h., zhang, y., and wan, m. (2021). detection and severity analysis of tea leaf blight based on deep learning. \u003ci>computers &amp; electrical engineering\u003c/i>, 90, 107023. doi:&#xa0;10.1016/j.compeleceng.2021.107023\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compeleceng.2021.107023\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=g.+hu&amp;author=h.+wang&amp;author=y.+zhang&amp;author=m.+wan&amp;publication_year=2021&amp;title=detection%20and%20severity%20analysis%20of%20tea%20leaf%20blight%20based%20on%20deep%20learning&amp;journal=computers+&amp;+electrical+engineering&amp;volume=90&amp;pages=107023\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b10\" id=\"b10\">\u003c/a> jadhav, s. b. and patil, s. b. (2016). grading of soybean leaf disease based on segmented image using k-means clustering. \u003ci>iaes international journal of artificial intelligence\u003c/i>, 5, 13&#x2013;13. doi:&#xa0;10.11591/ijai.v5.i1.pp13-21\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.11591/ijai.v5.i1.pp13-21\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20b.+jadhav&amp;author=s.%20b.+patil&amp;publication_year=2016&amp;title=grading%20of%20soybean%20leaf%20disease%20based%20on%20segmented%20image%20using%20k-means%20clustering&amp;journal=iaes+international+journal+of+artificial+intelligence&amp;volume=5&amp;pages=13\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b11\" id=\"b11\">\u003c/a> jo, w. j., kim, d. s., sim, h. s., ahn, s. r., lee, h. j., moon, y. h., et al. (2021). estimation of evapotranspiration and water requirements of strawberry plants in greenhouses using environmental data. \u003ci>frontiers in sustainable food systems\u003c/i>, 5, 684808. doi:&#xa0;10.3389/fsufs.2021.684808\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3389/fsufs.2021.684808\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=w.%20j.+jo&amp;author=d.%20s.+kim&amp;author=h.%20s.+sim&amp;author=s.%20r.+ahn&amp;author=h.%20j.+lee&amp;author=y.%20h.+moon&amp;publication_year=2021&amp;title=estimation%20of%20evapotranspiration%20and%20water%20requirements%20of%20strawberry%20plants%20in%20greenhouses%20using%20environmental%20data&amp;journal=frontiers+in+sustainable+food+systems&amp;volume=5&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b12\" id=\"b12\">\u003c/a> khan, m. a., ali, m., shah, m., mahmood, t., ahmad, m., jhanjhi, n., et al. (2021). machine learning-based detection and classification of walnut fungi diseases. \u003ci>intelligent automation &amp; soft computing\u003c/i>, 30, 771&#x2013;785. doi:&#xa0;10.32604/iasc.2021.018039\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.32604/iasc.2021.018039\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=m.%20a.+khan&amp;author=m.+ali&amp;author=m.+shah&amp;author=t.+mahmood&amp;author=m.+ahmad&amp;author=n.+jhanjhi&amp;publication_year=2021&amp;title=machine%20learning-based%20detection%20and%20classification%20of%20walnut%20fungi%20diseases&amp;journal=intelligent+automation+&amp;+soft+computing&amp;volume=30&amp;pages=771\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b13\" id=\"b13\">\u003c/a> kim, s.-k. and ahn, j.-g. (2021). tomato crop diseases classification models using deep cnn-based architectures. \u003ci>j korea acad-ind coop soc\u003c/i>, 22, 7&#x2013;14. doi:&#xa0;10.5762/kais.2021.22.5.7\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.5762/kais.2021.22.5.7\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.-k.+kim&amp;author=j.-g.+ahn&amp;publication_year=2021&amp;title=tomato%20crop%20diseases%20classification%20models%20using%20deep%20cnn-based%20architectures&amp;journal=j+korea+acad-ind+coop+soc&amp;volume=22&amp;pages=7\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b14\" id=\"b14\">\u003c/a> lamichhane, j. r. (2014). xanthomonas arboricola diseases of stone fruit, almond, and walnut trees: progress toward understanding and management. \u003ci>plant disease\u003c/i>, 98, 1600&#x2013;1610. doi:&#xa0;10.1094/pdis-08-14-0831-fe\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30703892/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-08-14-0831-fe\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.%20r.+lamichhane&amp;publication_year=2014&amp;title=xanthomonas%20arboricola%20diseases%20of%20stone%20fruit%2c%20almond%2c%20and%20walnut%20trees%3a%20progress%20toward%20understanding%20and%20management&amp;journal=plant+disease&amp;volume=98&amp;pages=1600\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b15\" id=\"b15\">\u003c/a> li, x., zha, t., liu, p., bourque, c. p.-a., jia, x., and tian, y. (2023). interannual variation in gross ecosystem production and evapotranspiration in a temperate semiarid grassland undergoing vegetation recovery. \u003ci>agricultural and forest meteorology\u003c/i>, 341, 109672. doi:&#xa0;10.1016/j.agrformet.2023.109672\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.agrformet.2023.109672\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=x.+li&amp;author=t.+zha&amp;author=p.+liu&amp;author=c.%20p.-a.+bourque&amp;author=x.+jia&amp;author=y.+tian&amp;publication_year=2023&amp;title=interannual%20variation%20in%20gross%20ecosystem%20production%20and%20evapotranspiration%20in%20a%20temperate%20semiarid%20grassland%20undergoing%20vegetation%20recovery&amp;journal=agricultural+and+forest+meteorology&amp;volume=341&amp;pages=109672\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b16\" id=\"b16\">\u003c/a> lin, k., gong, l., huang, y., liu, c., and pan, j. (2019). deep learning-based segmentation and quantification of cucumber powdery mildew using convolutional neural network. \u003ci>frontiers in plant science\u003c/i>, 10, 155. doi:&#xa0;10.3389/fpls.2019.00155\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30891048/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.3389/fpls.2019.00155\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=k.+lin&amp;author=l.+gong&amp;author=y.+huang&amp;author=c.+liu&amp;author=j.+pan&amp;publication_year=2019&amp;title=deep%20learning-based%20segmentation%20and%20quantification%20of%20cucumber%20powdery%20mildew%20using%20convolutional%20neural%20network&amp;journal=frontiers+in+plant+science&amp;volume=10&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b17\" id=\"b17\">\u003c/a> ma, j., du, k., zhang, l., zheng, f., chu, j., and sun, z. (2017). a segmentation method for greenhouse vegetable foliar disease spots images using color information and region growing. \u003ci>computers and electronics in agriculture\u003c/i>, 142, 110&#x2013;117. doi:&#xa0;10.1016/j.compag.2017.08.023\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2017.08.023\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.+ma&amp;author=k.+du&amp;author=l.+zhang&amp;author=f.+zheng&amp;author=j.+chu&amp;author=z.+sun&amp;publication_year=2017&amp;title=a%20segmentation%20method%20for%20greenhouse%20vegetable%20foliar%20disease%20spots%20images%20using%20color%20information%20and%20region%20growing&amp;journal=computers+and+electronics+in+agriculture&amp;volume=142&amp;pages=110\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b18\" id=\"b18\">\u003c/a> mao, r., wang, z., li, f., zhou, j., chen, y., and hu, x. (2023). gseyolox-s: an improved lightweight network for identifying the severity of wheat fusarium head blight. \u003ci>agronomy\u003c/i>, 13, 242. doi:&#xa0;10.3390/agronomy13010242\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agronomy13010242\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=r.+mao&amp;author=z.+wang&amp;author=f.+li&amp;author=j.+zhou&amp;author=y.+chen&amp;author=x.+hu&amp;publication_year=2023&amp;title=gseyolox-s%3a%20an%20improved%20lightweight%20network%20for%20identifying%20the%20severity%20of%20wheat%20fusarium%20head%20blight&amp;journal=agronomy&amp;volume=13&amp;pages=242\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b19\" id=\"b19\">\u003c/a> mcgranahan, g. and leslie, c. (1991). walnuts (juglans). \u003ci>molecules\u003c/i>, 907&#x2013;924. doi:&#xa0;10.17660/actahortic.1991.290.20\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.17660/actahortic.1991.290.20\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=g.+mcgranahan&amp;author=c.+leslie&amp;publication_year=1991&amp;title=walnuts%20%28juglans%29&amp;journal=molecules&amp;pages=907\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b20\" id=\"b20\">\u003c/a> mircetich, s. m., sanborn, r., and ramos, d. (1980). natural spread, graft-transmission, and possible etiology of walnut blackline disease. \u003ci>phytopathology\u003c/i>, 70, 962&#x2013;968. doi:&#xa0;10.1094/phyto-70-962\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/phyto-70-962\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20m.+mircetich&amp;author=r.+sanborn&amp;author=d.+ramos&amp;publication_year=1980&amp;title=natural%20spread%2c%20graft-transmission%2c%20and%20possible%20etiology%20of%20walnut%20blackline%20disease&amp;journal=phytopathology&amp;volume=70&amp;pages=962\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b21\" id=\"b21\">\u003c/a> moragrega, c., matias, j., alet&#xe0;, n., montesinos, e., and rovira, m. (2011). apical necrosis and premature drop of persian (english) walnut fruit caused by xanthomonas arboricola pv. juglandis. \u003ci>plant disease\u003c/i>, 95, 1565&#x2013;1570. doi:&#xa0;10.1094/pdis-03-11-0259\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30732020/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-03-11-0259\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=c.+moragrega&amp;author=j.+matias&amp;author=n.+alet%c3%a0&amp;author=e.+montesinos&amp;author=m.+rovira&amp;publication_year=2011&amp;title=apical%20necrosis%20and%20premature%20drop%20of%20persian%20%28english%29%20walnut%20fruit%20caused%20by%20xanthomonas%20arboricola%20pv.%20juglandis&amp;journal=plant+disease&amp;volume=95&amp;pages=1565\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b22\" id=\"b22\">\u003c/a> ngugi, l. c., abdelwahab, m., and abo-zahhad, m. (2020). tomato leaf segmentation algorithms for mobile phone applications using deep learning. \u003ci>computers and electronics in agriculture\u003c/i>, 178, 105788. doi:&#xa0;10.1016/j.compag.2020.105788\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2020.105788\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=l.%20c.+ngugi&amp;author=m.+abdelwahab&amp;author=m.+abo-zahhad&amp;publication_year=2020&amp;title=tomato%20leaf%20segmentation%20algorithms%20for%20mobile%20phone%20applications%20using%20deep%20learning&amp;journal=computers+and+electronics+in+agriculture&amp;volume=178&amp;pages=105788\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b23\" id=\"b23\">\u003c/a> ozturk, o., sarica, b., and seker, d. z. (2025). interpretable and robust ensemble deep learning framework for tea leaf disease classification. \u003ci>horticulturae\u003c/i>, 11, 437. doi:&#xa0;10.3390/horticulturae11040437\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/horticulturae11040437\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=o.+ozturk&amp;author=b.+sarica&amp;author=d.%20z.+seker&amp;publication_year=2025&amp;title=interpretable%20and%20robust%20ensemble%20deep%20learning%20framework%20for%20tea%20leaf%20disease%20classification&amp;journal=horticulturae&amp;volume=11&amp;pages=437\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b24\" id=\"b24\">\u003c/a> parashar, n., johri, p., khan, a. a., gaur, n., and kadry, s. (2024). an integrated analysis of yield prediction models: a comprehensive review of advancements and challenges. \u003ci>computers, materials &amp; continua\u003c/i>, 80 (1). doi:&#xa0;10.32604/cmc.2024.050240\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.32604/cmc.2024.050240\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=n.+parashar&amp;author=p.+johri&amp;author=a.%20a.+khan&amp;author=n.+gaur&amp;author=s.+kadry&amp;publication_year=2024&amp;title=an%20integrated%20analysis%20of%20yield%20prediction%20models%3a%20a%20comprehensive%20review%20of%20advancements%20and%20challenges&amp;journal=computers,+materials+&amp;+continua&amp;volume=80&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b25\" id=\"b25\">\u003c/a> picon, a., alvarez-gila, a., seitz, m., ortiz-barredo, a., echazarra, j., and johannes, a. (2019). deep convolutional neural networks for mobile capture device-based crop disease classification in the wild. \u003ci>computers and electronics in agriculture\u003c/i>, 161, 280&#x2013;290. doi:&#xa0;10.1016/j.compag.2018.04.002\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2018.04.002\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=a.+picon&amp;author=a.+alvarez-gila&amp;author=m.+seitz&amp;author=a.+ortiz-barredo&amp;author=j.+echazarra&amp;author=a.+johannes&amp;publication_year=2019&amp;title=deep%20convolutional%20neural%20networks%20for%20mobile%20capture%20device-based%20crop%20disease%20classification%20in%20the%20wild&amp;journal=computers+and+electronics+in+agriculture&amp;volume=161&amp;pages=280\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b26\" id=\"b26\">\u003c/a> rastogi, a., arora, r., and sharma, s. (2015). &#x201c;leaf disease detection and grading using computer vision technology &amp; fuzzy logic,&#x201d; in paper presented at the 2015 2nd international conference on signal processing and integrated networks (spin).\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=a.+rastogi&amp;author=r.+arora&amp;author=s.+sharma&amp;publication_year=2015&amp;title=leaf%20disease%20detection%20and%20grading%20using%20computer%20vision%20technology%20%26%20fuzzy%20logic&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b27\" id=\"b27\">\u003c/a> rustia, d. j. a., lee, w.-c., lu, c.-y., wu, y.-f., shih, p.-y., and chen, s.-k. (2023). edge-based wireless imaging system for continuous monitoring of insect pests in a remote outdoor mango orchard. \u003ci>computers and electronics in agriculture\u003c/i>, 211, 108019. doi:&#xa0;10.1016/j.compag.2023.108019\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2023.108019\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=d.%20j.%20a.+rustia&amp;author=w.-c.+lee&amp;author=c.-y.+lu&amp;author=y.-f.+wu&amp;author=p.-y.+shih&amp;author=s.-k.+chen&amp;publication_year=2023&amp;title=edge-based%20wireless%20imaging%20system%20for%20continuous%20monitoring%20of%20insect%20pests%20in%20a%20remote%20outdoor%20mango%20orchard&amp;journal=computers+and+electronics+in+agriculture&amp;volume=211&amp;pages=108019\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b28\" id=\"b28\">\u003c/a> shi, t., liu, y., zheng, x., hu, k., huang, h., liu, h., et al. (2023). recent advances in plant disease severity assessment using convolutional neural networks. \u003ci>scientific reports\u003c/i>, 13, 2336. doi:&#xa0;10.1038/s41598-023-29230-7\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/36759626/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1038/s41598-023-29230-7\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=t.+shi&amp;author=y.+liu&amp;author=x.+zheng&amp;author=k.+hu&amp;author=h.+huang&amp;author=h.+liu&amp;publication_year=2023&amp;title=recent%20advances%20in%20plant%20disease%20severity%20assessment%20using%20convolutional%20neural%20networks&amp;journal=scientific+reports&amp;volume=13&amp;pages=2336\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b29\" id=\"b29\">\u003c/a> singh, u. p., chouhan, s. s., jain, s., and jain, s. (2019). multilayer convolution neural network for the classification of mango leaves infected by anthracnose disease. \u003ci>ieee access\u003c/i>, 7, 43721&#x2013;43729. doi:&#xa0;10.1109/access.2019.2907383\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/access.2019.2907383\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=u.%20p.+singh&amp;author=s.%20s.+chouhan&amp;author=s.+jain&amp;author=s.+jain&amp;publication_year=2019&amp;title=multilayer%20convolution%20neural%20network%20for%20the%20classification%20of%20mango%20leaves%20infected%20by%20anthracnose%20disease&amp;journal=ieee+access&amp;volume=7&amp;pages=43721\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b30\" id=\"b30\">\u003c/a> su, y., wang, j., li, j., wang, l., wang, k., li, a., et al. (2023). spatiotemporal changes and driving factors of reference evapotranspiration and crop evapotranspiration for cotton production in china from 1960 to 2019. \u003ci>frontiers in environmental science\u003c/i>, 11, 1251789. doi:&#xa0;10.3389/fenvs.2023.1251789\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3389/fenvs.2023.1251789\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=y.+su&amp;author=j.+wang&amp;author=j.+li&amp;author=l.+wang&amp;author=k.+wang&amp;author=a.+li&amp;publication_year=2023&amp;title=spatiotemporal%20changes%20and%20driving%20factors%20of%20reference%20evapotranspiration%20and%20crop%20evapotranspiration%20for%20cotton%20production%20in%20china%20from%201960%20to%202019&amp;journal=frontiers+in+environmental+science&amp;volume=11&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b31\" id=\"b31\">\u003c/a> tripathi, r. (2021). &#x201c;a deep learning approach for plant material disease identification,&#x201d; in paper presented at the iop conference series: materials science and engineering.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=r.+tripathi&amp;publication_year=2021&amp;title=a%20deep%20learning%20approach%20for%20plant%20material%20disease%20identification&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b32\" id=\"b32\">\u003c/a> vishnoi, v. k., kumar, k., kumar, b., mohan, s., and khan, a. a. (2022). detection of apple plant diseases using leaf images through convolutional neural network . \u003ci>ieee access\u003c/i>, 11, 6594&#x2013;6609. doi:&#xa0;10.1109/access.2022.3232917\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/access.2022.3232917\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=v.%20k.+vishnoi&amp;author=k.+kumar&amp;author=b.+kumar&amp;author=s.+mohan&amp;author=a.%20a.+khan&amp;publication_year=2022&amp;title=detection%20of%20apple%20plant%20diseases%20using%20leaf%20images%20through%20convolutional%20neural%20network&amp;journal=ieee+access&amp;volume=11&amp;pages=6594\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b33\" id=\"b33\">\u003c/a> wang, f., dun, c., tang, t., duan, y., guo, x., and you, j. (2022). boeremia exigua causes leaf spot of walnut trees (juglans regia) in china. \u003ci>plant disease\u003c/i>, 106, 1993. doi:&#xa0;10.1094/pdis-10-21-2304-pdn\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/pdis-10-21-2304-pdn\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=f.+wang&amp;author=c.+dun&amp;author=t.+tang&amp;author=y.+duan&amp;author=x.+guo&amp;author=j.+you&amp;publication_year=2022&amp;title=boeremia%20exigua%20causes%20leaf%20spot%20of%20walnut%20trees%20%28juglans%20regia%29%20in%20china&amp;journal=plant+disease&amp;volume=106&amp;pages=1993\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b34\" id=\"b34\">\u003c/a> wang, q.-h., fan, k., li, d.-w., han, c.-m., qu, y.-y., qi, y.-k., et al. (2020). identification, virulence and fungicide sensitivity of colletotrichum gloeosporioides ss responsible for walnut anthracnose disease in china. \u003ci>plant disease\u003c/i>, 104, 1358&#x2013;1368. doi:&#xa0;10.1094/pdis-12-19-2569-re\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/32196416/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-12-19-2569-re\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=q.-h.+wang&amp;author=k.+fan&amp;author=d.-w.+li&amp;author=c.-m.+han&amp;author=y.-y.+qu&amp;author=y.-k.+qi&amp;publication_year=2020&amp;title=identification%2c%20virulence%20and%20fungicide%20sensitivity%20of%20colletotrichum%20gloeosporioides%20ss%20responsible%20for%20walnut%20anthracnose%20disease%20in%20china&amp;journal=plant+disease&amp;volume=104&amp;pages=1358\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b35\" id=\"b35\">\u003c/a> weber, b. c. (1980). \u003ci>how to diagnose black walnut damage\u003c/i> vol. 57 (north central forest experiment station, forest service, us department of agriculture).\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=b.%20c.+weber&amp;publication_year=1980&amp;book=how+to+diagnose+black+walnut+damage&amp;volume=57&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b36\" id=\"b36\">\u003c/a> xu, w. (2024). hcf-net: hierarchicalcontextfusion network forinfrared small object detection. \u003ci>arxiv\u003c/i>. arxiv, 2403.10778. doi:&#xa0;10.1109/icme57554.2024.10687431\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/icme57554.2024.10687431\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=w.+xu&amp;publication_year=2024&amp;title=hcf-net%3a%20hierarchicalcontextfusion%20network%20forinfrared%20small%20object%20detection&amp;journal=arxiv&amp;volume=arxiv&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b37\" id=\"b37\">\u003c/a> yang, c., deng, y., wang, f., yang, h., xu, x., zeng, q., et al. (2021). brown leaf spot on \u003ci>juglans sigillata\u003c/i> caused by \u003ci>ophiognomonia leptostyla\u003c/i> in sichuan, china. \u003ci>plant disease\u003c/i>, 105, 4160. doi:&#xa0;10.1094/pdis-02-21-0344-pdn\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/pdis-02-21-0344-pdn\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=c.+yang&amp;author=y.+deng&amp;author=f.+wang&amp;author=h.+yang&amp;author=x.+xu&amp;author=q.+zeng&amp;publication_year=2021&amp;title=brown%20leaf%20spot%20on%20juglans%20sigillata%20caused%20by%20ophiognomonia%20leptostyla%20in%20sichuan%2c%20china&amp;journal=plant+disease&amp;volume=105&amp;pages=4160\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b38\" id=\"b38\">\u003c/a> yang, q., duan, s., and wang, l. (2022). efficient identification of apple leaf diseases in the wild using convolutional neural networks. \u003ci>agronomy\u003c/i>, 12, 2784. doi:&#xa0;10.3390/agronomy12112784\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agronomy12112784\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=q.+yang&amp;author=s.+duan&amp;author=l.+wang&amp;publication_year=2022&amp;title=efficient%20identification%20of%20apple%20leaf%20diseases%20in%20the%20wild%20using%20convolutional%20neural%20networks&amp;journal=agronomy&amp;volume=12&amp;pages=2784\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b39\" id=\"b39\">\u003c/a> zarei, s., taghavi, s. m., banihashemi, z., hamzehzarghani, h., and osdaghi, e. (2019). etiology of leaf spot and fruit canker symptoms on stone fruits and nut trees in iran. \u003ci>journal of plant pathology\u003c/i>, 101, 1133&#x2013;1142. doi:&#xa0;10.1007/s42161-019-00283-w\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1007/s42161-019-00283-w\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+zarei&amp;author=s.%20m.+taghavi&amp;author=z.+banihashemi&amp;author=h.+hamzehzarghani&amp;author=e.+osdaghi&amp;publication_year=2019&amp;title=etiology%20of%20leaf%20spot%20and%20fruit%20canker%20symptoms%20on%20stone%20fruits%20and%20nut%20trees%20in%20iran&amp;journal=journal+of+plant+pathology&amp;volume=101&amp;pages=1133\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b40\" id=\"b40\">\u003c/a> zhang, y., li, x., &#x160;im&#x16f;nek, j., shi, h., chen, n., and hu, q. (2023). quantifying water and salt movement in a soil-plant system of a corn field using hydrus (2d/3d) and the stable isotope method. \u003ci>agricultural water management\u003c/i>, 288, 108492. doi:&#xa0;10.1016/j.agwat.2023.108492\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.agwat.2023.108492\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=y.+zhang&amp;author=x.+li&amp;author=j.+%c5%a0im%c5%afnek&amp;author=h.+shi&amp;author=n.+chen&amp;author=q.+hu&amp;publication_year=2023&amp;title=quantifying%20water%20and%20salt%20movement%20in%20a%20soil-plant%20system%20of%20a%20corn%20field%20using%20hydrus%20%282d%2f3d%29%20and%20the%20stable%20isotope%20method&amp;journal=agricultural+water+management&amp;volume=288&amp;pages=108492\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003c/div>\u003cdiv class=\"thinlinem20\">\u003c/div>\u003cdiv class=\"abstractsummary\">\u003cp>\u003cspan>keywords:\u003c/span> walnut, brown spot disease (\u003ci>ophiognomonia leptostyla\u003c/i>), hierarchical feature selection, edge features perception, adaptive multi-scale dilated convolution, disease grading\u003c/p>\u003cp>\u003cspan>citation:\u003c/span> wei y, zeng d and zheng l (2025) a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution. \u003ci>front. plant sci.\u003c/i> 16:1641677. doi: 10.3389/fpls.2025.1641677\u003c/p>\u003cp id=\"timestamps\">\u003cspan>received:\u003c/span> 05 june 2025; \u003cspan>accepted:\u003c/span> 09 september 2025;\u003cbr>\u003cspan>published:\u003c/span> 03 october 2025.\u003c/p>\u003cdiv>\u003cp>edited by:\u003c/p>\u003ca href=\"https://loop.frontiersin.org/people/1037951\">ravinder kumar\u003c/a>, indian agricultural research institute (icar), india\u003c/div>\u003cdiv>\u003cp>reviewed by:\u003c/p>\u003ca href=\"https://loop.frontiersin.org/people/2082015\">geza bujdoso\u003c/a>, hungarian university of agricultural and life sciences, hungary\u003cbr>\r\n\u003ca href=\"https://loop.frontiersin.org/people/3078480\">vasudha vedula\u003c/a>, university of texas of the permian basin, united states\u003c/div>\u003cp>\u003cspan>copyright\u003c/span> &#xa9; 2025 wei, zeng and zheng. this is an open-access article distributed under the terms of the \u003ca rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\" target=\"_blank\">creative commons attribution license (cc by)\u003c/a>. the use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. no use, distribution or reproduction is permitted which does not comply with these terms.\u003c/p>\u003cp>\u003cspan>*correspondence:\u003c/span> liangfang zheng, \u003ca id=\"encmail\">mtgxnjaynta0othamtyzlmnvbq==\u003c/a>\u003c/p>\u003cp>\u003cspan>\u003csup>&#x2020;\u003c/sup>\u003c/span>these authors have contributed equally to this work\u003c/p>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>","\u003cul class=\"flyoutjournal\">\u003cli>\u003ca href=\"#h1\">abstract\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h2\">1 introduction\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h3\">2 materials and methods\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h4\">3 experiments and results analysis\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h5\">4 conclusion\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h6\">5 discussion and future work\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h7\">data availability statement\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h8\">author contributions\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h9\">funding\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h10\">conflict of interest\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h11\">generative ai statement\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h12\">publisher&#x2019;s note\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h13\">references\u003c/a>\u003c/li>\u003c/ul>",[629,636,642],{"name":630,"fileserverpackageentryid":19,"fileserverid":631,"fileserverversionnumber":632,"type":633},"epub.epub","1641677/epub",2,{"code":634,"name":635},"epub","epub",{"name":637,"fileserverpackageentryid":637,"fileserverid":638,"fileserverversionnumber":632,"type":639},"fpls-16-1641677.xml","1641677/xml",{"code":640,"name":641},"nlm_xml","xml",{"name":643,"fileserverpackageentryid":19,"fileserverid":644,"fileserverversionnumber":632,"type":645},"publishers-proof.pdf","1641677/publishers-proof",{"code":646,"name":647},"pdf","pdf","v3",{"title":650,"link":651,"meta":655,"script":748},"frontiers | a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution",[652],{"rel":653,"href":654},"canonical","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/full",[656,659,662,664,667,671,673,677,680,683,686,688,690,692,694,696,699,702,704,707,709,711,714,717,720,723,726,730,734,737,740,743,745],{"hid":657,"property":657,"name":657,"content":658},"description","walnut leaf brown spot disease, caused by ophiognomonia leptostyla, is among the most destructive fungal diseases in walnut cultivation. in the development o...",{"hid":660,"property":660,"name":661,"content":650},"og:title","title",{"hid":663,"property":663,"name":657,"content":658},"og:description",{"hid":665,"name":665,"content":666},"keywords","walnut,brown spot disease (ophiognomonia leptostyla),hierarchical feature selection,edge features perception,adaptive multi-scale dilated convolution,disease grading",{"hid":668,"property":668,"name":669,"content":670},"og:site_name","site_name","frontiers",{"hid":672,"property":672,"name":385,"content":404},"og:image",{"hid":674,"property":674,"name":675,"content":676},"og:type","type","article",{"hid":678,"property":678,"name":679,"content":654},"og:url","url",{"hid":681,"name":681,"content":682},"twitter:card","summary_large_image",{"hid":684,"name":684,"content":685},"citation_volume","16",{"hid":687,"name":687,"content":116},"citation_journal_title",{"hid":689,"name":689,"content":670},"citation_publisher",{"hid":691,"name":691,"content":611},"citation_journal_abbrev",{"hid":693,"name":693,"content":612},"citation_issn",{"hid":695,"name":695,"content":538},"citation_doi",{"hid":697,"name":697,"content":698},"citation_firstpage","1641677",{"hid":700,"name":700,"content":701},"citation_language","english",{"hid":703,"name":703,"content":539},"citation_title",{"hid":705,"name":705,"content":706},"citation_keywords","walnut; brown spot disease (ophiognomonia leptostyla); hierarchical feature selection; edge features perception; adaptive multi-scale dilated convolution; disease grading",{"hid":708,"name":708,"content":544},"citation_abstract",{"hid":710,"name":710,"content":553},"citation_article_type",{"hid":712,"name":712,"content":713},"citation_pdf_url","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/pdf",{"hid":715,"name":715,"content":716},"citation_xml_url","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/xml",{"hid":718,"name":718,"content":719},"citation_fulltext_world_readable","yes",{"hid":721,"name":721,"content":722},"citation_online_date","2025/09/09",{"hid":724,"name":724,"content":725},"citation_publication_date","2025/10/03",{"hid":727,"name":728,"content":729},"citation_author_0","citation_author","wei, yuting ",{"hid":731,"name":732,"content":733},"citation_author_institution_0","citation_author_institution","college of information engineering, tarim university, china",{"hid":735,"name":728,"content":736},"citation_author_1","zeng, debin ",{"hid":738,"name":732,"content":739},"citation_author_institution_1","key laboratory of tarim oasis agriculture, ministry of education, tarim university, china",{"hid":741,"name":728,"content":742},"citation_author_2","zheng, liangfang ",{"hid":744,"name":732,"content":739},"citation_author_institution_2",{"hid":746,"name":746,"content":747},"dc.identifier","doi:10.3389/fpls.2025.1641677",[749,752,754,756,758],{"src":750,"body":13,"type":751,"async":13},"https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6","text/javascript",{"src":753,"body":13,"type":751,"async":13},"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/mathjax.js?config=tex-mml-am_chtml",{"src":755,"body":13,"type":751,"async":13},"https://d1bxh8uas1mnw7.cloudfront.net/assets/altmetric_badges-f0bc9b243ff5677d05460c1eb71834ca998946d764eb3bc244ab4b18ba50d21e.js",{"src":757,"body":13,"type":751,"async":13},"https://api.altmetric.com/v1/doi/10.3389/fpls.2025.1641677?callback=_altmetric.embed_callback&domain=www.frontiersin.org&key=3c130976ca2b8f2e88f8377633751ba1&cache_until=14-15",{"src":759,"body":13,"type":751,"async":13},"https://crossmark-cdn.crossref.org/widget/v2.0/widget.js",{"articlehubslug":19,"articlehubpage":761,"articlehubarticleslist":762,"canjournalhasarticlehub":367,"articledoilist":763},{},[],[],{"title":19,"image":-1,"breadcrumbs":765,"linkscollection":766,"metricscollection":768},[],{"total":389,"items":767},[],{"total":389,"items":769},[]] window.__nuxt__={};window.__nuxt__.config={public:{isdevmode:false,appname:"article-pages-2024",baseurl:"https://www.frontiersin.org",domain:"frontiersin.org",loopurl:"https://loop.frontiersin.org",ssmaindomain:"frontiersin.org",contentfulurl:"https://graphql.contentful.com/content/v1/spaces",spaceid:1,spacename:"frontiers",livespaceid:1,tenantlogourl:"",frontiersgraphurl:"https://apollo-federation-gateway.frontiersin.org",registrationapiurl:"https://onboarding-ui.frontiersin.org",emaildigestapiurl:"https://api-email-digest.frontiersin.org",journalfilesapiurl:"https://files-journal-api.frontiersin.org",searchapiurl:"https://search-api.frontiersin.org",productionforumapiurl:"https://production-forum-api-v2.frontiersin.org",gtmserverurl:"https://tag-manager.frontiersin.org",gtmid:"gtm-m322fv2",gtmauth:"owvbwxfajr21yqv1fe1caq",gtmpreview:"env-1",figsharepluginurl:"https://widgets.figshare.com/static/figshare.js",figshareapiurl:"https://api.figshare.com/v2/collections/search",figsharetimeout:3000,crossmarkpublisheddate:"2015/08/24",googlerecaptchasitekey:"6ldg3i0uaaaaaoc4quh35ubhgjotehp_stxhgr_v",googlerecaptchakeyname:"frontiersrecaptchav2",faviconsize16:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_16-tenantfavicon-frontiers.png",faviconsize32:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_32-tenantfavicon-frontiers.png",faviconsize180:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_180-tenantfavicon-frontiers.png",faviconsize192:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_192-tenantfavicon-frontiers.png",faviconsize512:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_512-tenantfavicon-frontiers.png",socialmediaimg:"https://brand.frontiersin.org/m/1c8bcb536c789e11/guidelines-frontiers_logo_1200x628_1-91to1.png",linkedarticlecopytext:"'{\"articletypecopytext\":[{\"articletypeid\":0,\"originalarticlecopytext\":\"part of this article's content has been mentioned in:\",\"linkedarticlecopytext\":\"this article mentions parts of:\"},{\"articletypeid\":122,\"originalarticlecopytext\":\"parts of this article's content have been modified or rectified in:\",\"linkedarticlecopytext\":\"this article is an erratum on:\"},{\"articletypeid\":129,\"originalarticlecopytext\":\"parts of this article's content have been modified or rectified in:\",\"linkedarticlecopytext\":\"this article is an addendum to:\"},{\"articletypeid\":128,\"originalarticlecopytext\":\"a correction has been applied to this article in:\",\"linkedarticlecopytext\":\"this article is a correction to:\"},{\"articletypeid\":134,\"originalarticlecopytext\":\"a retraction of this article was approved in:\",\"linkedarticlecopytext\":\"this article is a retraction of:\"},{\"articletypeid\":29,\"originalarticlecopytext\":\"a commentary has been posted on this article:\",\"linkedarticlecopytext\":\"this article is a commentary on:\"},{\"articletypeid\":30,\"originalarticlecopytext\":\"a commentary has been posted on this article:\",\"linkedarticlecopytext\":\"this article is a commentary on:\"}],\"articleidcopytext\":[]}'\n",acceptedarticleexcludedjournalids:"'[2766,979]'\n",articletypeconfigurablelabel:"\u003c\u003carticle-type:uppercase>> article",settingsfeaturesswitchers:"'{\"displaytitlepilllabels\":true,\"displayrelatedarticlesbox\":true,\"showeditors\":true,\"showreviewers\":true,\"showloopimpactlink\":true,\"enablefigshare\":false,\"usexmlimages\":true}'\n",journalswitharticlehub:"'{\"strings\":[\"science\"]}'\n",terminologysettings:"'{\"terms\":[{\"sequencenumber\":1,\"key\":\"frontiers\",\"tenantterm\":\"frontiers\",\"frontiersdefaultterm\":\"frontiers\",\"category\":\"customer\"},{\"sequencenumber\":2,\"key\":\"submission_system\",\"tenantterm\":\"submission system\",\"frontiersdefaultterm\":\"submission system\",\"category\":\"product\"},{\"sequencenumber\":3,\"key\":\"public_pages\",\"tenantterm\":\"public pages\",\"frontiersdefaultterm\":\"public pages\",\"category\":\"product\"},{\"sequencenumber\":4,\"key\":\"my_frontiers\",\"tenantterm\":\"my frontiers\",\"frontiersdefaultterm\":\"my frontiers\",\"category\":\"product\"},{\"sequencenumber\":5,\"key\":\"digital_editorial_office\",\"tenantterm\":\"digital editorial office\",\"frontiersdefaultterm\":\"digital editorial office\",\"category\":\"product\"},{\"sequencenumber\":6,\"key\":\"deo\",\"tenantterm\":\"deo\",\"frontiersdefaultterm\":\"deo\",\"category\":\"product\"},{\"sequencenumber\":7,\"key\":\"digital_editorial_office_for_chiefs\",\"tenantterm\":\"digital editorial office for chiefs\",\"frontiersdefaultterm\":\"digital editorial office for chiefs\",\"category\":\"product\"},{\"sequencenumber\":8,\"key\":\"digital_editorial_office_for_eof\",\"tenantterm\":\"digital editorial office for eof\",\"frontiersdefaultterm\":\"digital editorial office for eof\",\"category\":\"product\"},{\"sequencenumber\":9,\"key\":\"editorial_office\",\"tenantterm\":\"editorial office\",\"frontiersdefaultterm\":\"editorial office\",\"category\":\"product\"},{\"sequencenumber\":10,\"key\":\"eof\",\"tenantterm\":\"eof\",\"frontiersdefaultterm\":\"eof\",\"category\":\"product\"},{\"sequencenumber\":11,\"key\":\"research_topic_management\",\"tenantterm\":\"research topic management\",\"frontiersdefaultterm\":\"research topic management\",\"category\":\"product\"},{\"sequencenumber\":12,\"key\":\"review_forum\",\"tenantterm\":\"review forum\",\"frontiersdefaultterm\":\"review forum\",\"category\":\"product\"},{\"sequencenumber\":13,\"key\":\"accounting_office\",\"tenantterm\":\"accounting office\",\"frontiersdefaultterm\":\"accounting office\",\"category\":\"product\"},{\"sequencenumber\":14,\"key\":\"aof\",\"tenantterm\":\"aof\",\"frontiersdefaultterm\":\"aof\",\"category\":\"product\"},{\"sequencenumber\":15,\"key\":\"publishing_office\",\"tenantterm\":\"publishing office\",\"frontiersdefaultterm\":\"publishing office\",\"category\":\"product\"},{\"sequencenumber\":16,\"key\":\"production_office\",\"tenantterm\":\"production office forum\",\"frontiersdefaultterm\":\"production office forum\",\"category\":\"product\"},{\"sequencenumber\":17,\"key\":\"pof\",\"tenantterm\":\"pof\",\"frontiersdefaultterm\":\"pof\",\"category\":\"product\"},{\"sequencenumber\":18,\"key\":\"book_office_forum\",\"tenantterm\":\"book office forum\",\"frontiersdefaultterm\":\"book office forum\",\"category\":\"product\"},{\"sequencenumber\":19,\"key\":\"bof\",\"tenantterm\":\"bof\",\"frontiersdefaultterm\":\"bof\",\"category\":\"product\"},{\"sequencenumber\":20,\"key\":\"aira\",\"tenantterm\":\"aira\",\"frontiersdefaultterm\":\"aira\",\"category\":\"product\"},{\"sequencenumber\":21,\"key\":\"editorial_board_management\",\"tenantterm\":\"editorial board management\",\"frontiersdefaultterm\":\"editorial board management\",\"category\":\"product\"},{\"sequencenumber\":22,\"key\":\"ebm\",\"tenantterm\":\"ebm\",\"frontiersdefaultterm\":\"ebm\",\"category\":\"product\"},{\"sequencenumber\":23,\"key\":\"domain\",\"tenantterm\":\"domain\",\"frontiersdefaultterm\":\"domain\",\"category\":\"taxonomy\"},{\"sequencenumber\":24,\"key\":\"journal\",\"tenantterm\":\"journal\",\"frontiersdefaultterm\":\"journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":25,\"key\":\"section\",\"tenantterm\":\"section\",\"frontiersdefaultterm\":\"section\",\"category\":\"taxonomy\"},{\"sequencenumber\":26,\"key\":\"domains\",\"tenantterm\":\"domains\",\"frontiersdefaultterm\":\"domains\",\"category\":\"taxonomy\"},{\"sequencenumber\":27,\"key\":\"specialty_section\",\"tenantterm\":\"specialty section\",\"frontiersdefaultterm\":\"specialty section\",\"category\":\"taxonomy\"},{\"sequencenumber\":28,\"key\":\"specialty_journal\",\"tenantterm\":\"specialty journal\",\"frontiersdefaultterm\":\"specialty journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":29,\"key\":\"journals\",\"tenantterm\":\"journals\",\"frontiersdefaultterm\":\"journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":30,\"key\":\"sections\",\"tenantterm\":\"sections\",\"frontiersdefaultterm\":\"sections\",\"category\":\"taxonomy\"},{\"sequencenumber\":31,\"key\":\"specialty_sections\",\"tenantterm\":\"specialty sections\",\"frontiersdefaultterm\":\"specialty sections\",\"category\":\"taxonomy\"},{\"sequencenumber\":32,\"key\":\"specialty_journals\",\"tenantterm\":\"specialty journals\",\"frontiersdefaultterm\":\"specialty journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":33,\"key\":\"manuscript\",\"tenantterm\":\"manuscript\",\"frontiersdefaultterm\":\"manuscript\",\"category\":\"core\"},{\"sequencenumber\":34,\"key\":\"manuscripts\",\"tenantterm\":\"manuscripts\",\"frontiersdefaultterm\":\"manuscripts\",\"category\":\"core\"},{\"sequencenumber\":35,\"key\":\"article\",\"tenantterm\":\"article\",\"frontiersdefaultterm\":\"article\",\"category\":\"core\"},{\"sequencenumber\":36,\"key\":\"articles\",\"tenantterm\":\"articles\",\"frontiersdefaultterm\":\"articles\",\"category\":\"core\"},{\"sequencenumber\":37,\"key\":\"article_type\",\"tenantterm\":\"article type\",\"frontiersdefaultterm\":\"article type\",\"category\":\"core\"},{\"sequencenumber\":38,\"key\":\"article_types\",\"tenantterm\":\"article types\",\"frontiersdefaultterm\":\"article types\",\"category\":\"core\"},{\"sequencenumber\":39,\"key\":\"author\",\"tenantterm\":\"author\",\"frontiersdefaultterm\":\"author\",\"category\":\"label (role)\"},{\"sequencenumber\":40,\"key\":\"authors\",\"tenantterm\":\"authors\",\"frontiersdefaultterm\":\"authors\",\"category\":\"label (role)\"},{\"sequencenumber\":41,\"key\":\"authoring\",\"tenantterm\":\"authoring\",\"frontiersdefaultterm\":\"authoring\",\"category\":\"core\"},{\"sequencenumber\":42,\"key\":\"authored\",\"tenantterm\":\"authored\",\"frontiersdefaultterm\":\"authored\",\"category\":\"core\"},{\"sequencenumber\":43,\"key\":\"accept\",\"tenantterm\":\"accept\",\"frontiersdefaultterm\":\"accept\",\"category\":\"process\"},{\"sequencenumber\":44,\"key\":\"accepted\",\"tenantterm\":\"accepted\",\"frontiersdefaultterm\":\"accepted\",\"category\":\"process\"},{\"sequencenumber\":45,\"key\":\"assistant_field_chief_editor\",\"tenantterm\":\"assistant field chief editor\",\"frontiersdefaultterm\":\"assistant field chief editor\",\"description\":\"an editorial role on a field journal that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":46,\"key\":\"assistant_specialty_chief_editor\",\"tenantterm\":\"assistant specialty chief editor\",\"frontiersdefaultterm\":\"assistant specialty chief editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":47,\"key\":\"assistant_specialty_chief_editors\",\"tenantterm\":\"assistant specialty chief editors\",\"frontiersdefaultterm\":\"assistant specialty chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":48,\"key\":\"associate_editor\",\"tenantterm\":\"associate editor\",\"frontiersdefaultterm\":\"associate editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":49,\"key\":\"specialty_chief_editor\",\"tenantterm\":\"specialty chief editor\",\"frontiersdefaultterm\":\"specialty chief editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":50,\"key\":\"specialty_chief_editors\",\"tenantterm\":\"specialty chief editors\",\"frontiersdefaultterm\":\"specialty chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":51,\"key\":\"chief_editor\",\"tenantterm\":\"chief editor\",\"frontiersdefaultterm\":\"chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":52,\"key\":\"chief_editors\",\"tenantterm\":\"chief editors\",\"frontiersdefaultterm\":\"chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":53,\"key\":\"call_for_participation\",\"tenantterm\":\"call for participation\",\"frontiersdefaultterm\":\"call for participation\",\"category\":\"process\"},{\"sequencenumber\":54,\"key\":\"citation\",\"tenantterm\":\"citation\",\"frontiersdefaultterm\":\"citation\",\"category\":\"misc.\"},{\"sequencenumber\":55,\"key\":\"citations\",\"tenantterm\":\"citations\",\"frontiersdefaultterm\":\"citations\",\"category\":\"misc.\"},{\"sequencenumber\":56,\"key\":\"contributor\",\"tenantterm\":\"contributor\",\"frontiersdefaultterm\":\"contributor\",\"category\":\"label (role)\"},{\"sequencenumber\":57,\"key\":\"contributors\",\"tenantterm\":\"contributors\",\"frontiersdefaultterm\":\"contributors\",\"category\":\"label (role)\"},{\"sequencenumber\":58,\"key\":\"corresponding_author\",\"tenantterm\":\"corresponding author\",\"frontiersdefaultterm\":\"corresponding author\",\"category\":\"label (role)\"},{\"sequencenumber\":59,\"key\":\"corresponding_authors\",\"tenantterm\":\"corresponding authors\",\"frontiersdefaultterm\":\"corresponding authors\",\"category\":\"label (role)\"},{\"sequencenumber\":60,\"key\":\"decline\",\"tenantterm\":\"decline\",\"frontiersdefaultterm\":\"decline\",\"category\":\"process\"},{\"sequencenumber\":61,\"key\":\"declined\",\"tenantterm\":\"declined\",\"frontiersdefaultterm\":\"declined\",\"category\":\"process\"},{\"sequencenumber\":62,\"key\":\"reject\",\"tenantterm\":\"reject\",\"frontiersdefaultterm\":\"reject\",\"category\":\"process\"},{\"sequencenumber\":63,\"key\":\"rejected\",\"tenantterm\":\"rejected\",\"frontiersdefaultterm\":\"rejected\",\"category\":\"process\"},{\"sequencenumber\":64,\"key\":\"publish\",\"tenantterm\":\"publish\",\"frontiersdefaultterm\":\"publish\",\"category\":\"core\"},{\"sequencenumber\":65,\"key\":\"published\",\"tenantterm\":\"published\",\"frontiersdefaultterm\":\"published\",\"category\":\"core\"},{\"sequencenumber\":66,\"key\":\"publication\",\"tenantterm\":\"publication\",\"frontiersdefaultterm\":\"publication\",\"category\":\"core\"},{\"sequencenumber\":67,\"key\":\"peer_review\",\"tenantterm\":\"peer review\",\"frontiersdefaultterm\":\"peer review\",\"category\":\"peer review process\"},{\"sequencenumber\":68,\"key\":\"peer_reviewed\",\"tenantterm\":\"peer reviewed\",\"frontiersdefaultterm\":\"peer reviewed\",\"category\":\"peer review process\"},{\"sequencenumber\":69,\"key\":\"initial_validation\",\"tenantterm\":\"initial validation\",\"frontiersdefaultterm\":\"initial validation\",\"category\":\"peer review process\"},{\"sequencenumber\":70,\"key\":\"editorial_assignment\",\"tenantterm\":\"editorial assignment\",\"frontiersdefaultterm\":\"editorial assignment\",\"category\":\"peer review process\"},{\"sequencenumber\":71,\"key\":\"independent_review\",\"tenantterm\":\"independent review\",\"frontiersdefaultterm\":\"independent review\",\"category\":\"peer review process\"},{\"sequencenumber\":72,\"key\":\"interactive_review\",\"tenantterm\":\"interactive review\",\"frontiersdefaultterm\":\"interactive review\",\"category\":\"peer review process\"},{\"sequencenumber\":73,\"key\":\"review\",\"tenantterm\":\"review\",\"frontiersdefaultterm\":\"review\",\"category\":\"peer review process\"},{\"sequencenumber\":74,\"key\":\"reviewing\",\"tenantterm\":\"reviewing\",\"frontiersdefaultterm\":\"reviewing\",\"category\":\"peer review process\"},{\"sequencenumber\":75,\"key\":\"reviewer\",\"tenantterm\":\"reviewer\",\"frontiersdefaultterm\":\"reviewer\",\"category\":\"label (role)\"},{\"sequencenumber\":76,\"key\":\"reviewers\",\"tenantterm\":\"reviewers\",\"frontiersdefaultterm\":\"reviewers\",\"category\":\"label (role)\"},{\"sequencenumber\":77,\"key\":\"review_finalized\",\"tenantterm\":\"review finalized\",\"frontiersdefaultterm\":\"review finalized\",\"category\":\"peer review process\"},{\"sequencenumber\":78,\"key\":\"final_decision\",\"tenantterm\":\"final decision\",\"frontiersdefaultterm\":\"final decision\",\"category\":\"peer review process\"},{\"sequencenumber\":79,\"key\":\"final_validation\",\"tenantterm\":\"final validation\",\"frontiersdefaultterm\":\"final validation\",\"category\":\"peer review process\"},{\"sequencenumber\":80,\"key\":\"ae_accept_manuscript\",\"tenantterm\":\"recommend to accept manuscript\",\"frontiersdefaultterm\":\"accept manuscript\",\"category\":\"process\"},{\"sequencenumber\":81,\"key\":\"fee\",\"tenantterm\":\"fee\",\"frontiersdefaultterm\":\"fee\",\"category\":\"accounting\"},{\"sequencenumber\":82,\"key\":\"fees\",\"tenantterm\":\"fees\",\"frontiersdefaultterm\":\"fees\",\"category\":\"accounting\"},{\"sequencenumber\":83,\"key\":\"guest_associate_editor\",\"tenantterm\":\"guest associate editor\",\"frontiersdefaultterm\":\"guest associate editor\",\"category\":\"label (role)\"},{\"sequencenumber\":84,\"key\":\"guest_associate_editors\",\"tenantterm\":\"guest associate editors\",\"frontiersdefaultterm\":\"guest associate editors\",\"category\":\"label (role)\"},{\"sequencenumber\":85,\"key\":\"in_review\",\"tenantterm\":\"in review\",\"frontiersdefaultterm\":\"in review\",\"category\":\"peer review process\"},{\"sequencenumber\":86,\"key\":\"institutional_member\",\"tenantterm\":\"institutional partner\",\"frontiersdefaultterm\":\"institutional partner\",\"category\":\"accounting\"},{\"sequencenumber\":87,\"key\":\"institutional_membership\",\"tenantterm\":\"institutional partnership\",\"frontiersdefaultterm\":\"institutional partnership\",\"category\":\"accounting\"},{\"sequencenumber\":88,\"key\":\"article_processing_charge\",\"tenantterm\":\"article processing charge\",\"frontiersdefaultterm\":\"article processing charge\",\"category\":\"accounting\"},{\"sequencenumber\":89,\"key\":\"article_processing_charges\",\"tenantterm\":\"article processing charges\",\"frontiersdefaultterm\":\"article processing charges\",\"category\":\"accounting\"},{\"sequencenumber\":90,\"key\":\"apcs\",\"tenantterm\":\"apcs\",\"frontiersdefaultterm\":\"apcs\",\"category\":\"accounting\"},{\"sequencenumber\":91,\"key\":\"apc\",\"tenantterm\":\"apc\",\"frontiersdefaultterm\":\"apc\",\"category\":\"accounting\"},{\"sequencenumber\":92,\"key\":\"received\",\"tenantterm\":\"received\",\"frontiersdefaultterm\":\"received\",\"description\":\"date manuscript was received on.\",\"category\":\"core\"},{\"sequencenumber\":93,\"key\":\"transferred\",\"tenantterm\":\"transferred\",\"frontiersdefaultterm\":\"transferred\",\"category\":\"core\"},{\"sequencenumber\":94,\"key\":\"transfer\",\"tenantterm\":\"transfer\",\"frontiersdefaultterm\":\"transfer\",\"category\":\"core\"},{\"sequencenumber\":95,\"key\":\"research_topic\",\"tenantterm\":\"research topic\",\"frontiersdefaultterm\":\"research topic\",\"category\":\"core\"},{\"sequencenumber\":96,\"key\":\"research_topics\",\"tenantterm\":\"research topics\",\"frontiersdefaultterm\":\"research topics\",\"category\":\"core\"},{\"sequencenumber\":97,\"key\":\"topic_editor\",\"tenantterm\":\"topic editor\",\"frontiersdefaultterm\":\"topic editor\",\"category\":\"label (role)\"},{\"sequencenumber\":98,\"key\":\"review_editor\",\"tenantterm\":\"community reviewer\",\"frontiersdefaultterm\":\"review editor\",\"category\":\"label (role)\"},{\"sequencenumber\":99,\"key\":\"title\",\"tenantterm\":\"title\",\"frontiersdefaultterm\":\"title\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":100,\"key\":\"running_title\",\"tenantterm\":\"running title\",\"frontiersdefaultterm\":\"running title\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":101,\"key\":\"submit\",\"tenantterm\":\"submit\",\"frontiersdefaultterm\":\"submit\",\"category\":\"process\"},{\"sequencenumber\":102,\"key\":\"submitted\",\"tenantterm\":\"submitted\",\"frontiersdefaultterm\":\"submitted\",\"category\":\"process\"},{\"sequencenumber\":103,\"key\":\"submitting\",\"tenantterm\":\"submitting\",\"frontiersdefaultterm\":\"submitting\",\"category\":\"process\"},{\"sequencenumber\":104,\"key\":\"t_e\",\"tenantterm\":\"te\",\"frontiersdefaultterm\":\"te\",\"category\":\"label (role)\"},{\"sequencenumber\":105,\"key\":\"topic\",\"tenantterm\":\"topic\",\"frontiersdefaultterm\":\"topic\",\"category\":\"process\"},{\"sequencenumber\":106,\"key\":\"topic_summary\",\"tenantterm\":\"topic summary\",\"frontiersdefaultterm\":\"topic summary\",\"category\":\"process\"},{\"sequencenumber\":107,\"key\":\"figure\",\"tenantterm\":\"figure\",\"frontiersdefaultterm\":\"figure\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":108,\"key\":\"figures\",\"tenantterm\":\"figures\",\"frontiersdefaultterm\":\"figures\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":109,\"key\":\"editorial_file\",\"tenantterm\":\"editorial file\",\"frontiersdefaultterm\":\"editorial file\",\"category\":\"core\"},{\"sequencenumber\":110,\"key\":\"editorial_files\",\"tenantterm\":\"editorial files\",\"frontiersdefaultterm\":\"editorial files\",\"category\":\"core\"},{\"sequencenumber\":111,\"key\":\"e_book\",\"tenantterm\":\"e-book\",\"frontiersdefaultterm\":\"e-book\",\"category\":\"core\"},{\"sequencenumber\":112,\"key\":\"organization\",\"tenantterm\":\"organization\",\"frontiersdefaultterm\":\"organization\",\"category\":\"core\"},{\"sequencenumber\":113,\"key\":\"institution\",\"tenantterm\":\"institution\",\"frontiersdefaultterm\":\"institution\",\"category\":\"core\"},{\"sequencenumber\":114,\"key\":\"reference\",\"tenantterm\":\"reference\",\"frontiersdefaultterm\":\"reference\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":115,\"key\":\"references\",\"tenantterm\":\"references\",\"frontiersdefaultterm\":\"references\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":116,\"key\":\"sce\",\"tenantterm\":\"sce\",\"frontiersdefaultterm\":\"sce\",\"description\":\"abbreviation for specialty chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":117,\"key\":\"submission\",\"tenantterm\":\"submission\",\"frontiersdefaultterm\":\"submission\",\"category\":\"process\"},{\"sequencenumber\":118,\"key\":\"submissions\",\"tenantterm\":\"submissions\",\"frontiersdefaultterm\":\"submissions\",\"category\":\"process\"},{\"sequencenumber\":119,\"key\":\"editing\",\"tenantterm\":\"editing\",\"frontiersdefaultterm\":\"editing\",\"category\":\"process\"},{\"sequencenumber\":120,\"key\":\"in_preparation\",\"tenantterm\":\"in preparation\",\"frontiersdefaultterm\":\"in preparation\",\"category\":\"process\"},{\"sequencenumber\":121,\"key\":\"country_region\",\"tenantterm\":\"country/region\",\"frontiersdefaultterm\":\"country/region\",\"description\":\"because of political issues, some of the country listings are actually classified as `regions` and we need to include this. however other clients may not want to do this.\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":122,\"key\":\"countries_regions\",\"tenantterm\":\"countries/regions\",\"frontiersdefaultterm\":\"countries/regions\",\"description\":\"because of political issues, some of the country listings are actually classified as `regions` and we need to include this. however other clients may not want to do this.\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":123,\"key\":\"specialty\",\"tenantterm\":\"specialty\",\"frontiersdefaultterm\":\"specialty\",\"category\":\"core\"},{\"sequencenumber\":124,\"key\":\"specialties\",\"tenantterm\":\"specialties\",\"frontiersdefaultterm\":\"specialties\",\"category\":\"core\"},{\"sequencenumber\":125,\"key\":\"associate_editors\",\"tenantterm\":\"associate editors\",\"frontiersdefaultterm\":\"associate editors\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":126,\"key\":\"reviewed\",\"tenantterm\":\"reviewed\",\"frontiersdefaultterm\":\"reviewed\",\"category\":\"peer review process\"},{\"sequencenumber\":127,\"key\":\"institutional_members\",\"tenantterm\":\"institutional partners\",\"frontiersdefaultterm\":\"institutional partners\",\"category\":\"accounting\"},{\"sequencenumber\":128,\"key\":\"institutional_memberships\",\"tenantterm\":\"institutional partnerships\",\"frontiersdefaultterm\":\"institutional partnerships\",\"category\":\"accounting\"},{\"sequencenumber\":129,\"key\":\"assistant_field_chief_editors\",\"tenantterm\":\"assistant field chief editors\",\"frontiersdefaultterm\":\"assistant field chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":130,\"key\":\"publications\",\"tenantterm\":\"publications\",\"frontiersdefaultterm\":\"publications\",\"category\":\"process\"},{\"sequencenumber\":131,\"key\":\"ae_accepted\",\"tenantterm\":\"recommended acceptance\",\"frontiersdefaultterm\":\"accepted\",\"category\":\"process\"},{\"sequencenumber\":132,\"key\":\"field_journal\",\"tenantterm\":\"field journal\",\"frontiersdefaultterm\":\"field journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":133,\"key\":\"field_journals\",\"tenantterm\":\"field journals\",\"frontiersdefaultterm\":\"field journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":134,\"key\":\"program_manager\",\"tenantterm\":\"program manager\",\"frontiersdefaultterm\":\"program manager\",\"category\":\"label (role)\"},{\"sequencenumber\":135,\"key\":\"journal_manager\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"journal manager\",\"category\":\"label (role)\"},{\"sequencenumber\":136,\"key\":\"journal_specialist\",\"tenantterm\":\"journal specialist\",\"frontiersdefaultterm\":\"journal specialist\",\"category\":\"label (role)\"},{\"sequencenumber\":137,\"key\":\"program_managers\",\"tenantterm\":\"program managers\",\"frontiersdefaultterm\":\"program managers\",\"category\":\"label (role)\"},{\"sequencenumber\":138,\"key\":\"journal_managers\",\"tenantterm\":\"journal managers\",\"frontiersdefaultterm\":\"journal managers\",\"category\":\"label (role)\"},{\"sequencenumber\":139,\"key\":\"journal_specialists\",\"tenantterm\":\"journal specialists\",\"frontiersdefaultterm\":\"journal specialists\",\"category\":\"label (role)\"},{\"sequencenumber\":140,\"key\":\"cover_letter\",\"tenantterm\":\"manuscript contribution to the field\",\"frontiersdefaultterm\":\"manuscript contribution to the field\",\"category\":\"process\"},{\"sequencenumber\":141,\"key\":\"ae_accepted_manuscript\",\"tenantterm\":\"recommended to accept manuscript\",\"frontiersdefaultterm\":\"accepted manuscript\",\"category\":\"process\"},{\"sequencenumber\":142,\"key\":\"recommend_for_rejection\",\"tenantterm\":\"recommend for rejection\",\"frontiersdefaultterm\":\"recommend for rejection\",\"category\":\"process\"},{\"sequencenumber\":143,\"key\":\"recommended_for_rejection\",\"tenantterm\":\"recommended for rejection\",\"frontiersdefaultterm\":\"recommended for rejection\",\"category\":\"process\"},{\"sequencenumber\":144,\"key\":\"ae\",\"tenantterm\":\"ae\",\"frontiersdefaultterm\":\"ae\",\"description\":\"associate editor - board member\",\"category\":\"label (role)\"},{\"sequencenumber\":145,\"key\":\"re\",\"tenantterm\":\"re\",\"frontiersdefaultterm\":\"re\",\"description\":\"review editor\",\"category\":\"label (role)\"},{\"sequencenumber\":146,\"key\":\"rev\",\"tenantterm\":\"rev\",\"frontiersdefaultterm\":\"rev\",\"description\":\"reviewer\",\"category\":\"label (role)\"},{\"sequencenumber\":147,\"key\":\"aut\",\"tenantterm\":\"aut\",\"frontiersdefaultterm\":\"aut\",\"description\":\"author\",\"category\":\"label (role)\"},{\"sequencenumber\":148,\"key\":\"coraut\",\"tenantterm\":\"coraut\",\"frontiersdefaultterm\":\"coraut\",\"description\":\"corresponding author\",\"category\":\"label (role)\"},{\"sequencenumber\":149,\"key\":\"saut\",\"tenantterm\":\"saut\",\"frontiersdefaultterm\":\"saut\",\"description\":\"submitting author\",\"category\":\"label (role)\"},{\"sequencenumber\":150,\"key\":\"coaut\",\"tenantterm\":\"coaut\",\"frontiersdefaultterm\":\"coaut\",\"description\":\"co-author\",\"category\":\"label (role)\"},{\"sequencenumber\":151,\"key\":\"tsof\",\"tenantterm\":\"tsof\",\"frontiersdefaultterm\":\"tsof\",\"description\":\"typesetter\",\"category\":\"label (role)\"},{\"sequencenumber\":152,\"key\":\"typesetting_office\",\"tenantterm\":\"typesetting office\",\"frontiersdefaultterm\":\"typesetting office\",\"category\":\"product\"},{\"sequencenumber\":153,\"key\":\"config\",\"tenantterm\":\"config\",\"frontiersdefaultterm\":\"config\",\"description\":\"configuration office role\",\"category\":\"label (role)\"},{\"sequencenumber\":154,\"key\":\"jm\",\"tenantterm\":\"jm\",\"frontiersdefaultterm\":\"jm\",\"description\":\"journal manager\",\"category\":\"label (role)\"},{\"sequencenumber\":155,\"key\":\"rte\",\"tenantterm\":\"rte\",\"frontiersdefaultterm\":\"rte\",\"description\":\"research topic editor\",\"category\":\"label (role)\"},{\"sequencenumber\":156,\"key\":\"organizations\",\"tenantterm\":\"organizations\",\"frontiersdefaultterm\":\"organizations\",\"category\":\"core\"},{\"sequencenumber\":157,\"key\":\"publishing\",\"tenantterm\":\"publishing\",\"frontiersdefaultterm\":\"publishing\",\"category\":\"core\"},{\"sequencenumber\":158,\"key\":\"acceptance\",\"tenantterm\":\"acceptance\",\"frontiersdefaultterm\":\"acceptance\",\"category\":\"process\"},{\"sequencenumber\":159,\"key\":\"preferred_associate_editor\",\"tenantterm\":\"preferred associate editor\",\"frontiersdefaultterm\":\"preferred associate editor\",\"category\":\"label (role)\"},{\"sequencenumber\":160,\"key\":\"topic_editors\",\"tenantterm\":\"topic editors\",\"frontiersdefaultterm\":\"topic editors\",\"category\":\"label (role)\"},{\"sequencenumber\":161,\"key\":\"institutions\",\"tenantterm\":\"institutions\",\"frontiersdefaultterm\":\"institutions\",\"category\":\"core\"},{\"sequencenumber\":162,\"key\":\"author(s)\",\"tenantterm\":\"author(s)\",\"frontiersdefaultterm\":\"author(s)\",\"category\":\"label (role)\"},{\"sequencenumber\":163,\"key\":\"figure(s)\",\"tenantterm\":\"figure(s)\",\"frontiersdefaultterm\":\"figure(s)\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":164,\"key\":\"co-authors\",\"tenantterm\":\"co-authors\",\"frontiersdefaultterm\":\"co-authors\",\"description\":\"co-authors\",\"category\":\"label (role)\"},{\"sequencenumber\":165,\"key\":\"editorial_board_members\",\"tenantterm\":\"editorial board members\",\"frontiersdefaultterm\":\"editorial board members\",\"description\":\"editorial board members\",\"category\":\"label (role)\"},{\"sequencenumber\":166,\"key\":\"editorial_board\",\"tenantterm\":\"editorial board\",\"frontiersdefaultterm\":\"editorial board\",\"description\":\"editorial board\",\"category\":\"product\"},{\"sequencenumber\":167,\"key\":\"co-authorship\",\"tenantterm\":\"co-authorship\",\"frontiersdefaultterm\":\"co-authorship\",\"description\":\"co-authorship\",\"category\":\"misc.\"},{\"sequencenumber\":168,\"key\":\"role_id_1\",\"tenantterm\":\"registration office\",\"frontiersdefaultterm\":\"registration office\",\"category\":\"user role\"},{\"sequencenumber\":169,\"key\":\"role_id_2\",\"tenantterm\":\"editorial office\",\"frontiersdefaultterm\":\"editorial office\",\"category\":\"user role\"},{\"sequencenumber\":170,\"key\":\"role_id_7\",\"tenantterm\":\"field chief editor\",\"frontiersdefaultterm\":\"field chief editor\",\"category\":\"user role\"},{\"sequencenumber\":171,\"key\":\"role_id_8\",\"tenantterm\":\"assistant field chief editor\",\"frontiersdefaultterm\":\"assistant field chief editor\",\"category\":\"user role\"},{\"sequencenumber\":172,\"key\":\"role_id_9\",\"tenantterm\":\"specialty chief editor\",\"frontiersdefaultterm\":\"specialty chief editor\",\"category\":\"user role\"},{\"sequencenumber\":173,\"key\":\"role_id_10\",\"tenantterm\":\"assistant specialty chief editor\",\"frontiersdefaultterm\":\"assistant specialty chief editor\",\"category\":\"user role\"},{\"sequencenumber\":174,\"key\":\"role_id_11\",\"tenantterm\":\"associate editor\",\"frontiersdefaultterm\":\"associate editor\",\"category\":\"user role\"},{\"sequencenumber\":175,\"key\":\"role_id_12\",\"tenantterm\":\"guest associate editor\",\"frontiersdefaultterm\":\"guest associate editor\",\"category\":\"user role\"},{\"sequencenumber\":176,\"key\":\"role_id_13\",\"tenantterm\":\"review editor\",\"frontiersdefaultterm\":\"review editor\",\"category\":\"user role\"},{\"sequencenumber\":177,\"key\":\"role_id_14\",\"tenantterm\":\"reviewer\",\"frontiersdefaultterm\":\"reviewer\",\"category\":\"user role\"},{\"sequencenumber\":178,\"key\":\"role_id_15\",\"tenantterm\":\"author\",\"frontiersdefaultterm\":\"author\",\"category\":\"user role\"},{\"sequencenumber\":179,\"key\":\"role_id_16\",\"tenantterm\":\"corresponding author\",\"frontiersdefaultterm\":\"corresponding author\",\"category\":\"user role\"},{\"sequencenumber\":180,\"key\":\"role_id_17\",\"tenantterm\":\"submitting author\",\"frontiersdefaultterm\":\"submitting author\",\"category\":\"user role\"},{\"sequencenumber\":181,\"key\":\"role_id_18\",\"tenantterm\":\"co-author\",\"frontiersdefaultterm\":\"co-author\",\"category\":\"user role\"},{\"sequencenumber\":182,\"key\":\"role_id_20\",\"tenantterm\":\"production office\",\"frontiersdefaultterm\":\"production office\",\"category\":\"user role\"},{\"sequencenumber\":183,\"key\":\"role_id_22\",\"tenantterm\":\"typesetting office (typesetter)\",\"frontiersdefaultterm\":\"typesetting office (typesetter)\",\"category\":\"user role\"},{\"sequencenumber\":184,\"key\":\"role_id_24\",\"tenantterm\":\"registered user\",\"frontiersdefaultterm\":\"registered user\",\"category\":\"user role\"},{\"sequencenumber\":185,\"key\":\"role_id_35\",\"tenantterm\":\"job office\",\"frontiersdefaultterm\":\"job office\",\"category\":\"user role\"},{\"sequencenumber\":186,\"key\":\"role_id_41\",\"tenantterm\":\"special event administrator\",\"frontiersdefaultterm\":\"special event administrator\",\"category\":\"user role\"},{\"sequencenumber\":187,\"key\":\"role_id_42\",\"tenantterm\":\"special event reviewer\",\"frontiersdefaultterm\":\"special event reviewer\",\"category\":\"user role\"},{\"sequencenumber\":188,\"key\":\"role_id_43\",\"tenantterm\":\"submit abstract\",\"frontiersdefaultterm\":\"submit abstract\",\"category\":\"user role\"},{\"sequencenumber\":189,\"key\":\"role_id_52\",\"tenantterm\":\"events office\",\"frontiersdefaultterm\":\"events office\",\"category\":\"user role\"},{\"sequencenumber\":190,\"key\":\"role_id_53\",\"tenantterm\":\"event administrator\",\"frontiersdefaultterm\":\"event administrator\",\"category\":\"user role\"},{\"sequencenumber\":191,\"key\":\"role_id_89\",\"tenantterm\":\"content management office\",\"frontiersdefaultterm\":\"content management office\",\"category\":\"user role\"},{\"sequencenumber\":192,\"key\":\"role_id_98\",\"tenantterm\":\"accounting office\",\"frontiersdefaultterm\":\"accounting office\",\"category\":\"user role\"},{\"sequencenumber\":193,\"key\":\"role_id_99\",\"tenantterm\":\"projects\",\"frontiersdefaultterm\":\"projects\",\"category\":\"user role\"},{\"sequencenumber\":194,\"key\":\"role_id_103\",\"tenantterm\":\"configuration office\",\"frontiersdefaultterm\":\"configuration office\",\"category\":\"user role\"},{\"sequencenumber\":195,\"key\":\"role_id_104\",\"tenantterm\":\"beta user\",\"frontiersdefaultterm\":\"beta user\",\"category\":\"user role\"},{\"sequencenumber\":196,\"key\":\"role_id_106\",\"tenantterm\":\"wfconf\",\"frontiersdefaultterm\":\"wfconf\",\"category\":\"user role\"},{\"sequencenumber\":197,\"key\":\"role_id_107\",\"tenantterm\":\"rt management beta user\",\"frontiersdefaultterm\":\"rt management beta user\",\"category\":\"user role\"},{\"sequencenumber\":198,\"key\":\"role_id_108\",\"tenantterm\":\"deo beta user\",\"frontiersdefaultterm\":\"deo beta user\",\"category\":\"user role\"},{\"sequencenumber\":199,\"key\":\"role_id_109\",\"tenantterm\":\"search beta user\",\"frontiersdefaultterm\":\"search beta user\",\"category\":\"user role\"},{\"sequencenumber\":200,\"key\":\"role_id_110\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"journal manager\",\"category\":\"user role\"},{\"sequencenumber\":201,\"key\":\"role_id_111\",\"tenantterm\":\"myfrontiers beta user\",\"frontiersdefaultterm\":\"myfrontiers beta user\",\"category\":\"user role\"},{\"sequencenumber\":202,\"key\":\"role_id_21\",\"tenantterm\":\"copy editor\",\"frontiersdefaultterm\":\"copy editor\",\"category\":\"user role\"},{\"sequencenumber\":203,\"key\":\"role_id_1_abr\",\"tenantterm\":\"rof\",\"frontiersdefaultterm\":\"rof\",\"category\":\"user role\"},{\"sequencenumber\":204,\"key\":\"role_id_2_abr\",\"tenantterm\":\"eof\",\"frontiersdefaultterm\":\"eof\",\"category\":\"user role\"},{\"sequencenumber\":205,\"key\":\"role_id_7_abr\",\"tenantterm\":\"fce\",\"frontiersdefaultterm\":\"fce\",\"category\":\"user role\"},{\"sequencenumber\":206,\"key\":\"role_id_8_abr\",\"tenantterm\":\"afce\",\"frontiersdefaultterm\":\"afce\",\"category\":\"user role\"},{\"sequencenumber\":207,\"key\":\"role_id_9_abr\",\"tenantterm\":\"sce\",\"frontiersdefaultterm\":\"sce\",\"category\":\"user role\"},{\"sequencenumber\":208,\"key\":\"role_id_10_abr\",\"tenantterm\":\"asce\",\"frontiersdefaultterm\":\"asce\",\"category\":\"user role\"},{\"sequencenumber\":209,\"key\":\"role_id_11_abr\",\"tenantterm\":\"ae\",\"frontiersdefaultterm\":\"ae\",\"category\":\"user role\"},{\"sequencenumber\":210,\"key\":\"role_id_12_abr\",\"tenantterm\":\"gae\",\"frontiersdefaultterm\":\"gae\",\"category\":\"user role\"},{\"sequencenumber\":211,\"key\":\"role_id_13_abr\",\"tenantterm\":\"re\",\"frontiersdefaultterm\":\"re\",\"category\":\"user role\"},{\"sequencenumber\":212,\"key\":\"role_id_14_abr\",\"tenantterm\":\"rev\",\"frontiersdefaultterm\":\"rev\",\"category\":\"user role\"},{\"sequencenumber\":213,\"key\":\"role_id_15_abr\",\"tenantterm\":\"aut\",\"frontiersdefaultterm\":\"aut\",\"category\":\"user role\"},{\"sequencenumber\":214,\"key\":\"role_id_16_abr\",\"tenantterm\":\"coraut\",\"frontiersdefaultterm\":\"coraut\",\"category\":\"user role\"},{\"sequencenumber\":215,\"key\":\"role_id_17_abr\",\"tenantterm\":\"saut\",\"frontiersdefaultterm\":\"saut\",\"category\":\"user role\"},{\"sequencenumber\":216,\"key\":\"role_id_18_abr\",\"tenantterm\":\"coaut\",\"frontiersdefaultterm\":\"coaut\",\"category\":\"user role\"},{\"sequencenumber\":217,\"key\":\"role_id_20_abr\",\"tenantterm\":\"pof\",\"frontiersdefaultterm\":\"pof\",\"category\":\"user role\"},{\"sequencenumber\":218,\"key\":\"role_id_22_abr\",\"tenantterm\":\"tsof\",\"frontiersdefaultterm\":\"tsof\",\"category\":\"user role\"},{\"sequencenumber\":219,\"key\":\"role_id_24_abr\",\"tenantterm\":\"ru\",\"frontiersdefaultterm\":\"ru\",\"category\":\"user role\"},{\"sequencenumber\":220,\"key\":\"role_id_35_abr\",\"tenantterm\":\"jof\",\"frontiersdefaultterm\":\"jof\",\"category\":\"user role\"},{\"sequencenumber\":221,\"key\":\"role_id_41_abr\",\"tenantterm\":\"se-adm\",\"frontiersdefaultterm\":\"se-adm\",\"category\":\"user role\"},{\"sequencenumber\":222,\"key\":\"role_id_42_abr\",\"tenantterm\":\"se-rev\",\"frontiersdefaultterm\":\"se-rev\",\"category\":\"user role\"},{\"sequencenumber\":223,\"key\":\"role_id_43_abr\",\"tenantterm\":\"se-aut\",\"frontiersdefaultterm\":\"se-aut\",\"category\":\"user role\"},{\"sequencenumber\":224,\"key\":\"role_id_52_abr\",\"tenantterm\":\"evof\",\"frontiersdefaultterm\":\"evof\",\"category\":\"user role\"},{\"sequencenumber\":225,\"key\":\"role_id_53_abr\",\"tenantterm\":\"ev-adm\",\"frontiersdefaultterm\":\"ev-adm\",\"category\":\"user role\"},{\"sequencenumber\":226,\"key\":\"role_id_89_abr\",\"tenantterm\":\"comof\",\"frontiersdefaultterm\":\"comof\",\"category\":\"user role\"},{\"sequencenumber\":227,\"key\":\"role_id_98_abr\",\"tenantterm\":\"aof\",\"frontiersdefaultterm\":\"aof\",\"category\":\"user role\"},{\"sequencenumber\":228,\"key\":\"role_id_99_abr\",\"tenantterm\":\"projects\",\"frontiersdefaultterm\":\"projects\",\"category\":\"user role\"},{\"sequencenumber\":229,\"key\":\"role_id_103_abr\",\"tenantterm\":\"config\",\"frontiersdefaultterm\":\"config\",\"category\":\"user role\"},{\"sequencenumber\":230,\"key\":\"role_id_104_abr\",\"tenantterm\":\"beta\",\"frontiersdefaultterm\":\"beta\",\"category\":\"user role\"},{\"sequencenumber\":231,\"key\":\"role_id_106_abr\",\"tenantterm\":\"wfconf\",\"frontiersdefaultterm\":\"wfconf\",\"category\":\"user role\"},{\"sequencenumber\":232,\"key\":\"role_id_107_abr\",\"tenantterm\":\"rtbeta\",\"frontiersdefaultterm\":\"rtbeta\",\"category\":\"user role\"},{\"sequencenumber\":233,\"key\":\"role_id_108_abr\",\"tenantterm\":\"deobeta\",\"frontiersdefaultterm\":\"deobeta\",\"category\":\"user role\"},{\"sequencenumber\":234,\"key\":\"role_id_109_abr\",\"tenantterm\":\"searchbeta\",\"frontiersdefaultterm\":\"searchbeta\",\"category\":\"user role\"},{\"sequencenumber\":235,\"key\":\"role_id_110_abr\",\"tenantterm\":\"jm\",\"frontiersdefaultterm\":\"jm\",\"category\":\"user role\"},{\"sequencenumber\":236,\"key\":\"role_id_111_abr\",\"tenantterm\":\"mfbeta\",\"frontiersdefaultterm\":\"mfbeta\",\"category\":\"user role\"},{\"sequencenumber\":237,\"key\":\"role_id_21_abr\",\"tenantterm\":\"coped\",\"frontiersdefaultterm\":\"coped\",\"category\":\"user role\"},{\"sequencenumber\":238,\"key\":\"reviewer_editorial_board\",\"tenantterm\":\"editorial board\",\"frontiersdefaultterm\":\"editorial board\",\"description\":\"this is the label for the review editorial board\",\"category\":\"label\"},{\"sequencenumber\":239,\"key\":\"field_chief_editor\",\"tenantterm\":\"field chief editor\",\"frontiersdefaultterm\":\"field chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":240,\"key\":\"field_chief_editors\",\"tenantterm\":\"field chief editors\",\"frontiersdefaultterm\":\"field chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":241,\"key\":\"editor\",\"tenantterm\":\"editor\",\"frontiersdefaultterm\":\"editor\",\"category\":\"label (role)\"},{\"sequencenumber\":242,\"key\":\"editors\",\"tenantterm\":\"editors\",\"frontiersdefaultterm\":\"editors\",\"category\":\"label (role)\"},{\"sequencenumber\":243,\"key\":\"board\",\"tenantterm\":\"board\",\"frontiersdefaultterm\":\"board\",\"category\":\"label\"},{\"sequencenumber\":244,\"key\":\"boards\",\"tenantterm\":\"boards\",\"frontiersdefaultterm\":\"boards\",\"category\":\"label\"},{\"sequencenumber\":245,\"key\":\"article_collection\",\"tenantterm\":\"article collection\",\"frontiersdefaultterm\":\"article collection\",\"category\":\"label\"},{\"sequencenumber\":246,\"key\":\"article_collections\",\"tenantterm\":\"article collections\",\"frontiersdefaultterm\":\"article collections\",\"category\":\"label\"},{\"sequencenumber\":247,\"key\":\"handling_editor\",\"tenantterm\":\"handling editor\",\"frontiersdefaultterm\":\"associate editor\",\"description\":\"this terminology key is for the person assigned to edit a manuscript. it is a label for the temporary handling editor assignment.\",\"category\":\"label (role)\"},{\"sequencenumber\":248,\"key\":\"handling_editors\",\"tenantterm\":\"handling editors\",\"frontiersdefaultterm\":\"associate editors\",\"description\":\"this terminology key is for the person assigned to edit a manuscript. it is a label for the temporary handling editor assignment.\",\"category\":\"label (role)\"},{\"sequencenumber\":249,\"key\":\"ae_accept\",\"tenantterm\":\"recommend acceptance\",\"frontiersdefaultterm\":\"accept\",\"category\":\"process\"},{\"sequencenumber\":250,\"key\":\"rtm\",\"tenantterm\":\"rtm\",\"frontiersdefaultterm\":\"rtm\",\"category\":\"product\"},{\"sequencenumber\":251,\"key\":\"frontiers_media_sa\",\"tenantterm\":\"frontiers media s.a\",\"frontiersdefaultterm\":\"frontiers media s.a\",\"category\":\"customer\"},{\"sequencenumber\":252,\"key\":\"review_editors\",\"tenantterm\":\"community reviewers\",\"frontiersdefaultterm\":\"review editors\",\"category\":\"label (role)\"},{\"sequencenumber\":253,\"key\":\"journal_card_chief_editor\",\"tenantterm\":\"chief editor\",\"frontiersdefaultterm\":\"chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":254,\"key\":\"journal_card_chief_editors\",\"tenantterm\":\"chief editors\",\"frontiersdefaultterm\":\"chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":255,\"key\":\"call_for_papers\",\"tenantterm\":\"call for papers\",\"frontiersdefaultterm\":\"call for papers\",\"category\":\"label\"},{\"sequencenumber\":256,\"key\":\"calls_for_papers\",\"tenantterm\":\"calls for papers\",\"frontiersdefaultterm\":\"calls for papers\",\"category\":\"label\"},{\"sequencenumber\":257,\"key\":\"supervising_editor\",\"tenantterm\":\"supervising editor\",\"frontiersdefaultterm\":\"supervising editor\",\"description\":\"a chief or assistant chief editor who is assigned to a manuscript to supervise.\",\"category\":\"role\",\"externalkey\":\"supervising_editor\"},{\"sequencenumber\":258,\"key\":\"supervising_editors\",\"tenantterm\":\"supervising editors\",\"frontiersdefaultterm\":\"supervising editors\",\"description\":\"a chief or assistant chief editor who is assigned to a manuscript to supervise.\",\"category\":\"role\",\"externalkey\":\"supervising_editors\"},{\"sequencenumber\":259,\"key\":\"reviewer_endorse\",\"tenantterm\":\"endorse\",\"frontiersdefaultterm\":\"endorse\",\"category\":\"label\"},{\"sequencenumber\":260,\"key\":\"reviewer_endorsed\",\"tenantterm\":\"endorsed\",\"frontiersdefaultterm\":\"endorsed\",\"category\":\"label\"},{\"sequencenumber\":261,\"key\":\"reviewer_endorse_publication\",\"tenantterm\":\"endorse publication\",\"frontiersdefaultterm\":\"endorse publication\",\"category\":\"label\"},{\"sequencenumber\":262,\"key\":\"reviewer_endorsed_publication\",\"tenantterm\":\"endorsed publication\",\"frontiersdefaultterm\":\"endorsed publication\",\"category\":\"label\"},{\"sequencenumber\":263,\"key\":\"editor_role\",\"tenantterm\":\"editor role\",\"frontiersdefaultterm\":\"editor role\",\"category\":\"label\"},{\"sequencenumber\":264,\"key\":\"editor_roles\",\"tenantterm\":\"editor roles\",\"frontiersdefaultterm\":\"editor roles\",\"category\":\"label\"},{\"sequencenumber\":265,\"key\":\"editorial_role\",\"tenantterm\":\"editorial role\",\"frontiersdefaultterm\":\"editorial role\",\"category\":\"label\"},{\"sequencenumber\":266,\"key\":\"editorial_roles\",\"tenantterm\":\"editorial roles\",\"frontiersdefaultterm\":\"editorial roles\",\"category\":\"label\"},{\"sequencenumber\":267,\"key\":\"call_for_paper\",\"tenantterm\":\"call for paper\",\"frontiersdefaultterm\":\"call for paper\",\"category\":\"label\"},{\"sequencenumber\":268,\"key\":\"research_topic_abstract\",\"tenantterm\":\"manuscript summary\",\"frontiersdefaultterm\":\"manuscript summary\",\"category\":\"process\"},{\"sequencenumber\":269,\"key\":\"research_topic_abstracts\",\"tenantterm\":\"manuscript summaries\",\"frontiersdefaultterm\":\"manuscript summaries\",\"category\":\"process\"},{\"sequencenumber\":270,\"key\":\"submissions_team_manager\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"content manager\",\"category\":\"process\"},{\"sequencenumber\":271,\"key\":\"submissions_team\",\"tenantterm\":\"journal team\",\"frontiersdefaultterm\":\"content team\",\"category\":\"process\"},{\"sequencenumber\":272,\"key\":\"topic_coordinator\",\"tenantterm\":\"topic coordinator\",\"frontiersdefaultterm\":\"topic coordinator\",\"category\":\"process\"},{\"sequencenumber\":273,\"key\":\"topic_coordinators\",\"tenantterm\":\"topic coordinators\",\"frontiersdefaultterm\":\"topic coordinators\",\"category\":\"process\"},{\"sequencenumber\":274,\"key\":\"frontiers_journal_team\",\"tenantterm\":\"frontiers journal team\",\"frontiersdefaultterm\":\"frontiers journal team\",\"category\":\"label (role)\"},{\"sequencenumber\":275,\"key\":\"research_topic_ic_submission\",\"tenantterm\":\"rt:ic submission\",\"frontiersdefaultterm\":\"rt:ic submission\",\"category\":\"label (role)\"},{\"sequencenumber\":276,\"key\":\"research_topic_ic_submission_participant\",\"tenantterm\":\"rt:ic submission participant\",\"frontiersdefaultterm\":\"rt:ic submission participant\",\"category\":\"label (role)\"}]}'\n",impactgtmid:"gtm-njf2ml9b",impactgtmauth:"fyo-7nodm9w37qgfms03sa",impactgtmpreview:"env-1",impactgooglemapsapikey:"aizasyctehx2eu8pwfi86gw9fi00ftcykk6ifr8",qualtricsv4tov3:"'{\"enabled\":true,\"interceptid\":\"si_er0e2ze69oz8yn4\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",qualtricsumux:"'{\"enabled\":false,\"interceptid\":\"si_89fphy3etxqbwtu\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",qualtricscontinuousbrand:"'{\"enabled\":true,\"interceptid\":\"si_9zut94ut81fcrh4\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",defaultarticletemplate:"v3"},app:{baseurl:"/",buildid:"f2d5b6d8-d01c-4534-90b9-d659f10f17ca",buildassetsdir:"ap-2024/_nuxt",cdnurl:""}}

## Methods
for acquiring disease information suffer from insufficient segmentation accuracy for plant leaf disease images with complex textures and unclear lesion boundaries, thereby resulting in poor disease severity grading performance. with the continuous advancement of computational power, deep learning has increasingly become a key technology for addressing complex lesion segmentation, primarily due to its superior capability in automatic feature extraction ( mao et&#xa0;al., 2023 ). chen et al. (2021) proposed the blsnet method based on the unet semantic segmentation network, enhancing lesion segmentation accuracy through the introduction of attention mechanisms and multi-scale feature fusion. experiments showed that its segmentation and classification accuracy outperformed benchmark models such as deeplabv3+ and unet, preliminarily verifying the reliability of this method in automatic assessment of bls disease severity. ngugi et al. (2020) developed the kijaninet segmentation network based on fully convolutional neural networks, demonstrating excellent performance in tomato leaf segmentation under complex backgrounds; lin et al. (2019) developed a cnn semantic segmentation model that achieved high-precision pixel-level segmentation of cucumber powdery mildew on leaves. additionally, some studies have fused traditional features with deep learning: tripathi (2021) proposed a convolutional neural network method based on alexnet, achieving disease grading by fusing features extracted by the model with external leaf segmentation features; ma et al. (2017) introduced comprehensive color features (ccf) combining hyper-red index, hsv/h and lab/b components, and achieved lesion segmentation via interactive region growing, with experiments confirming that this method enables accurate grading of disease images such as cucumber downy mildew under actual field conditions. however, although such methods have improved grading accuracy to some extent, they still suffer from insufficient feature capture of blurred disease edges, making it difficult to achieve fine-grained differentiation of subtle differences between disease grades ( rastogi et&#xa0;al., 2015 ). in recent years, the application of deep learning in plant disease classification has continued to expand. parashar et al. (2024) systematically validated the capability of complex feature modeling for crop yield prediction, further substantiating the critical role of adaptive feature extraction in agricultural intelligent decision-making. concurrently, vishnoi et al. (2022) enhanced small-target detection precision in apple disease identification through a spatial attention mechanism-based cnn architecture for leaf disease diagnosis. ozturk et al. (2025) constructed an ensemble learning classification model based on resnet50, mobile net, efficientnetb0, and densenet121, enhancing generalization performance through statistical cross-validation and improving decision interpretability via grad-cam visualization. experimental results showed that the ensemble model achieved stable high-precision classification performance across multiple validation rounds. these studies provide robust and interpretable solutions for intelligent plant disease recognition through technical integration and architectural innovation. hu et al. (2021) integrated retinex enhancement, faster r-cnn detection, and vgg16 classification to improve the grading and detection accuracy of blurred diseased leaves. picon et al. (2019) proposed an adaptive deep residual neural network algorithm for the classification of three european wheat diseases (septoria leaf blotch, brown spot, and rust) in real-world scenarios, which effectively improved the classification accuracy of wheat diseases. kim and ahn (2021) employed deep cnn architectures such as resnet, xception, and densenet, combined with transfer learning and fine-tuning, to classify 9 categories of pests, diseases, and healthy states in tomatoes. although densenet combined with the rmsprop algorithm achieved an accuracy of 98.63%, single cnn architectures have clear bottlenecks in handling complex plant lesions and overlapping multiple diseases, including insufficient specificity in feature extraction and limited ability to distinguish subtle differences between lesions. shi et al. (2023) analyzed the application of cnns in evaluating the severity of plant diseases, pointing out that hybrid architectures such as classical cnns, improved cnns, and segmentation networks have limitations in handling complex plant lesions: classification errors caused by concurrent multiple diseases, as well as constraints on model generalization ability due to unbalanced datasets and insufficient annotation accuracy. these issues significantly restrict their precise application in practical agricultural scenarios. although significant advancements have been made in plant lesion segmentation and disease classification using deep learning, the grading task for walnut leaf brown spot disease&#x2014;characterized by blurred edge representation and complex small lesions&#x2014;still faces notable challenges. current mobilevit-based hybrid models fall into two categories: general architectures (mobilevitv1/v2/v3) focus on optimizing the cnn-transformer balance for natural images; domain-improved models (mobile-former/edgevit) are oriented toward medical/industrial tasks, with their task focus on localization rather than grading, which mismatches the pain points and fails to address the blurriness of agricultural lesions, tissue interference, and morphological variations. thus, the present study proposes a novel cogfuse-mobilevit model specifically designed for disease grading, with its specific contributions as follows: 1. a diverse field-collected walnut leaf dataset was constructed, with images divided into four distinct severity levels based on qualitative assessment of disease severity, ensuring the accuracy of disease severity grading. 2. the hierarchical feature selection module (hfsm) enhances the fusion of local details (such as lesion texture and color) with global context through local and global attention mechanisms and task-driven feature selection, while suppressing interference from healthy regions. 3. the edge convolution fusion module (ecfm) strengthens edge details through sobel convolution and residual connections, achieving effective integration of edge-specific features with general features, further enhancing edge details and enabling more precise capture of lesion contour details. 4. the adaptive multi-scale dilated dense inception convolution module (amsddicm) extracts differentiated features using multi-shaped convolution kernels and adaptively fuses multi-scale information via a dynamic weight mechanism, capturing lesion morphologies at different pathogenesis stages. 2 materials and methods 2.1 characteristics of walnut leaf brown spot and classification of disease severity levels in the local standard technical regulations for prevention and control of walnut brown spot ( mcgranahan and leslie, 1991 ), walnut brown spot is divided into four severity levels. the severity of plant leaf diseases is generally determined using the spot coverage method. in this study, walnut leaves with different disease severities were processed to separate disease-infected spots from healthy leaf areas, and the percentage of lesion area to total leaf area was calculated. with reference to the local standard, the disease grades specified in the standard were re-determined through calculations in this study, as shown in table&#xa0;1 . classification of different severity levels of walnut leaf brown spot was conducted in accordance with technical regulations for prevention and control of walnut brown spot ( wang et&#xa0;al., 2022 ; yang et&#xa0;al., 2021 ). table&#xa0;1 table&#xa0;1 . walnut leaf brown spot disease severity grading criteria. walnut leaf brown spot is caused by infection with the fungus ophiognomonia leptostyla ( mircetich et&#xa0;al., 1980 ). in the early infection stage, near-circular or irregular small spots appear on the leaves, with a gray-brown center and dark yellow-green to purplish-brown edges, and diseased leaves tend to fall off prematurely. in the middle stage, elongated elliptical or irregular slightly sunken dark brown lesions form, with larger spot size and light brown edges; longitudinal cracks are often present in the center of the lesions. in the late stage, lesions often coalesce to form large scorched necrotic areas, surrounded by yellow to golden-yellow zones, and small black granules (conidiomata and conidia of the pathogen) are scattered on the surface of the diseased tissue ( mircetich et&#xa0;al., 1980 ). according to the degree of color and texture feature changes in infected leaves, walnut leaf disease is divided into four stages: healthy, early, middle, and late stages, corresponding to disease severity levels: healthy (level 0), mild (level 1), moderate (level 2), and severe (level 3). the different severity levels of walnut leaf brown spot are shown in figure&#xa0;1 . figure&#xa0;1 figure&#xa0;1 . walnut leaves infected with brown spot disease at different severity levels. (a) healthy leaves; (b) mildly infected leaves; (c) moderately infected leaves; (d) severely infected leaves. table&#xa0;1 shows the ratio of walnut leaf brown spot lesion area to total leaf area. level 0 (healthy): healthy leaves without symptoms. level 1 (mild): near-circular or irregular small white spots appear on leaves, accounting for less than 5% of the leaf area. level 2 (moderate): lesions expand into irregular dark brown spots, covering 5% to 30% of the leaf area. level 3 (severe): abundant lesion coalesce to form large scorched necrotic areas, exceeding 30% of the leaf area. 2.2 calculation algorithm for walnut leaf brown spot disease severity levels after capturing walnut leaf brown spot samples using a camera, to minimize subjective bias, this study strictly adhered to the objective quantitative criteria defined in the technical regulations for the control of walnut brown leaf spot ( table&#xa0;1 ), utilizing the lesion area percentage (k) as the primary grading indicator. we re-determined the severity levels through computational analysis, with the specific determination process shown in figure&#xa0;2 . figure&#xa0;2 figure&#xa0;2 . computational method for different severity levels of brown spot infection in walnut leaves. first, a python-based color segmentation algorithm was employed to convert images from the bgr color space to the hsv color space for processing. the hsv color space offers inherent advantages in color segmentation tasks. by analyzing the hue, saturation, and value characteristics of diseased regions, the color threshold range in the hsv space was determined ( chaudhary et&#xa0;al., 2012 ). after generating an initial disease region mask based on this threshold range, morphological operations such as closing and opening were applied to optimize mask quality, effectively eliminating noise while preserving the integrity of lesion contours. subsequently, the original image was converted to grayscale mode, and the leaf region was extracted using an adaptive threshold binarization algorithm. leaf boundaries were localized via contour detection technology, and a complete leaf mask was generated through contour filling. pixel statistical analysis was performed on both the leaf mask and disease mask to calculate the total leaf area and diseased region area, respectively. when a valid leaf area (&gt;0) was detected, the disease severity index was calculated using the following formula: disease severity index ( % ) = s 1 s 2 &#xd7; 100 % &#xa0;&#xa0;&#xa0;s 1 : total area of diseased regions s 2 : total area of the complete leaf. 2.3 dataset construction data were collected from the walnut plantation base of tarim university (alar, xinjiang) between may and october 2024. leaves from three locally dominant early-fruiting cultivars (&#x2018;wen 185&#x2019;, &#x2018;xinxin 2&#x2019;, &#x2018;zha 343&#x2019;)widely cultivated in southern xinjiang were vertically photographed using an iphone 13. this genotypic diversity ensures model generalizability by encompassing varied disease phenotypes. the orchard follows standardized cultivation with 4m plant spacing, 5m row spacing (&#x2248;50 plants/mu), and conventional management practices. sampled trees were in full fruiting stage. to capture disease traits across microenvironments, samples were collected from safely accessible crown layers. the dataset includes healthy and brown spot-infected leaves three severity levels, spanning varied time periods, light conditions, and angles. after removing duplicates and invalid images,5,120 high-quality jpgs were retained for analysis. 2.3.1 development of the walnut leaf brown spot dataset disease severity grading was established through quantitative lesion area analysis ( figure&#xa0;2 ). a representative subset of 512 images (10% of the full 5,120-image dataset) was selected via stratified random sampling, accurately preserving the original severity distribution. to ensure labeling rigor, two plant protection specialists (&gt;5 years&#x2019; expertise in walnut pathology, certified in local protocols) executed standardized annotation: pre-annotation training unified diagnostic criteria for ambiguous cases, with formal annotation commencing only after achieving pre-calibration inter-rater agreement (kappa coefficient &#x2265;0.75). as validated in table&#xa0;2 , dual statistical metrics confirmed gold-standard reliability&#x2014;inter-rater cohen&#x2019;s kappa reached 0.71, meeting landis &amp; koch&#x2019;s &#x201c;substantial agreement&#x201d; threshold; algorithm-expert consensus attained a weighted fleiss&#x2019; kappa of 0.77, substantially outperforming conventional cohen&#x2019;s kappa in multi-annotator scenarios. a three-stage standardization pipeline eliminated preprocessing discrepancies. the final dataset, partitioned into training/testing sets (8:2 ratio), strictly adheres to plant disease survey protocols and provides benchmarked data for deep learning model development ( table&#xa0;3 ). table&#xa0;2 table&#xa0;2 . algorithm vs. expert consensus agreement evaluation. table&#xa0;3 table&#xa0;3 . walnut leaf brown spot disease image acquisition data. 2.4 construction of walnut leaf brown spot disease severity grading model 2.4.1 the optimized mobilevitv3 network: cogfuse-mobilevit to address the challenge of difficult feature capture for small lesions in walnut leaf brown spot severity grading, this study proposes an innovative model, cogfuse-mobilevit. mobilevitv3 was selected as the backbone network due to its suitability for walnut brown spot grading. its hybrid mobilenet-vit architecture integrates cnn-based local feature extraction essential for lesion detail capture with transformer-enabled global contextual modeling critical for analyzing lesion spatial distribution. this design aligns with pathological requirements for severity grading, necessitating concurrent attention to local lesion characteristics and global infection patterns. the lightweight architecture further supports real-time processing on embedded devices, enabling future field deployment. the model embodies a &#x201c;grading task-driven&#x201d; design principle. conceptually, it establishes a disease-specific framework of &#x201c;hierarchical screening to edge enhancement to dynamic fusion.&#x201d; this framework elevates general feature extraction to targeted solutions addressing three major agricultural challenges: suppressing interference from healthy tissues via hierarchical screening resolving feature confusion; strengthening blurred lesion contours through edge enhancement overcoming the bottleneck of edge blurriness; adaptively handling multi-stage morphological variations using dynamic fusion addressing morphological variation challenges. structurally, as illustrated in figure&#xa0;3 , the network implements a progressive feature extraction strategy. the hierarchical feature selection module (hfsm) first fuses shallow and middle-layer features. it employs a hierarchical attention mechanism to enhance semantic consistency while preserving spatial details. the edge convolution fusion module (ecfm) then processes these features. it utilizes learnable sobel operators to extract edge information, which is fused with conventional convolutional features via residual connections to augment edge perception capability. finally, the adaptive multi-scale dilated inception convolution module (amsddicm) enables adaptive processing of multi-scale edge features. this module is capable of both capturing fine edge changes in minute lesions and grasping the overall contour structure of large lesions, thereby comprehensively covering edge features across different developmental stages. based on these enhanced edge features, the network accurately outputs the final disease severity grade. figure&#xa0;3 figure&#xa0;3 . cogfuse-mobilevit architecture diagram (hfsm suppresses healthy regions through dynamic masks, ecfm explicitly extracts edges via sobel convolution, and amsddicm fuses multi-scale features through dynamic weights). 2.4.2 the hierarchical feature selection module the hfsm (hierarchical feature selection module) is an innovative architecture proposed in this study. unlike mobilevit&#x2019;s direct feature concatenation, hfsm utilizes learnable prompt vectors ( equation 1 ) to generate spatial masks, dynamically suppressing non-lesion regions. such task-driven selection is crucial for tiny objects. in the grading task of walnut leaf brown spot disease, this module utilizes a local attention mechanism to focus on micro-regions of walnut leaves, fusing shallow and middle-layer features. meanwhile, the hierarchical feature selection mechanism enables precise capture of local detail features such as lesion texture and color, effectively suppressing interference from healthy leaf regions and enhancing disease features. this provides high-discriminative feature representations for brown spot disease grading while preparing for subsequent lesion edge processing. as depicted in figure&#xa0;4 , the module processes two hierarchical feature maps. initial 1&#xd7;1 convolutions reduce both maps&#x2019; channels to half the output dimension curtailing computational load. one reduced map undergoes bilinear upsampling for spatial alignment with the other. these aligned features are summed and processed by a 3&#xd7;3 convolution to generate base-path features. simultaneously, the original reduced map and upsampled map are fed into dual local-global attention modules for parallel processing ( xu, 2024 ). each group contains local and global branches. during branch processing, the feature map is partitioned into p&#xd7;p non-overlapping patches p i , j via the unfold operation. after calculating the mean of pixel features within each patch, the result is processed using the following core formula: attention distribution generation. figure&#xa0;4 figure&#xa0;4 . architecture diagram of the hierarchical feature selection module (hfsm). z i , j = ml p 2 ( layernor ( ml p 1 ( mean ( p i , j ) ) ) ) ( 1 ) a i , j = softmax ( z i , j ) ( 2 ) equations 1 and 2 convert the mean value of patch features into a high-dimensional feature vector through multi-layer perceptrons (mlp) and layer normalization (layernorm) (wherein( mlp 1 &#xa0;and&#xa0;&#xa0;mlp 2 ) achieve&#xa0;dimensional&#xa0;transformation&#xa0;p 2 &#x2192; ouc / 2 &#x2192; ouc / 2 )), and then generates the attention distribution via the softmax function a i , j . this mechanism enables the model to focus on the key features of lesion regions, weaken the interference from healthy leaf areas, enhance the specificity of feature selection, and provide high-quality features for subsequent precise grading. after the local and global features output by the two modules are spliced along the channel dimension, they are combined with the basic path features to generate the output features of the final processing module. for leaf images with over 80% healthy tissue, dynamic masks are generated via learnable prompt vectors to suppress green texture features while enhancing the gray-brown features of lesions ( mcgranahan and leslie, 1991 ). 2.4.3 ecfm edge convolutional fusion module in the walnut leaf brown spot grading task, lesion edge features are crucial for accurately determining disease severity. the standard mobilevit relies on cnn-transformer blocks to implicitly learn edges, whereas the ecfm (edge convolution fusion module) incorporates sobel convolutions, conventional convolutions, and residual connections, effectively fusing edge features with general features and thereby enhancing edge details. as shown in figure&#xa0;5 , the sobel branch employs sobel convolution to extract image edge information, accurately capturing the contour details of brown spot lesions; the convolutional branch captures general features such as leaf color and texture through standard convolution. the two realize feature fusion via the following core formula ( equation 3 ): figure&#xa0;5 figure&#xa0;5 . architecture diagram of the edge convolutional fusion module (ecfm). s = sobelconv ( x ) &#xa0; ( 3 ) processing the input feature map ( x ) through the sobel convolution operator (sobelconv) specifically extracts edge information of lesions (such as the boundary contours between lesions and healthy tissues, and the edge textures of small lesions). for the commonly seen blurred edges in brown spot disease, sobel convolution can enhance edge gradient changes, making the originally blurred lesion contours clearer, thus providing key edge feature support for subsequent grading. conventional feature extraction and fusion ( equation 4 ): &#xa0; c = conv ( x ) , &#xa0; f concat = concat ( s , c ) ( 4 ) &#xa0;c = conv ( x ) extracting the overall features of leaves (such as the color distribution of lesions and the overall texture of leaves) through standard convolution forms a complement to edge features, preventing the model from focusing only on local edges while ignoring global lesion information. f concat = concat ( s , c ) concatenating the edge feature s and the conventional feature c along the channel dimension achieves the initial fusion of edge details and global features. this fusion enables the model to both identify the fine contours of lesions and judge the disease condition by combining the overall color and texture changes of lesions, thereby improving the accuracy of grading. the module also introduces feature addition and subsequent convolution operations: the fused features are first processed by the first convolution layer f 1 = conv 1 ( f concat ) , the corresponding operation results are added to the original input, and the final output is generated through the second convolution layer f final = conv 2 ( f 1 + x ) . in this process, the residual connection retains the original feature information, further strengthens the integration of edge features and conventional features, and ensures the effective transmission and enhancement of multi-level features. the edge gradient information extracted by sobel convolution (such as grayscale differences between diseased spots and healthy tissues) and the texture features from conventional convolution (such as the roughness of necrotic areas) are fused via residual connections. this not only preserves the blurred edges of early-stage lesions but also prevents edge features from being disconnected from the overall texture ( ghaiwat and arora, 2014 ). 2.4.4 amsddicm adaptive multi-scale dilated depthwise inseparable convolution module this module differs from mobilevit&#x2019;s single-scale convolution, leverages multi-shaped convolution kernels to accurately extract lesion features of circular, irregular, and other shapes from multiple dimensions, enabling the identification of subtle differences in lesion edges and internal colors to enrich feature dimensions. meanwhile, depth wise separable convolutions are employed to accommodate lesion scales at different stages of disease development. specifically, the input features are first split into two groups, each entering the amsddicm module to extract features via multi-scale depth wise separable convolutions. a dynamic weighting mechanism (global pooling + convolution + softmax) is used to generate weights for fusing multi-scale features. the processed features from the two groups are concatenated, and finally, cross-channel fusion is completed via 1&#xd7;1 convolution to output the final optimized features. the entire process integrates multi-scale convolution, dynamic weight allocation, and feature fusion to enhance the model&#x2019;s ability to capture complex features, as shown in figure&#xa0;6 . in response to the morphological differences between early-stage small spots (1-3mm in diameter) and late-stage fused spots (over 20mm in diameter), the dynamic weight mechanism enables the model to adaptively allocate the contributions of 3&#xd7;3 kernels for capturing local textures and 11&#xd7;1 kernels for extracting strip-shaped spreading features. this addresses the limitation of fixed weights in traditional inception architectures in adapting to multi-scale morphologies. figure&#xa0;6 figure&#xa0;6 . architecture diagram of the adaptive multi-scale dilated depth wise inseparable convolution module (amsddicm). when the input feature tensor x with the number of channels c enters the amsddicm module, it first undergoes a feature grouping operation: the input features are evenly split into two groups along the channel dimension, with each group containing half of the original number of channels (c//2). this grouping strategy not only reduces computational complexity but also creates conditions for subsequent multi-scale feature extraction. each feature group is fed into an independent dmsconv2d module, which adopts different configurations such as 3&#xd7;3, 5&#xd7;5 square convolution kernels and 1&#xd7;11, 11&#xd7;1 strip-shaped convolution kernels &#x2014; the square kernels capture local textures, while the strip-shaped kernels extract direction-sensitive long-range dependencies. this dynamic weight allocation draws inspiration from adaptive strategies in agricultural iot systems. similar to how salinity-aware ets models prioritize soil conductivity features under salt stress ( zhang et&#xa0;al., 2023 ), our mechanism selectively amplifies lesion morphological features critical for severity grading. the dynamic weighting mechanism of dmsconv2d generates weights through global average pooling and 1&#xd7;1 convolution, with the core formulas as follows: weighted basic feature generation: x dkw = rearrange ( conv 1 &#xd7; 1 ( avgpoo l2 d ( x ) ) ) ( 5 ) equation 5 compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information (such as the overall distribution characteristics of lesions at different scales), and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization: x dkw = f . softmax ( x dkw ) ( 6 ) equation 6 compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information, such as the overall distribution characteristics of lesions at different scales, and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization: x = &#x2211; i = 0 2 ( dwcon v i ( x ) &#xd7; x dkw , i ) ( 7 ) equation 7 is based on dynamic weights x dkw , i ) for different convolution kernels &#xa0;dwconv i ( x ) weighted fusion is performed on the extracted features. for walnut brown spot lesions exhibiting size heterogeneity and morphological complexity&#x2014;ranging from early-stage circular micro-lesions to late-stage coalesced irregular lesions&#x2014;this mechanism adaptively modulates multi-scale feature contributions. it prioritizes retention of morphology-discriminative features, local textures in small lesions or directional distributions in large lesions, thereby comprehensively capturing pathological characteristics across developmental stages. the processed features undergo channel-wise concatenation to generate optimized outputs ( mircetich et&#xa0;al., 1980 ). 2.5 experimental process for severity grading of walnut leaf brown spot disease figure&#xa0;7 is the overall flow chart of severity grading of walnut leaf spot disease. first is the image acquisition stage, where raw images of walnut leaves are captured and collected to construct an image database containing pictures to be classified ( singh et&#xa0;al., 2019 ). next is the image preprocessing stage, which involves sequentially calculating and classifying disease severity levels, establishing image labels, and performing image preprocessing operations. subsequently, the processed data are used to train the constructed dataset. finally, in the model training and performance evaluation stage, the preprocessed images are input into the model for classifying walnut leaf brown spot disease, after which performance evaluation is conducted on the model&#x2019;s classification results to determine the model&#x2019;s effectiveness and accuracy. figure&#xa0;7 figure&#xa0;7 . overall flow chart for severity grading of walnut leaf brown spot disease. 2.6 experimental parameters and evaluation metrics 2.6.1 test environment and hyperparameter setting the experimental setup of this study is based on deep learning technology and leverages high-performance computing resources. all experiments were conducted on the windows 10 operating system. the hardware platform consists of an amd ryzen 7 3700x processor and an nvidia rtx 2080ti graphics card. the software environment was built using python 3.8, the pytorch 1.13.0 deep learning framework, and the cuda 11.3 parallel computing platform. additionally, pytorch 1.13.0&#x2014;a widely adopted open-source deep learning library renowned for its high flexibility&#x2014;was selected, making it well-suited for research and development. during model training, multiple adjustments were made to the hyperparameters to compare test results and select the optimal hyperparameter combination. the model accepts input images sized at 224&#xd7;224 pixels, with a configured batch size of 32 during training and a maximum of 100 training epochs. the optimizer employs an initial learning rate of 0.003. 2.6.2 evaluation metrics this paper aims to evaluate the performance of the model and verify the effectiveness of the improvement measures. we selected multiple evaluation metrics, and all subsequent experimental results adopt the method of calculating averages via 5-fold cross-validation, aiming to comprehensively and reliably evaluate the model performance. including precision (p), recall (r), etc. ( equations 8 &#x2013; 16 ) these metrics can be calculated using the following formulas. the arithmetic mean of the metric values across the five folds: m &#xaf; = 1 5 &#x2211; i = 1 5 m i ( 8 ) precision = tp / ( tp + fp ) ( 9 ) recall = tp / ( tp + fn ) ( 10 ) f 1 score = 2 tp 2 tp + fp + fn ( 11 ) macro&#xa0;precision= 1 c &#x2211; i = 1 c p i ( 12 ) macro&#xa0;recall= 1 c &#x2211; i = 1 c r i ( 13 ) weighted&#xa0;avg&#xa0;precision= &#x2211; ( n i &#xb7; precisio n i ) &#x2211; n i ( 14 ) weighted&#xa0;avg&#xa0;recall= &#x2211; ( n i &#xb7; precisio n i ) &#x2211; n i ( 15 ) weighted&#xa0;avg&#xa0;recall= &#x2211; i = 1 c t p i n ( 16 ) 3 experiments and results analysis 3.1 core module design and validity experimental verification 3.1.1 comparative test of necessity of hfsm module to validate the necessity of the hierarchical feature selection module (hfsm) within the cogfuse-mobilevit framework, this study conducted comparative tests against two mainstream lightweight attention modules: se (squeeze-and-excitation) and cbam (convolutional block attention module). as illustrated in table&#xa0;4 , the hfsm module achieves a significantly higher accuracy of 86.61%, outperforming the se module and cbam module by 7.38% and 4.46%, respectively. this demonstrates that its hierarchical feature selection mechanism more effectively captures discriminative features. in terms of computational efficiency, the parameters and flops of hfsm remain comparable with those of the se module and cbam module, indicating that its performance breakthrough stems from innovative structural design under equivalent lightweight constraints. comprehensive results confirm that replacing hfsm with se or cbam modules would incur a performance degradation exceeding 4%, thereby validating the indispensable value of hfsm in enabling efficient feature selection within the cogfuse-mobilevit framework. table&#xa0;4 table&#xa0;4 . performance comparison of attention modules within the cogfuse-mobilevit framework. 3.1.2 convolutional kernel selection in amsddicm rigorous validation via kernel combination ablation studies ( table&#xa0;5 ) demonstrates. the hybrid configuration (3&#xd7;3 + 5&#xd7;5 + 1&#xd7;11) significantly outperformed square-only kernels (3&#xd7;3 + 5&#xd7;5) accuracy 86.61% and 82.15% stage-specific recall gains. mid-stage lesions (level 2) raise 9.4 percentage points. late-stage lesions (level 3) raise 6.1 percentage points. this empirically validates the necessity of 1&#xd7;11 rectangular kernels for capturing linear pathological features, overcoming the limitation of isotropic kernels in detecting anisotropic structures. table&#xa0;5 table&#xa0;5 . comparison of different convolution kernels in amsddicm. 3.1.3 the impact of new modules on computational complexity figure&#xa0;8 shows that the hfsm module incurs a 169% flops increase to achieve a breakthrough improvement in early-stage lesion detection&#x2014;reducing level 0/1 misclassification by 24%, thereby establishing the pathological foundation for severity grading. the ecfm module contributes a mere 3% flops increment yet drives a 3.68 percentage point accuracy gain through enhanced edge feature representation. with only a 0.028g flops overhead 1.3%, the amsddicm module enables adaptive fusion of multiscale pathological deformations, culminating in a 7.80-pp accuracy leap. these modules form a cascaded optimization paradigm: hfsm&#x2019;s substantial cost resolves the core pathological bottleneck, while subsequent modules deliver superlinear returns&#x2014;harvesting 6.85-pp accuracy gain with just 14% additional flops&#x2014;collectively establishing the globally optimal computation-performance equilibrium. figure&#xa0;8 figure&#xa0;8 . comparison of the impact of each newly added module on computational complexity (a) accuracy (%); (b) flops (g); (c) params (m). measured on nvidia rtx 2080ti gpu (pytorch 1.13.0, cuda 11.3) with 224&#xd7;224 input. 3.1.4 comparison of the influence of different module fusion on model performance to validate the effect of module fusion, table&#xa0;6 compares model performances with different combinations. when only hfsm (hierarchical feature selection module) is introduced, precision increases from the baseline of 82.23% to 83.77%, but recall decreases by 3 percentage points to 72.31%, causing the f1 score to slightly decline to 78.04%. accuracy rises to 82.15%, indicating improved overall classification correctness of the model, but with potential risk of missed detections. table&#xa0;6 table&#xa0;6 . impact of fusion of different modules on model performance. when hfsm and ecfm (edge-context feature module) are synergistically introduced, precision increases to 86.34%, recall recovers to 74.69%, and both f1 score and accuracy are significantly optimized. hfsm fuses shallow and middle-layer features through a hierarchical attention mechanism, laying the foundation for subsequent edge feature processing; ecfm enhances lesion edge features. the combination of the two effectively improves the integrity of feature representation and the clarity of lesion boundaries. the combination of hfsm and amsddicm (adaptive multi-scale dilated depthwise inseparable convolution module) further pushes recall to 76.24%, accuracy to 83.94%, and f1 score to 80.79%, outperforming the hfsm+ecfm combination. amsddicm compensates for hfsm&#x2019;s deficiency in local feature refinement through attention-guided multi-scale detail fusion, which integrates multi-layer detail feature weights, especially suitable for scenarios with small targets or blurred features. when the three modules work synergistically, all indicators reach optimal levels: precision, recall, f1 score, and accuracy increase by 12.19%, 7.67%, 9.62%, and 10.00% respectively compared with the baseline. among them, hfsm lays the foundation for cross-layer feature fusion, ecfm effectively integrates edge-specific features with general features to enhance image edge information, and amsddicm adaptively fuses multi-scale and multi-type features through an attention mechanism, forming a progressive optimization chain from &#x201c;hierarchical feature extraction&#x201d; to &#x201c;edge semantic enhancement&#x201d; and then to &#x201c;multi-layer detail fusion&#x201d;. the experimental results show a balanced improvement in both precision and recall, indicating that the fusion of the three modules enables the model to focus more on learning the features of small brown spot lesions, thereby improving its classification performance. this synergistic task-specific design, combining hierarchical selection, explicit edge enhancement, and adaptive multi-scale fusion, fundamentally distinguishes cogfuse-mobilevit from prior mobilevit hybrids designed for general vision or localization tasks. 3.1.5 influence of different module combinations on f1-score of level (0-3) in order to verify the targeted improvement of each module for a specific disease level, we conducted the following comparative experiments as shown in the table&#xa0;6 , when hfsm is introduced alone, the f1-score of level 0 increases from 83.10% to 85.60%, effectively reducing feature confusion between healthy leaves and early-stage lesions. after adding ecfm, the f1-score of level 1 rises from 73.20% (with hfsm alone) to 79.80%, significantly mitigating the recognition bias caused by blurred edges of small lesions. amsddicm increases the f1-score of level 3 from 80.37% to 85.54%, enhancing the adaptability to the morphology of large-scale fused scorched areas. with the collaboration of the three modules, the f1-scores of level 0&#x2013;3 reach 93.99%, 82.57%, 84.28%, and 85.54% respectively, achieving balanced optimization of performance across all levels. 3.2 results comparison of different algorithms and statistical significance verification 3.2.1 comparison of grading results for different classification models to verify the effectiveness of the cogfuse-mobilevit model, we selected 9 commonly used classification models to compare with the optimized cogfuse-mobilevit model, and the results are the average of 5-fold cross-validation over 120 epochs. as shown in table&#xa0;7 , the proposed cogfuse-mobilevit model achieved the highest grading performance in terms of precision, recall, and f1-score for identifying walnut leaf brown spot disease at different severity levels among all compared models. the improved cogfuse-mobilevit model exhibits an accuracy of 86.61%, representing a 7.80-percentage-point improvement over the original model. overall, the experimental results highlight that the enhanced cogfuse-mobilevit model is more conducive to focusing on lesion edge details and accurately learning the features of different severity levels of walnut leaf brown spot disease, thereby improving the model&#x2019;s classification performance. table&#xa0;7 table&#xa0;7 . comparison of grading results for different classification models. 3.2.2 comparison of performance and reliability validation of different algorithms in model training, to quantify the reliability of results and avoid random biases from single experiments, this study employs 5-fold cross-validation to generate 95% confidence intervals, as shown in the figure&#xa0;9 . the results demonstrate that cogfuse-mobilevit takes a significant lead with an accuracy of 86.61% (95% ci: [85.24%, 87.89%]). its narrowest confidence interval range indicates the strongest generalization capability. among the comparative models, mobilevitv3 has a 95% ci of [77.20%, 80.42%]; its lower bound is significantly lower than that of cogfuse-mobilevit, confirming that the 7.8% performance improvement is not due to random fluctuations. furthermore, the confidence intervals constructed through 5-fold cross-validation further highlight the reliability of the experimental results. figure&#xa0;9 figure&#xa0;9 . confidence intervals of accuracy for different classification models in 5-fold cross-validation. 3.2.3 statistical significance verification of model improvement to statistically evaluate the performance improvement of cogfuse-mobilevit over the baseline mobilevitv3, an independent samples t-test was conducted ( table&#xa0;8 ). for each model architecture, five independent training runs. the null hypothesis stated that the mean accuracy of cogfuse-mobilevit was equal to that of mobilevitv3 h 0 : &#x3bc; c og = &#x3bc; m o b ), while the alternative hypothesis stated that cogfuse-mobilevit had a higher mean accuracy ( h 1 : &#x3bc; c og &gt; &#x3bc; m o b )cogfuse-mobilevit achieved a mean accuracy, significantly outperforming mobilevitv3. the independent samples t-test confirmed this improvement (t(8)=18.92,p=3.7&#xd7;10 &#x2212;8 ). table&#xa0;8 table&#xa0;8 . comparison of accuracy results between mobilevitv3 and cogfuse-mobilevit using independent samples t-test. under 5-fold cross-validation ( figure&#xa0;10 ), cogfuse-mobilevit achieves rapid convergence within the first 20 epochs, demonstrating more efficient feature learning capability. upon entering the steady-state phase, its validation loss is reduced by over 5-fold compared to the baseline mobilevitv3, directly confirming smaller prediction errors and superior generalization capability. meanwhile, the smooth and minimally fluctuating loss curve of cogfuse-mobilevit reflects strong robustness against data noise and distribution variations. combined with the previous statistical findings from accuracy confidence intervals and independent t-tests, these results collectively validate the statistical significance of the improved model. figure&#xa0;10 figure&#xa0;10 . comparison of loss curves between the cogfuse-mobilevit model and the original model in 5-fold cross-validation within 120 epochs. 3.3 model result analysis performance comparison of different models figure&#xa0;11a shows that in the walnut leaf brown spot disease severity grading task, the classification accuracy of all models gradually improved and stabilized with the increase of training epochs, indicating that the models continuously optimized during learning until convergence. cogfuse-mobilevit stood out prominently: it achieved rapid accuracy improvement, reached a high level in relatively early training epochs with minimal subsequent fluctuations, and stably maintained the highest accuracy, demonstrating fast convergence and strong generalization ability to efficiently extract features distinguishing different disease levels. densenet also maintained high accuracy in the late training stage, but its accuracy improvement was slower, with slightly inferior convergence speed and final stability compared to cogfuse-mobilevit. models like resnet and regnet showed limited accuracy gains with gentle upward trends, and their final stable accuracy values were significantly lower, reflecting insufficient feature extraction capabilities possibly due to network architecture or parameter optimization efficiency. figure&#xa0;11 figure&#xa0;11 . comparison of accuracy and loss across different models (a) accuracy line chart; (b) loss line chart. in figure&#xa0;11b , all models showed high initial loss values that dropped rapidly and then stabilized, following the typical learning process of iterative optimization. cogfuse-mobilevit achieved significant early loss decline and stabilized near the minimum, indicating efficient feature learning and strong fitting capability. while densenet also reached a relatively low loss level, it remained slightly higher than cogfuse-mobilevit. other models had higher late-stage loss values: some showed fast initial decline but converged to higher levels, indicating shortcomings in capturing key disease features. 3.4 analysis of detection results for different classification models the confusion matrix is a key indicator for evaluating classification models: the higher the diagonal values, the higher the classification accuracy for the corresponding category, and the lower the off-diagonal values, the fewer misjudgments. figure&#xa0;12 shows the classification results of different models for the four disease grades (0&#x2013;3) of walnut leaf brown spot disease. it is clearly evident that the overall classification performance, particularly the accuracy of cogfuse-mobilevit across all categories, is remarkably high with minimal misjudgments, demonstrating that the model has enhanced discrimination ability for disease grades and performs optimally in distinguishing the four disease levels. figure&#xa0;12 figure&#xa0;12 . confusion matrices of the improved cogfuse-mobilevit model and traditional classification models (a) densenet; (b) efficientnet; (c) efficientnetv2; (d) swin transformer; (e) mobilevitv3; (f) cogfuse-mobilevit. the edge-enhanced feature module (ecfm) effectively addressed level 0 and level 1 misclassification by capturing incipient lesion features. baseline analysis revealed substantial false negatives for early-stage lesions, with level 1 and level 0 misclassification reaching 32%. ecfm reduced this rate to 8%, demonstrating its capacity to resolve texture confusion through enhanced edge contour delineation. similarly, the adaptive multi-scale dilated convolution module (amsddicm) significantly mitigated level 2 and level 3 mutual misclassification. amsddicm drastically reduced both errors by enhancing local fine-grained features in mid-stage lesions (level 2) while strengthening global fusion features in late-stage coalesced lesions (level 3). this resolves grading ambiguity arising from the morphological continuum of lesion progression. to establish a comprehensive performance evaluation framework and quantify the model&#x2019;s overall discriminative capability, figure&#xa0;13 presents the receiver operating characteristic (roc) curves of cogfuse-mobilevit across four severity levels. these curves compare the true positive rate (tpr) against the false positive rate (fpr) by dynamically adjusting the classification threshold. the area under the curve (auc) serves as the primary performance metric, where a higher value indicates stronger classification ability. notably, the auc values for all categories significantly exceed the level of random guessing (auc=0.5), fully demonstrating that the model possesses robust discriminative capability across different severity levels. figure&#xa0;13 figure&#xa0;13 . roc curves of the four different severity levels for cogfuse-mobilevit. 3.5 tsne visualization of features extracted by different models the tsne visualization of features extracted from the models is shown in figures&#xa0;14a&#x2013;d . this study analyzed the features of the original model at the 20th, 50th, and 100th training epochs, as well as the improved model at the 100th epoch. after 20 epochs of training, the original model showed a highly discrete feature distribution. limited by the number of training epochs, the model failed to fully learn discriminative features, resulting in insufficient distinction between categories and significant overlap among different classes. this indicates that under this training intensity, the original model&#x2019;s ability to capture meaningful patterns was relatively limited. when the original model was trained to 50 epochs, compared with (a), the feature aggregation significantly improved. however, inter-class overlap still existed, suggesting that although prolonged training aided feature learning, the original model&#x2019;s architecture had inherent defects in achieving clear feature separation. at 100 epochs of training, the original model exhibited more prominent feature clustering. nevertheless, some regions still lacked clear separation between different categories, indicating that even after prolonged training, the original model faced challenges in maximizing inter-class distance and intra-class compactness. in figure (d), the improved model after 100 epochs of training demonstrated a more superior feature distribution: each category was tightly clustered, achieving high intra-class compactness, while distinct boundaries between different categories were established, resulting in significant inter-class distance. this suggests that the model improvements effectively enhanced its ability to extract discriminative features, greatly reduced feature ambiguity, and improved feature discriminative power. compared with the original model, it showed stronger feature discrimination and optimization potential. figure&#xa0;14 figure&#xa0;14 . tsne visualization of features extracted by different models. after the three modules work synergistically, the boundaries of feature clustering between level 0 (healthy) and level 1 (early-stage) are significantly clearer, which verifies the effectiveness of edge-texture collaborative learning (a) the original model was trained for 20 epochs; (b) the original model was trained for 50 epochs; (c) the original model was trained for 100 epochs; (d) the improved model was trained for 100 epochs. 3.6 radar chart for comparison of classification performance between original and improved models as shown in figure&#xa0;15 , a radar chart compares the performance of the original model and the improved cogfuse-mobilevit model across multiple evaluation metrics. in terms of accuracy, cogfuse-mobilevit exhibits significantly higher values than the original mobilevitv3 model, indicating that the improved model achieves overall higher classification accuracy. macro-average precision, which considers the average precision of each category, clearly shows that cogfuse-mobilevit also performs better, meaning it has superior precision across all categories. meanwhile, cogfuse-mobilevit also demonstrates better macro-average recall. when examining weighted-average precision and weighted-average recall&#x2014;metrics that account for class imbalance&#x2014;cogfuse-mobilevit outperforms mobilevitv3 in both, indicating that in practical applications, even with uneven class distribution, the cogfuse-mobilevit model can deliver better performance. these results further demonstrate the model&#x2019;s reliability and practicality. figure&#xa0;15 figure&#xa0;15 . multi-metric comparison between mobilevitv3 and improved cogfuse-mobilevit model. 3.7 public data set experiment to further validate the efficacy and generalizability of the proposed cogfuse-mobilevit model, experiments were conducted on the public dataset appleleaf9 ( yang et&#xa0;al., 2022 ). this dataset comprises healthy apple leaves and eight categories of apple leaf diseases captured in field environments without restrictions on imaging angles or noise interference. the dataset was partitioned into training and test sets at an 8:2 ratio. all images were resized to 224&#xd7;224 pixels to optimize deep learning model training efficiency. following the hyperparameter configurations specified in &#x201c;experimental parameters and evaluation metrics&#x201d;, both the baseline mobilevitv3 and cogfuse-mobilevit models were trained and evaluated. cross-species validation demonstrates that the cogfuse-mobilevit model delivers exceptional performance on the appleleaf9 dataset. as presented in table&#xa0;9 , the model comprehensively surpasses the baseline mobilevitv3 across all four core metrics: precision increases by 0.16 percentage points, recall achieves a breakthrough improvement of 3.45 percentage points, f1-score rises by 1.18 percentage points, and accuracy elevates by 1.14 percentage points. the synergistic enhancement in both precision and recall signifies that performance gains stem from strengthened feature discriminability rather than metric trade-off compromises. the remarkable percentage points recall gain substantially mitigates leaf disease omission rates, while the holistic advance in f1-score further attests to the model&#x2019;s robustness. these results collectively validate the generalizability of cogfuse-mobilevit&#x2019;s core innovations in agricultural fine-grained disease grading, establishing a transferable paradigm for small-target pathology identification across plant species. table&#xa0;9 table&#xa0;9 . experimental results of the original model and cogfuse-mobilevit model on appleleaf9 data set. 4 conclusion this study addresses the challenges in precise grading of walnut leaf brown spot disease. by adopting dynamic feature filtering, edge gradient reinforcement, and multi-scale morphological adaptation, it effectively resolves three key limitations of existing hybrid architectures and the baseline model mobilevitv3 in plant disease grading weak capability in capturing blurred edges, poor multi-scale adaptability, and interference from healthy tissues. experimental results show that the model achieves an 86.61% grading accuracy on a dataset encompassing diverse lighting conditions, cultivars, and lesion stages, representing a 7.80% improvement over the baseline mobilevitv3 and outperforming nine state-of-the-art models. at the same time, this study confirmed that the performance improvement of cogfuse-mobilevit was statistically significant and stable through strict statistical tests, providing a reliable method for accurate classification of walnut leaf spot disease. the constructed image dataset and proposed grading re-measurement method lay the foundation for accuracy. this approach provides a new paradigm for small-target disease grading, with core modules transferable to multi-crop disease recognition. beyond disease classification, context-aware ai frameworks demonstrate significant efficacy in agricultural management ( acharya et&#xa0;al., 2022 ). iot systems dynamically adjust fertilization recommendations by integrating real-time soil-crop data, while salinity-corrected evapotranspiration (ets) models optimize irrigation strategies for saline-alkali soils ( li et&#xa0;al., 2023 ; su et&#xa0;al., 2023 ). similarly, multimodal fusion techniques enhance reference evapotranspiration (eto) prediction accuracy, facilitating precision water allocation ( jo et&#xa0;al., 2021 ; rustia et&#xa0;al., 2023 ). these breakthroughs collectively validate the robust capability of adaptive feature processing in complex agricultural environments&#x2014;a core principle that resonates with our dynamic weighting strategy for disease feature extraction. future research should therefore integrate cross-modal and cross-domain capabilities to establish multidimensional assessment systems and regional dynamic monitoring frameworks, thereby delivering comprehensive technical solutions for small-target disease control. 5 discussion and future work compared with existing methods, this study overcomes the limitation of traditional deep learning models relying on fixed convolution kernels, enabling adaptive capture of lesion features with multi-shaped convolution kernels to accurately extract edge textures of circular micro-lesions and regional contours of irregular lesions. although the constructed multi-source dataset covers diverse lighting conditions, cultivars, and lesion development stages, the homogeneous internal features of severe diseases lead to limited recall rate. meanwhile, when lesions are severely overlapped or mixed with mechanical damage, insect damage, or other types of injuries, the discriminative accuracy of the model is affected. for ultra-small lesions, the local feature information is too weak and easily overlooked, and the computational efficiency still needs optimization on some hardware platforms. furthermore, the adaptability of the current model in extreme field scenarios, such as high-density occlusion and compound damage from pests and diseases, remains to be further verified. in these scenarios, the coupling of complex interference factors exacerbates feature confusion, affecting grading reliability. to address these issues, future work will focus on the following research directions. future research will focus on enhancing the model&#x2019;s performance in complex field environments through multiple synergistic strategies. this includes constructing multi-interference factor coupled datasets that incorporating insect holes, lesions, and soil adhesion to strengthen robustness against extreme field disturbances; introducing attention-based multi-damage feature decoupling modules and dedicated micro-lesion enhancement modules combining super-resolution and feature interpolation to improve discriminability in complex scenarios and for tiny lesions;&#xa0;optimizing dynamic weight calculations via approximate computation or hardware-friendly reconstruction, while developing lightweight real-time deployment frameworks integrated with uav near-ground sensing technology to boost efficiency and practical monitoring coverage; and establishing cross-crop pathological transfer learning mechanisms, extending the model to multi-crop disease recognition tasks with self-supervised pre-training to enhance algorithm universality and low-contrast lesion feature mining capabilities. through the above research, it is expected to further improve the applicability and practicality of the model in complex field environments, promoting the development of intelligent plant disease grading technology towards more accurate, efficient, and universal directions. data availability statement the raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. author contributions yw: writing &#x2013; original draft. dz: writing &#x2013; original draft. lz:&#xa0;writing &#x2013; review &amp; editing. funding the author(s) declare financial support was received for the research and/or publication of this article. this work was supported in part by the science and technology key project of the xinjiang production and construction corps (2023ab063) development and application demonstration of green technology for online monitoring of fresh fruits and cold chain logistics in xinjiang. conflict of interest the authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. generative ai statement the author(s) declare that no generative ai was used in the creation of this manuscript. any alternative text (alt text) provided alongside figures in this article has been generated by frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. if you identify any issues, please contact us. publisher&#x2019;s note all claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. references acharya, p., burgers, t., and nguyen, k.-d. (2022). ai-enabled droplet detection and tracking for agricultural spraying systems. computers and electronics in agriculture , 202, 107325. doi:&#xa0;10.1016/j.compag.2022.107325 crossref full text | google scholar adaskaveg, j., f&#xf6;rster, h., thompson, d., driever, g., connell, j., buchner, r., et al. (2009). epidemiology and management of walnut blight. walnut res. rep , 94, 225&#x2013;256. doi:&#xa0;10.1016/j.inpa.2016.10.005 crossref full text | google scholar arivazhagan, s., shebiah, r. n., ananthi, s., and varthini, s. v. (2013). detection of unhealthy region of plant leaves and classification of plant leaf diseases using texture features. agricultural engineering international: cigr journal , 15, 211&#x2013;217. google scholar chaudhary, p., chaudhari, a. k., cheeran, a., and godara, s. (2012). color transform based approach for disease spot detection on plant leaf. international journal of computer science and telecommunications , 3, 65&#x2013;70. google scholar chen, s., zhang, k., zhao, y., sun, y., ban, w., chen, y., et al. (2021). an approach for rice bacterial leaf streak disease segmentation and disease severity estimation. agriculture , 11, 420. doi:&#xa0;10.3390/agriculture11050420 crossref full text | google scholar chiang, k.-s., bock, c., el jarroudi, m., delfosse, p., lee, i., and liu, h. (2016). effects of rater bias and assessment method on disease severity estimation with regard to hypothesis testing. phytopathology , 65, 523&#x2013;535. doi:&#xa0;10.1111/ppa.12435 crossref full text | google scholar cooke, b. (2006). &#x201c;disease assessment and yield loss,&#x201d; in the epidemiology of plant diseases (dordrecht: springer), 43&#x2013;80. google scholar ghaiwat, s. n. and arora, p. (2014). detection and classification of plant leaf diseases using image processing techniques: a review. international journal of recent advances in engineering &amp; technology , 2, 1&#x2013;7. google scholar hu, g., wang, h., zhang, y., and wan, m. (2021). detection and severity analysis of tea leaf blight based on deep learning. computers &amp; electrical engineering , 90, 107023. doi:&#xa0;10.1016/j.compeleceng.2021.107023 crossref full text | google scholar jadhav, s. b. and patil, s. b. (2016). grading of soybean leaf disease based on segmented image using k-means clustering. iaes international journal of artificial intelligence , 5, 13&#x2013;13. doi:&#xa0;10.11591/ijai.v5.i1.pp13-21 crossref full text | google scholar jo, w. j., kim, d. s., sim, h. s., ahn, s. r., lee, h. j., moon, y. h., et al. (2021). estimation of evapotranspiration and water requirements of strawberry plants in greenhouses using environmental data. frontiers in sustainable food systems , 5, 684808. doi:&#xa0;10.3389/fsufs.2021.684808 crossref full text | google scholar khan, m. a., ali, m., shah, m., mahmood, t., ahmad, m., jhanjhi, n., et al. (2021). machine learning-based detection and classification of walnut fungi diseases. intelligent automation &amp; soft computing , 30, 771&#x2013;785. doi:&#xa0;10.32604/iasc.2021.018039 crossref full text | google scholar kim, s.-k. and ahn, j.-g. (2021). tomato crop diseases classification models using deep cnn-based architectures. j korea acad-ind coop soc , 22, 7&#x2013;14. doi:&#xa0;10.5762/kais.2021.22.5.7 crossref full text | google scholar lamichhane, j. r. (2014). xanthomonas arboricola diseases of stone fruit, almond, and walnut trees: progress toward understanding and management. plant disease , 98, 1600&#x2013;1610. doi:&#xa0;10.1094/pdis-08-14-0831-fe pubmed abstract | crossref full text | google scholar li, x., zha, t., liu, p., bourque, c. p.-a., jia, x., and tian, y. (2023). interannual variation in gross ecosystem production and evapotranspiration in a temperate semiarid grassland undergoing vegetation recovery. agricultural and forest meteorology , 341, 109672. doi:&#xa0;10.1016/j.agrformet.2023.109672 crossref full text | google scholar lin, k., gong, l., huang, y., liu, c., and pan, j. (2019). deep learning-based segmentation and quantification of cucumber powdery mildew using convolutional neural network. frontiers in plant science , 10, 155. doi:&#xa0;10.3389/fpls.2019.00155 pubmed abstract | crossref full text | google scholar ma, j., du, k., zhang, l., zheng, f., chu, j., and sun, z. (2017). a segmentation method for greenhouse vegetable foliar disease spots images using color information and region growing. computers and electronics in agriculture , 142, 110&#x2013;117. doi:&#xa0;10.1016/j.compag.2017.08.023 crossref full text | google scholar mao, r., wang, z., li, f., zhou, j., chen, y., and hu, x. (2023). gseyolox-s: an improved lightweight network for identifying the severity of wheat fusarium head blight. agronomy , 13, 242. doi:&#xa0;10.3390/agronomy13010242 crossref full text | google scholar mcgranahan, g. and leslie, c. (1991). walnuts (juglans). molecules , 907&#x2013;924. doi:&#xa0;10.17660/actahortic.1991.290.20 crossref full text | google scholar mircetich, s. m., sanborn, r., and ramos, d. (1980). natural spread, graft-transmission, and possible etiology of walnut blackline disease. phytopathology , 70, 962&#x2013;968. doi:&#xa0;10.1094/phyto-70-962 crossref full text | google scholar moragrega, c., matias, j., alet&#xe0;, n., montesinos, e., and rovira, m. (2011). apical necrosis and premature drop of persian (english) walnut fruit caused by xanthomonas arboricola pv. juglandis. plant disease , 95, 1565&#x2013;1570. doi:&#xa0;10.1094/pdis-03-11-0259 pubmed abstract | crossref full text | google scholar ngugi, l. c., abdelwahab, m., and abo-zahhad, m. (2020). tomato leaf segmentation algorithms for mobile phone applications using deep learning. computers and electronics in agriculture , 178, 105788. doi:&#xa0;10.1016/j.compag.2020.105788 crossref full text | google scholar ozturk, o., sarica, b., and seker, d. z. (2025). interpretable and robust ensemble deep learning framework for tea leaf disease classification. horticulturae , 11, 437. doi:&#xa0;10.3390/horticulturae11040437 crossref full text | google scholar parashar, n., johri, p., khan, a. a., gaur, n., and kadry, s. (2024). an integrated analysis of yield prediction models: a comprehensive review of advancements and challenges. computers, materials &amp; continua , 80 (1). doi:&#xa0;10.32604/cmc.2024.050240 crossref full text | google scholar picon, a., alvarez-gila, a., seitz, m., ortiz-barredo, a., echazarra, j., and johannes, a. (2019). deep convolutional neural networks for mobile capture device-based crop disease classification in the wild. computers and electronics in agriculture , 161, 280&#x2013;290. doi:&#xa0;10.1016/j.compag.2018.04.002 crossref full text | google scholar rastogi, a., arora, r., and sharma, s. (2015). &#x201c;leaf disease detection and grading using computer vision technology &amp; fuzzy logic,&#x201d; in paper presented at the 2015 2nd international conference on signal processing and integrated networks (spin). google scholar rustia, d. j. a., lee, w.-c., lu, c.-y., wu, y.-f., shih, p.-y., and chen, s.-k. (2023). edge-based wireless imaging system for continuous monitoring of insect pests in a remote outdoor mango orchard. computers and electronics in agriculture , 211, 108019. doi:&#xa0;10.1016/j.compag.2023.108019 crossref full text | google scholar shi, t., liu, y., zheng, x., hu, k., huang, h., liu, h., et al. (2023). recent advances in plant disease severity assessment using convolutional neural networks. scientific reports , 13, 2336. doi:&#xa0;10.1038/s41598-023-29230-7 pubmed abstract | crossref full text | google scholar singh, u. p., chouhan, s. s., jain, s., and jain, s. (2019). multilayer convolution neural network for the classification of mango leaves infected by anthracnose disease. ieee access , 7, 43721&#x2013;43729. doi:&#xa0;10.1109/access.2019.2907383 crossref full text | google scholar su, y., wang, j., li, j., wang, l., wang, k., li, a., et al. (2023). spatiotemporal changes and driving factors of reference evapotranspiration and crop evapotranspiration for cotton production in china from 1960 to 2019. frontiers in environmental science , 11, 1251789. doi:&#xa0;10.3389/fenvs.2023.1251789 crossref full text | google scholar tripathi, r. (2021). &#x201c;a deep learning approach for plant material disease identification,&#x201d; in paper presented at the iop conference series: materials science and engineering. google scholar vishnoi, v. k., kumar, k., kumar, b., mohan, s., and khan, a. a. (2022). detection of apple plant diseases using leaf images through convolutional neural network . ieee access , 11, 6594&#x2013;6609. doi:&#xa0;10.1109/access.2022.3232917 crossref full text | google scholar wang, f., dun, c., tang, t., duan, y., guo, x., and you, j. (2022). boeremia exigua causes leaf spot of walnut trees (juglans regia) in china. plant disease , 106, 1993. doi:&#xa0;10.1094/pdis-10-21-2304-pdn crossref full text | google scholar wang, q.-h., fan, k., li, d.-w., han, c.-m., qu, y.-y., qi, y.-k., et al. (2020). identification, virulence and fungicide sensitivity of colletotrichum gloeosporioides ss responsible for walnut anthracnose disease in china. plant disease , 104, 1358&#x2013;1368. doi:&#xa0;10.1094/pdis-12-19-2569-re pubmed abstract | crossref full text | google scholar weber, b. c. (1980). how to diagnose black walnut damage vol. 57 (north central forest experiment station, forest service, us department of agriculture). google scholar xu, w. (2024). hcf-net: hierarchicalcontextfusion network forinfrared small object detection. arxiv . arxiv, 2403.10778. doi:&#xa0;10.1109/icme57554.2024.10687431 crossref full text | google scholar yang, c., deng, y., wang, f., yang, h., xu, x., zeng, q., et al. (2021). brown leaf spot on juglans sigillata caused by ophiognomonia leptostyla in sichuan, china. plant disease , 105, 4160. doi:&#xa0;10.1094/pdis-02-21-0344-pdn crossref full text | google scholar yang, q., duan, s., and wang, l. (2022). efficient identification of apple leaf diseases in the wild using convolutional neural networks. agronomy , 12, 2784. doi:&#xa0;10.3390/agronomy12112784 crossref full text | google scholar zarei, s., taghavi, s. m., banihashemi, z., hamzehzarghani, h., and osdaghi, e. (2019). etiology of leaf spot and fruit canker symptoms on stone fruits and nut trees in iran. journal of plant pathology , 101, 1133&#x2013;1142. doi:&#xa0;10.1007/s42161-019-00283-w crossref full text | google scholar zhang, y., li, x., &#x160;im&#x16f;nek, j., shi, h., chen, n., and hu, q. (2023). quantifying water and salt movement in a soil-plant system of a corn field using hydrus (2d/3d) and the stable isotope method. agricultural water management , 288, 108492. doi:&#xa0;10.1016/j.agwat.2023.108492 crossref full text | google scholar keywords: walnut, brown spot disease ( ophiognomonia leptostyla ), hierarchical feature selection, edge features perception, adaptive multi-scale dilated convolution, disease grading citation: wei y, zeng d and zheng l (2025) a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution. front. plant sci. 16:1641677. doi: 10.3389/fpls.2025.1641677 received: 05 june 2025; accepted: 09 september 2025; published: 03 october 2025. edited by: ravinder kumar , indian agricultural research institute (icar), india reviewed by: geza bujdoso , hungarian university of agricultural and life sciences, hungary vasudha vedula , university of texas of the permian basin, united states copyright &#xa9; 2025 wei, zeng and zheng. this is an open-access article distributed under the terms of the creative commons attribution license (cc by) . the use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. no use, distribution or reproduction is permitted which does not comply with these terms. *correspondence: liangfang zheng, mtgxnjaynta0othamtyzlmnvbq== &#x2020; these authors have contributed equally to this work disclaimer: all claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. download article download pdf readcube epub xml share on export citation endnote reference manager simple text file bibtex 1,063 total views 103 downloads citation numbers are available from dimensions view article impact view altmetric score share on edited by r k ravinder kumar reviewed by g b geza bujdoso v v vasudha vedula table of contents abstract 1 introduction 2 materials and methods 3 experiments and results analysis 4 conclusion 5 discussion and future work data availability statement author contributions funding conflict of interest generative ai statement publisher&#x2019;s note references export citation endnote reference manager simple text file bibtex check for updates frontiers&#39; impact articles published with frontiers have received 12 million total citations your research is the real superpower - learn how we maximise its impact through our leading community journals explore our impact metrics supplementary material download article download download pdf readcube epub xml guidelines author guidelines services for authors policies and publication ethics editor guidelines fee policy explore articles research topics journals how we publish outreach frontiers forum frontiers policy labs frontiers for young minds frontiers planet prize connect help center emails and alerts contact us submit career opportunities follow us © 2025 frontiers media s.a. all rights reserved privacy policy | terms and conditions [["shallowreactive",1],{"data":2,"state":4,"once":10,"_errors":11,"serverrendered":13,"path":14,"pinia":15},["shallowreactive",3],{},["reactive",5],{"$ssite-config":6},{"env":7,"name":8,"url":9},"production","frontiers articles","https://article-pages-2024.frontiersin.org/",["set"],["shallowreactive",12],{},true,"/journals/plant-science/articles/10.3389/fpls.2025.1641677/full",["reactive",16],{"main":17,"user":534,"article":535,"articlehub":760,"mainheader":764},{"ibar":18,"footer":284,"newslettercomponent":-1,"snackbaritem":366,"toggleshowsnackbar":367,"contentfuljournal":368,"graphjournal":434,"settingsfeaturesswitchers":438,"templatetogglebanner":439,"tenantconfig":499},{"tenantlogo":19,"journallogo":19,"aboutus":20,"submiturl":113,"showsubmitbutton":13,"journal":114,"sectionterm":215,"aboutjournal":216,"mainlinks":265,"journallinks":272,"helpcenterlink":281},"",[21,38,47,71,87],{"title":22,"links":23},"who we are",[24,29,32,35],{"text":25,"url":26,"target":27,"arialabel":28},"mission and values","https://www.frontiersin.org/about/mission","_self",null,{"text":30,"url":31,"target":27,"arialabel":28},"history","https://www.frontiersin.org/about/history",{"text":33,"url":34,"target":27,"arialabel":28},"leadership","https://www.frontiersin.org/about/leadership",{"text":36,"url":37,"target":27,"arialabel":28},"awards","https://www.frontiersin.org/about/awards",{"title":39,"links":40},"impact and progress",[41,44],{"text":42,"url":43,"target":27,"arialabel":28},"frontiers' impact","https://www.frontiersin.org/about/impact",{"text":45,"url":46,"target":27,"arialabel":28},"our annual reports","https://www.frontiersin.org/about/annual-reports",{"title":48,"links":49},"publishing model",[50,53,56,59,62,65,68],{"text":51,"url":52,"target":27,"arialabel":28},"how we publish","https://www.frontiersin.org/about/how-we-publish",{"text":54,"url":55,"target":27,"arialabel":28},"open access","https://www.frontiersin.org/about/open-access",{"text":57,"url":58,"target":27,"arialabel":28},"peer review","https://www.frontiersin.org/about/peer-review",{"text":60,"url":61,"target":27,"arialabel":28},"research integrity","https://www.frontiersin.org/about/research-integrity",{"text":63,"url":64,"target":27,"arialabel":28},"research topics","https://www.frontiersin.org/about/research-topics",{"text":66,"url":67,"target":27,"arialabel":28},"fair² data management","https://www.frontiersin.org/about/fair-data-management",{"text":69,"url":70,"target":27,"arialabel":28},"fee policy","https://www.frontiersin.org/about/fee-policy",{"title":72,"links":73},"services",[74,78,81,84],{"text":75,"url":76,"target":77,"arialabel":28},"societies","https://publishingpartnerships.frontiersin.org/","_blank",{"text":79,"url":80,"target":27,"arialabel":28},"national consortia","https://www.frontiersin.org/open-access-agreements/consortia",{"text":82,"url":83,"target":27,"arialabel":28},"institutional partnerships","https://www.frontiersin.org/about/open-access-agreements",{"text":85,"url":86,"target":27,"arialabel":28},"collaborators","https://www.frontiersin.org/about/collaborators",{"title":88,"links":89},"more from frontiers",[90,94,97,101,105,109],{"text":91,"url":92,"target":77,"arialabel":93},"frontiers forum","https://forum.frontiersin.org/","this link will take you to the frontiers forum website",{"text":95,"url":96,"target":27,"arialabel":28},"frontiers planet prize","https://www.frontiersin.org/about/frontiers-planet-prize",{"text":98,"url":99,"target":77,"arialabel":100},"press office","https://pressoffice.frontiersin.org/","this link will take you to the frontiers press office website",{"text":102,"url":103,"target":27,"arialabel":104},"sustainability","https://www.frontiersin.org/about/sustainability","link to information about frontiers' sustainability",{"text":106,"url":107,"target":77,"arialabel":108},"career opportunities","https://careers.frontiersin.org/","this link will take you to the frontiers careers website",{"text":110,"url":111,"target":27,"arialabel":112},"contact us","https://www.frontiersin.org/about/contact","this link will take you to the help pages to contact our support team","https://www.frontiersin.org/submission/submit?domainid=1&fieldid=66&specialtyid=0&entitytype=2&entityid=373",{"id":115,"name":116,"slug":117,"sections":118},373,"frontiers in plant science","plant-science",[119,123,127,131,135,139,143,147,151,155,159,163,167,171,175,179,183,187,191,195,199,203,207,211],{"id":120,"name":121,"slug":122},1553,"aquatic photosynthetic organisms","aquatic-photosynthetic-organisms",{"id":124,"name":125,"slug":126},1356,"crop and product physiology","crop-and-product-physiology",{"id":128,"name":129,"slug":130},467,"functional plant ecology","functional-plant-ecology",{"id":132,"name":133,"slug":134},2844,"functional and applied plant genomics","functional-and-applied-plant-genomics",{"id":136,"name":137,"slug":138},2843,"photosynthesis and photobiology","photosynthesis-and-photobiology",{"id":140,"name":141,"slug":142},1312,"plant abiotic stress","plant-abiotic-stress",{"id":144,"name":145,"slug":146},2183,"plant bioinformatics","plant-bioinformatics",{"id":148,"name":149,"slug":150},589,"plant biophysics and modeling","plant-biophysics-and-modeling",{"id":152,"name":153,"slug":154},560,"plant biotechnology","plant-biotechnology",{"id":156,"name":157,"slug":158},468,"plant breeding","plant-breeding",{"id":160,"name":161,"slug":162},577,"plant cell biology","plant-cell-biology",{"id":164,"name":165,"slug":166},474,"plant development and evodevo","plant-development-and-evodevo",{"id":168,"name":169,"slug":170},2845,"plant genetics, epigenetics and chromosome biology","plant-genetics-epigenetics-and-chromosome-biology",{"id":172,"name":173,"slug":174},479,"plant membrane traffic and transport","plant-membrane-traffic-and-transport",{"id":176,"name":177,"slug":178},481,"plant metabolism and chemodiversity","plant-metabolism-and-chemodiversity",{"id":180,"name":181,"slug":182},486,"plant nutrition","plant-nutrition",{"id":184,"name":185,"slug":186},485,"plant pathogen interactions","plant-pathogen-interactions",{"id":188,"name":189,"slug":190},226,"plant physiology","plant-physiology",{"id":192,"name":193,"slug":194},580,"plant proteomics and protein structural biology","plant-proteomics-and-protein-structural-biology",{"id":196,"name":197,"slug":198},1570,"plant symbiotic interactions","plant-symbiotic-interactions",{"id":200,"name":201,"slug":202},1428,"plant systematics and evolution","plant-systematics-and-evolution",{"id":204,"name":205,"slug":206},483,"plant systems and synthetic biology","plant-systems-and-synthetic-biology",{"id":208,"name":209,"slug":210},2277,"sustainable and intelligent phytoprotection","sustainable-and-intelligent-phytoprotection",{"id":212,"name":213,"slug":214},484,"technical advances in plant science","technical-advances-in-plant-science","sections",[217,241],{"title":218,"links":219},"scope",[220,223,226,229,232,235,238],{"text":221,"url":222,"target":27,"arialabel":28},"field chief editors","https://www.frontiersin.org/journals/plant-science/about#about-editors",{"text":224,"url":225,"target":27,"arialabel":28},"mission & scope","https://www.frontiersin.org/journals/plant-science/about#about-scope",{"text":227,"url":228,"target":27,"arialabel":28},"facts","https://www.frontiersin.org/journals/plant-science/about#about-facts",{"text":230,"url":231,"target":27,"arialabel":28},"journal sections","https://www.frontiersin.org/journals/plant-science/about#about-submission",{"text":233,"url":234,"target":27,"arialabel":28},"open access statement","https://www.frontiersin.org/journals/plant-science/about#about-open",{"text":236,"url":237,"target":27,"arialabel":28},"copyright statement","https://www.frontiersin.org/journals/plant-science/about#copyright-statement",{"text":239,"url":240,"target":27,"arialabel":28},"quality","https://www.frontiersin.org/journals/plant-science/about#about-quality",{"title":242,"links":243},"for authors",[244,247,250,253,256,259,262],{"text":245,"url":246,"target":27,"arialabel":28},"why submit?","https://www.frontiersin.org/journals/plant-science/for-authors/why-submit",{"text":248,"url":249,"target":27,"arialabel":28},"article types","https://www.frontiersin.org/journals/plant-science/for-authors/article-types",{"text":251,"url":252,"target":27,"arialabel":28},"author guidelines","https://www.frontiersin.org/journals/plant-science/for-authors/author-guidelines",{"text":254,"url":255,"target":27,"arialabel":28},"editor guidelines","https://www.frontiersin.org/journals/plant-science/for-authors/editor-guidelines",{"text":257,"url":258,"target":27,"arialabel":28},"publishing fees","https://www.frontiersin.org/journals/plant-science/for-authors/publishing-fees",{"text":260,"url":261,"target":27,"arialabel":28},"submission checklist","https://www.frontiersin.org/journals/plant-science/for-authors/submission-checklist",{"text":263,"url":264,"target":27,"arialabel":28},"contact editorial office","https://www.frontiersin.org/journals/plant-science/for-authors/contact-editorial-office",[266,269],{"text":267,"url":268,"target":27,"arialabel":28},"all journals","https://www.frontiersin.org/journals",{"text":270,"url":271,"target":27,"arialabel":28},"all articles","https://www.frontiersin.org/articles",[273,276,278],{"text":274,"url":275,"target":27,"arialabel":28},"articles","articles",{"text":63,"url":277,"target":27,"arialabel":28},"research-topics",{"text":279,"url":280,"target":27,"arialabel":28},"editorial board","editors",{"text":282,"url":283,"target":77,"arialabel":282},"help center","https://helpcenter.frontiersin.org",{"blocks":285,"sociallinks":339,"copyright":363,"termsandconditionsurl":364,"privacypolicyurl":365},[286,300,310,324],{"title":287,"links":288},"guidelines",[289,291,294,297,299],{"text":251,"url":290,"target":27,"arialabel":28},"https://www.frontiersin.org/guidelines/author-guidelines",{"text":292,"url":293,"target":27,"arialabel":28},"services for authors","https://www.frontiersin.org/for-authors/author-services",{"text":295,"url":296,"target":27,"arialabel":28},"policies and publication ethics","https://www.frontiersin.org/guidelines/policies-and-publication-ethics",{"text":254,"url":298,"target":27,"arialabel":28},"https://www.frontiersin.org/guidelines/editor-guidelines",{"text":69,"url":70,"target":27,"arialabel":28},{"title":301,"links":302},"explore",[303,304,307,309],{"text":274,"url":271,"target":27,"arialabel":28},{"text":305,"url":306,"target":27,"arialabel":28},"research topics ","https://www.frontiersin.org/research-topics",{"text":308,"url":268,"target":27,"arialabel":28},"journals",{"text":51,"url":52,"target":27,"arialabel":28},{"title":311,"links":312},"outreach",[313,316,319,323],{"text":314,"url":92,"target":77,"arialabel":315},"frontiers forum ","frontiers forum website",{"text":317,"url":318,"target":77,"arialabel":28},"frontiers policy labs ","https://policylabs.frontiersin.org/",{"text":320,"url":321,"target":77,"arialabel":322},"frontiers for young minds","https://kids.frontiersin.org/","frontiers for young minds journal",{"text":95,"url":96,"target":27,"arialabel":28},{"title":325,"links":326},"connect",[327,328,332,335,338],{"text":282,"url":283,"target":77,"arialabel":282},{"text":329,"url":330,"target":77,"arialabel":331},"emails and alerts ","https://subscription-management.frontiersin.org/emails/preferences#block0","subscribe to frontiers emails",{"text":333,"url":111,"target":27,"arialabel":334},"contact us ","subscribe to newsletter",{"text":336,"url":337,"target":27,"arialabel":28},"submit","https://www.frontiersin.org/submission/submit",{"text":106,"url":107,"target":77,"arialabel":28},[340,348,353,358],{"link":341,"type":344,"color":345,"icon":346,"size":347,"hiddentext":13},{"text":342,"url":343,"target":77,"arialabel":342},"frontiers facebook","https://www.facebook.com/frontiersin","link","grey","facebook","medium",{"link":349,"type":344,"color":345,"icon":352,"size":347,"hiddentext":13},{"text":350,"url":351,"target":77,"arialabel":28},"frontiers twitter","https://twitter.com/frontiersin","twitter",{"link":354,"type":344,"color":345,"icon":357,"size":347,"hiddentext":13},{"text":355,"url":356,"target":77,"arialabel":28},"frontiers linkedin","https://www.linkedin.com/company/frontiers","linkedin",{"link":359,"type":344,"color":345,"icon":362,"size":347,"hiddentext":13},{"text":360,"url":361,"target":77,"arialabel":28},"frontiers instagram","https://www.instagram.com/frontiersin_","instagram","frontiers media s.a. all rights reserved","https://www.frontiersin.org/legal/terms-and-conditions","https://www.frontiersin.org/legal/privacy-policy",{},false,{"__typename":369,"identifier":115,"name":116,"slug":117,"banner":370,"description":427,"mission":428,"palette":429,"impactfactor":430,"citescore":431,"citations":432,"showtagline":28,"twitter":433},"journal",[371],{"id":372,"src":373,"name":374,"tags":375,"type":385,"width":386,"height":387,"idhash":388,"archive":389,"brandid":390,"limited":389,"filesize":391,"ispublic":392,"original":393,"copyright":394,"extension":395,"thumbnails":397,"datecreated":405,"description":406,"orientation":407,"usercreated":408,"watermarked":389,"datemodified":405,"datepublished":409,"ecsarchivefiles":410,"propertyoptions":411,"property_channel":416,"property_sub-type":418,"property_asset_type":420,"activeoriginalfocuspoint":422,"property_office_department":425},"450e9326-0272-405c-b8d614c72bed9f89","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/webimage-00a7f7fd-61fc-4329-b3bfcc0119b4b276.jpg","fpls_main visual_green_website",[376,377,378,379,380,381,382,383,384],"medical","vitality","decoration","five","cosmetic","r","cure","aloe vera","spike","image",4928,3264,"720507a162925515",0,"22c10171-81b3-4da6-99342f272a32e8bb",11359471,1,"https://brand.frontiersin.org/m/720507a162925515/original/fpls_main-visual_green_website.jpeg","copyright (c) 2017 sabine hortebusch/shutterstock. no use without permission.",[396],"jpeg",{"mini":398,"thul":399,"webimage":373,"guidelines":400,"websitejpg_xl":401,"websitewebp_l":402,"websitewebp_m":403,"websitewebp_xl":404},"https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/mini-006feee0-2d70-4630-b9abfb0a0fa410aa.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/thul-c817db1b-00e8-4b29-b71c4e5a6058947f.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/guidelines-fpls_main visual_green_website.png","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitejpg_xl-fpls_main visual_green_website.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_l-fpls_main visual_green_website.webp","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_m-fpls_main visual_green_website.webp","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_xl-fpls_main visual_green_website.webp","2022-06-27t10:00:48z","spiral aloe vera with water drops, closeup","landscape","caroline sutter","2022-06-27t09:27:09z",[],[412,413,414,415],"414fb2d4-2283-43fd-be14e534eca67928","6c18119b-14bd-4951-b437696f4357bd33","7c692885-db25-4858-b1fb4ff47b241e9b","d88c0047-ec30-4506-a7df28a4d765e1cf",[417],"frontiersin_org",[419],"main_visual",[421],"photography",{"x":423,"y":424},2464,1632,[426],"publishing","the most cited plant science journal, advancing our understanding of plant biology for sustainable food security, functional ecosystems and human health.","\u003cp>frontiers in plant science is a leading, multidisciplinary journal that seeks to advance our understanding of fundamental processes in plant biology.\u003c/p>\n\n\u003cp>led by field chief editor prof. chun-ming liu (institute of botany, chinese academy of sciences) and indexed in pubmed, pubmed central, and scopus, among others, the journal seeks original and significant contributions that cultivate plant biology and its applications. the journal has the long-term goal of supporting sustainable development, food security, functional ecosystems, biotechnology (including biofuels and biomaterials), and human health.\u003c/p>\n\u003cp>frontiers in plant science welcomes original research, review, opinion, and perspective articles, among other submission types, covering the journal’s specialty sections:\u003c/p>\n\n\u003cdiv> &bull; aquatic photosynthetic organisms\u003c/div>\n\u003cdiv> &bull; crop and product physiology\u003c/div>\n\u003cdiv> &bull; functional plant ecology\u003c/div>\n\u003cdiv> &bull; functional and applied plant genomics\u003c/div>\n\u003cdiv> &bull; photosynthesis and photobiology\u003c/div>\n\u003cdiv> &bull; plant abiotic stress\u003c/div>\n\u003cdiv> &bull; plant bioinformatics\u003c/div>\n\u003cdiv> &bull; plant biophysics and modeling\u003c/div>\n\u003cdiv> &bull; plant biotechnology\u003c/div>\n\u003cdiv> &bull; plant breeding\u003c/div>\n\u003cdiv> &bull; plant cell biology\u003c/div>\n\u003cdiv> &bull; plant development and evodevo\u003c/div>\n\u003cdiv> &bull; plant genetics, epigenetics and chromosome biology\u003c/div>\n\u003cdiv> &bull; plant membrane traffic and transport\u003c/div>\n\u003cdiv> &bull; plant metabolism and chemodiversity\u003c/div>\n\u003cdiv> &bull; plant nutrition\u003c/div>\n\u003cdiv> &bull; plant pathogen interactions\u003c/div>\n\u003cdiv> &bull; plant physiology\u003c/div>\n\u003cdiv> &bull; plant proteomics and protein structural biology\u003c/div>\n\u003cdiv> &bull; plant symbiotic interactions\u003c/div>\n\u003cdiv> &bull; plant systematics and evolution\u003c/div>\n\u003cdiv> &bull; plant systems and synthetic biology\u003c/div>\n\u003cdiv> &bull; sustainable and intelligent phytoprotection\u003c/div>\n\u003cdiv> &bull; technical advances in plant science.\u003c/div>\n\u003cbr>\n\u003cp>furthermore, the journal welcomes submissions that support and advance the un's sustainable development goals (sdgs), notably sdg 13: climate action and sdg 15: life on land.\u003c/p> \n\n\u003cp>frontiers in plant science is committed to advancing developments in the field of plant biology by allowing unrestricted access to articles and communicating scientific knowledge to researchers and the public alike, to enable the scientific breakthroughs of the future.\u003c/p> \n \n\u003cp>requirements\u003cp>\n\u003cp>manuscripts that focus on non-plant-related microbiology, human or animal genetics, and medical and pharmacological research are not suitable for publication in this journal. pure field agriculture studies such as those focusing on fertilizer application or yield optimization, without relevance to plant science, are also not within the scope of this journal.\u003c/p>\nstudies falling into the categories below will not be considered for review in this journal unless they are expanded and provide insight into the biological process being studied:\u003c/p>\n\n\u003cp>i) descriptive collections of transcripts, proteins, or metabolites, including comparative sets as a result of different conditions or treatments;\u003c/p>\n\u003cp>ii) descriptive studies that define gene families using pure phylogenetics and the assignment of cursory functional attributions (e.g. expression profiles, promoter analysis, and bioinformatic parameters).\u003c/p>\n\n\u003cp>quantitative analysis needs to be performed on a minimum of three biological replicates in order to enable an assessment of significance. this includes quantitative omics studies (transcriptomics, proteomics, metabolomics) as well as phenotypic measurements, quantitative assays, and qpcr expression analysis. studies that do not comply with these replication requirements will not be considered for review.\u003cp>\n\n\u003cp>studies using transgenic or mutant lines (plants and algae), for example, t-dna, transposon, rnai, crispr/cas9, chemically induced, overexpressors and reporter fusions (gus, gfps, luc), should be based on data from multiple alleles (minimum of two) displaying a common and stable phenotype. qualitative data can be presented from a single allele but should be indicative of observations from multiple alleles which should be explicitly stated in the text. quantitative data should be derived from multiple alleles (at least two) and should be displayed separately for each allele (with at least three independent replications for each allele). studies reporting single alleles may be considered acceptable when:\u003c/p>\n\n\u003cp>i) complementation via transformation is used for confirmation;\u003c/p> \n\u003cp>ii) the allele has been previously characterized and published, and is representative of multiple independent lines;\u003c/p>\n\u003cp>iii) in situations where genetic transformation is difficult or not yet possible, alternative evidence should be presented\u003c/p>\n\n","green","5.6","7.1","927148","@frontplantsci",{"id":115,"name":116,"slug":117,"abbreviation":435,"isonline":13,"isopenforsubmissions":13,"citescore":436,"impactfactor":437},"fpls",8.8,4.8,{"displaytitlepilllabels":13,"displayrelatedarticlesbox":13,"showeditors":13,"showreviewers":13,"showloopimpactlink":13,"enablefigshare":367,"usexmlimages":13},{"ispublic":13,"allowcompanyusers":367,"whitelistemails":440,"enablealljournals":13,"whitelistjournals":462},[441,442,443,444,445,446,447,448,449,446,450,451,452,453,454,455,456,457,458,459,460,461],"y2fybg9zlmxvcmnhqgzyb250awvyc2lulm9yzw==","zgfuawvslmx1ekbmcm9udgllcnnpbi5vcmc=","bgf1cmeudgvuyubmcm9udgllcnnpbi5vcmc=","cmfmywvslnjpyw5jag9aznjvbnrpzxjzaw4ub3jn","amftzxmuc21hbgxib25lqgzyb250awvyc2lulm9yzw==","znjhbi5tb3jlbm9aznjvbnrpzxjzaw4ub3jn","c2ftdwvslmzlcm5hbmrlekbmcm9udgllcnnpbi5vcmc=","ymfyymfyys5jyxjjyw5naxvaznjvbnrpzxjzaw4ub3jn","axzhbi5zyw50yw1hcmlhqgzyb250awvyc2lulm9yzw==","ahvllnryyw5aznjvbnrpzxjzaw4ub3jn","z29uy2fsby52yxjnyxnaznjvbnrpzxjzaw4ub3jn","bhvjawuuc2vubkbmcm9udgllcnnpbi5vcmc=","bg9ybi5mcmfzzxjaznjvbnrpzxjzaw4ub3jn","zwxzys5jyxjyb25aznjvbnrpzxjzaw4ub3jn","bwfhcnrlbi52yw5kawpja0bmcm9udgllcnnpbi5vcmc=","ymluzgh1lmtyaxnobmfuqgzyb250awvyc2lulm9yzw==","bwf0dghldy5hdhr3yxrlcnnaznjvbnrpzxjzaw4ub3jn","z2l1bglhlnzhbhnly2noaubmcm9udgllcnnpbi5vcmc=","zmfiawfulmrlbgxhbw9ydgvaznjvbnrpzxjzaw4ub3jn","dmlqyxlhbi5wcebwaxrzb2x1dglvbnmuy29t","z3vpbgxhdw1llnnldxjhdebmcm9udgllcnnpbi5vcmc=",[463,464,465,466,467,468,392,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498],2232,1729,2357,2456,2176,2333,1843,602,106,616,310,657,3123,1468,2137,628,1566,2726,2086,1490,451,141,1335,2655,1238,604,698,1547,176,1440,403,1239,755,2136,609,1534,{"spaceid":392,"name":392,"availablejournalpages":500,"announcement":504},[275,280,277,501,502,503],"volumes","about","community-reviewers",{"__typename":505,"sys":506,"preheader":42,"title":508,"description":509,"image":510,"link":532},"announcement",{"id":507},"5ittnttqgazppgh6rx0alm","articles published with frontiers have received 12 million total citations","your research is the real superpower - learn how we maximise its impact through our leading community journals",[511],{"archive":389,"brandid":390,"copyright":28,"datecreated":512,"datemodified":513,"datepublished":514,"description":28,"extension":515,"filesize":517,"height":518,"id":519,"ispublic":389,"limited":389,"name":520,"orientation":407,"original":28,"thumbnails":521,"type":385,"watermarked":389,"width":528,"videopreviewurls":529,"tags":530,"textmetaproperties":531,"src":522},"2025-06-18t12:58:01z","2025-06-18t14:15:34z","2025-06-18t12:57:32z",[516],"png",1365026,920,"7c5269c4-3c83-4591-951a74d15b95daea","impact metrics banner for article pages",{"webimage":522,"thul":523,"mini":524,"websitewebp_l":525,"websitewebp_m":526,"guidelines":527},"https://brand.frontiersin.org/m/59ee6275cb9a849c/webimage-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/m/59ee6275cb9a849c/thul-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/m/59ee6275cb9a849c/mini-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/websitewebp_l/impact-metrics-banner-for-article-pages.webp","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/websitewebp_m/impact-metrics-banner-for-article-pages.webp","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/guidelines/impact-metrics-banner-for-article-pages.png",1600,[],[],[],{"text":533,"url":43,"target":27,"arialabel":28},"explore our impact metrics",{"loggeduserinfo":-1},{"currentarticle":536,"ispreviewpage":367,"hassupplementaldata":367,"showcrossmarkwidget":13,"articletemplate":648,"currentarticlepagemetainfo":649,"xmlparsedarticlecontent":-1,"xmlparsingerror":-1},{"id":537,"doi":538,"title":539,"acceptancedate":540,"receptiondate":541,"publicationdate":542,"lastmodifieddate":543,"ispublished":13,"abstract":544,"researchtopic":545,"articletype":551,"stage":554,"keywords":556,"authors":563,"editors":587,"reviewers":595,"journal":610,"section":617,"impactmetrics":619,"volume":622,"articlevolume":623,"relatedarticles":624,"ispublishedv2":13,"contents":625,"files":628},1641677,"10.3389/fpls.2025.1641677","a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution","2025-09-09t14:48:37.000z","2025-06-05t09:39:42.000z","2025-10-03t00:00:00.000z","2025-10-21t02:25:15.907z","walnut leaf brown spot disease, caused by ophiognomonia leptostyla, is among the most destructive fungal diseases in walnut cultivation. in the development of smart agriculture, precision grading of plant diseases remains a core technical challenge; specifically, this disease is plagued by blurred lesion edges and inefficient extraction of complex features, which directly limits the accurate grading of the disease. to address these issues, this study proposes a disease grading method integrating hierarchical feature selection and adaptive multi-scale dilated convolution, and develops the cogfuse-mobilevit model. this model overcomes the limitations of the standard mobilevitv3 model in capturing blurred edges of tiny lesions via three innovative modules: specifically, the hierarchical feature screening module (hfsm) enables hierarchical screening of disease-related features; the edge feature focus module (ecfm) works in synergy with the hfsm to enhance the focus on lesion edge features; and the adaptive multi-scale dilated convolution fusion module (amsdidcm) achieves dynamic multi-scale fusion of lesion textures and global structures. experimental results demonstrate that the proposed model achieves an accuracy of 86.61% on the test set, representing an improvement of 7.8 percentage points compared with the original mobilevitv3 model and significantly outperforming other mainstream disease grading models. this study confirms that the cogfuse-mobilevit model can effectively resolve the issues of blurred edges and inefficient feature extraction in this disease, provides a reliable technical solution for its precision grading, and holds practical application value for the intelligent diagnosis of plant diseases in smart agriculture.",{"id":546,"title":547,"articlescount":548,"ismagazinepage":367,"slug":549,"isopenforsubmission":13,"views":550},66487,"innovative field diagnostics for real-time plant pathogen detection and management",9,"innovative-field-diagnostics-for-real-time-plant-pathogen-detection-and-management",14940,{"id":552,"name":553},24,"original research",{"id":555,"name":19},18,[557,558,559,560,561,562],"walnut","brown spot disease (ophiognomonia leptostyla)","hierarchical feature selection","edge features perception","adaptive multi-scale dilated convolution","disease grading",[564,575,581],{"id":565,"firstname":566,"middlename":19,"lastname":567,"givennames":568,"iscorresponding":367,"isprofilepublic":13,"userid":565,"email":-1,"affiliations":569},3089871,"yuting","wei","yuting ",[570,573],{"organizationname":571,"countryname":572,"cityname":19,"statename":19,"zipcode":19},"college of information engineering, tarim university","china",{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},"key laboratory of tarim oasis agriculture, ministry of education, tarim university",{"id":389,"firstname":576,"middlename":19,"lastname":577,"givennames":578,"iscorresponding":367,"isprofilepublic":367,"userid":389,"email":-1,"affiliations":579},"debin","zeng","debin ",[580],{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},{"id":389,"firstname":582,"middlename":19,"lastname":583,"givennames":584,"iscorresponding":13,"isprofilepublic":367,"userid":389,"email":19,"affiliations":585},"liangfang","zheng","liangfang ",[586],{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},[588],{"id":589,"firstname":590,"middlename":19,"lastname":591,"givennames":592,"iscorresponding":367,"isprofilepublic":13,"userid":589,"email":-1,"affiliations":593},1037951,"ravinder","kumar","ravinder ",[594],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},[596,603],{"id":597,"firstname":598,"middlename":19,"lastname":599,"givennames":600,"iscorresponding":367,"isprofilepublic":13,"userid":597,"email":-1,"affiliations":601},2082015,"geza","bujdoso","geza ",[602],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},{"id":604,"firstname":605,"middlename":19,"lastname":606,"givennames":607,"iscorresponding":367,"isprofilepublic":13,"userid":604,"email":-1,"affiliations":608},3078480,"vasudha","vedula","vasudha ",[609],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},{"id":115,"slug":117,"name":116,"shortname":611,"electronicissn":612,"field":613,"specialtyid":28,"journalsectionpaths":615},"front. plant sci.","1664-462x",{"id":614,"domainid":392},66,[616],{"section":617},{"id":208,"name":209,"slug":210,"specialtyid":618},2685,{"views":620,"downloads":621,"citations":389},1063,103,16,"volume 16 - 2025",[],{"titlehtml":539,"fulltexthtml":626,"menuhtml":627},"\u003cdiv class=\"journalabstract\">\u003ca id=\"h1\" name=\"h1\">\u003c/a>\u003cdiv class=\"authors\">\u003cspan class=\"author-wrapper notranslate\">\u003ca href=\"https://loop.frontiersin.org/people/3089871/overview\" class=\"user-id-3089871/overview\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/images/profile/3089871/overview/74\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\" alt=\"yuting wei,&#x;\">yuting wei\u003c/a>\u003csup>1,2&#x2020;\u003c/sup>\u003c/span>\u003cspan class=\"author-wrapper notranslate\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/cdn/images/profile/default_32.jpg\" alt=\"debin zeng&#x;\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\">debin zeng\u003csup>2&#x2020;\u003c/sup>\u003c/span>\u003cspan class=\"author-wrapper notranslate\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/cdn/images/profile/default_32.jpg\" alt=\"liangfang zheng*\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\">liangfang zheng\u003csup>2*\u003c/sup>\u003c/span>\u003c/div>\u003cul class=\"notes\">\u003cli>\u003cspan>\u003csup>1\u003c/sup>\u003c/span>college of information engineering, tarim university, alaer, china\u003c/li>\u003cli>\u003cspan>\u003csup>2\u003c/sup>\u003c/span>key laboratory of tarim oasis agriculture, ministry of education, tarim university, alaer, china\u003c/li>\u003c/ul>\u003cp>walnut leaf brown spot disease, caused by \u003ci>ophiognomonia leptostyla\u003c/i>, is among the most destructive fungal diseases in walnut cultivation. in the development of smart agriculture, precision grading of plant diseases remains a core technical challenge; specifically, this disease is plagued by blurred lesion edges and inefficient extraction of complex features, which directly limits the accurate grading of the disease. to address these issues, this study proposes a disease grading method integrating hierarchical feature selection and adaptive multi-scale dilated convolution, and develops the cogfuse-mobilevit model. this model overcomes the limitations of the standard mobilevitv3 model in capturing blurred edges of tiny lesions via three innovative modules: specifically, the hierarchical feature screening module (hfsm) enables hierarchical screening of disease-related features; the edge feature focus module (ecfm) works in synergy with the hfsm to enhance the focus on lesion edge features; and the adaptive multi-scale dilated convolution fusion module (amsdidcm) achieves dynamic multi-scale fusion of lesion textures and global structures. experimental results demonstrate that the proposed model achieves an accuracy of 86.61% on the test set, representing an improvement of 7.8 percentage points compared with the original mobilevitv3 model and significantly outperforming other mainstream disease grading models. this study confirms that the cogfuse-mobilevit model can effectively resolve the issues of blurred edges and inefficient feature extraction in this disease, provides a reliable technical solution for its precision grading, and holds practical application value for the intelligent diagnosis of plant diseases in smart agriculture.\u003c/p>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"journalfulltext\">\u003ca id=\"h2\" name=\"h2\">\u003c/a>\u003ch2>1 introduction\u003c/h2>\u003cp class=\"mb15\">walnut diseases pose a serious threat to walnut production in xinjiang. the spread of crop diseases significantly exacerbates the security risks of the walnut industry if timely prevention and control measures are not taken (\u003ca href=\"#b7\">cooke, 2006\u003c/a>; \u003ca href=\"#b12\">khan et&#xa0;al., 2021\u003c/a>). as a core component of the disease prevention and control system, early precise grading plays a critical role in agricultural production management (\u003ca href=\"#b2\">adaskaveg et&#xa0;al., 2009\u003c/a>; \u003ca href=\"#b6\">chiang et&#xa0;al., 2016\u003c/a>; \u003ca href=\"#b14\">lamichhane, 2014\u003c/a>). walnut brown spot, caused primarily by the fungus \u003ci>ophiognomonia leptostyla\u003c/i>, typically forms specific symptoms on organs such as leaves, flowers, and fruits. among them, leaves, as the primary carrier of plant diseases, exhibit characteristics such as lesion morphology and color changes on their surfaces, which often serve as important bases for disease grading (\u003ca href=\"#b8\">ghaiwat and arora, 2014\u003c/a>; \u003ca href=\"#b35\">weber, 1980\u003c/a>; \u003ca href=\"#b39\">zarei et&#xa0;al., 2019\u003c/a>). traditional disease identification relies on manual experience (\u003ca href=\"#b21\">moragrega et&#xa0;al., 2011\u003c/a>; \u003ca href=\"#b34\">wang et&#xa0;al., 2020\u003c/a>), a method that is not only time- color system to reduce interference from light and leaf veins, and lesion edge detection using the sobel operator, ultimately achieving fast and accurate grading based on the ratio of lesion area to leaf area. \u003ca href=\"#b10\">jadhav and patil (2016)\u003c/a> developed a leaf image partitioning technique based on k-means clustering and squared euclidean distance, automatically quantifying damaged leaf area through the pixel ratio of lesions to leaves for disease grading. \u003ca href=\"#b3\">arivazhagan et al. (2013)\u003c/a> proposed a four-step processing system involving color conversion, green pixel removal, segmentation, and texture feature classification for leaf disease grading. however, traditional methods for acquiring disease information suffer from insufficient segmentation accuracy for plant leaf disease images with complex textures and unclear lesion boundaries, thereby resulting in poor disease severity grading performance.\u003c/p>\u003cp class=\"mb15\">with the continuous advancement of computational power, deep learning has increasingly become a key technology for addressing complex lesion segmentation, primarily due to its superior capability in automatic feature extraction (\u003ca href=\"#b18\">mao et&#xa0;al., 2023\u003c/a>). \u003ca href=\"#b5\">chen et al. (2021)\u003c/a> proposed the blsnet method based on the unet semantic segmentation network, enhancing lesion segmentation accuracy through the introduction of attention mechanisms and multi-scale feature fusion. experiments showed that its segmentation and classification accuracy outperformed benchmark models such as deeplabv3+ and unet, preliminarily verifying the reliability of this method in automatic assessment of bls disease severity. \u003ca href=\"#b22\">ngugi et al. (2020)\u003c/a> developed the kijaninet segmentation network based on fully convolutional neural networks, demonstrating excellent performance in tomato leaf segmentation under complex backgrounds; \u003ca href=\"#b16\">lin et al. (2019)\u003c/a> developed a cnn semantic segmentation model that achieved high-precision pixel-level segmentation of cucumber powdery mildew on leaves. additionally, some studies have fused traditional features with deep learning: \u003ca href=\"#b31\">tripathi (2021)\u003c/a> proposed a convolutional neural network method based on alexnet, achieving disease grading by fusing features extracted by the model with external leaf segmentation features; \u003ca href=\"#b17\">ma et al. (2017)\u003c/a> introduced comprehensive color features (ccf) combining hyper-red index, hsv/h and lab/b components, and achieved lesion segmentation via interactive region growing, with experiments confirming that this method enables accurate grading of disease images such as cucumber downy mildew under actual field conditions. however, although such methods have improved grading accuracy to some extent, they still suffer from insufficient feature capture of blurred disease edges, making it difficult to achieve fine-grained differentiation of subtle differences between disease grades (\u003ca href=\"#b26\">rastogi et&#xa0;al., 2015\u003c/a>).\u003c/p>\u003cp class=\"mb15\">in recent years, the application of deep learning in plant disease classification has continued to expand. \u003ca href=\"#b24\">parashar et al. (2024)\u003c/a> systematically validated the capability of complex feature modeling for crop yield prediction, further substantiating the critical role of adaptive feature extraction in agricultural intelligent decision-making. concurrently, \u003ca href=\"#b32\">vishnoi et al. (2022)\u003c/a> enhanced small-target detection precision in apple disease identification through a spatial attention mechanism-based cnn architecture for leaf disease diagnosis. \u003ca href=\"#b23\">ozturk et al. (2025)\u003c/a> constructed an ensemble learning classification model based on resnet50, mobile net, efficientnetb0, and densenet121, enhancing generalization performance through statistical cross-validation and improving decision interpretability via grad-cam visualization. experimental results showed that the ensemble model achieved stable high-precision classification performance across multiple validation rounds. these studies provide robust and interpretable solutions for intelligent plant disease recognition through technical integration and architectural innovation. \u003ca href=\"#b9\">hu et al. (2021)\u003c/a> integrated retinex enhancement, faster r-cnn detection, and vgg16 classification to improve the grading and detection accuracy of blurred diseased leaves. \u003ca href=\"#b25\">picon et al. (2019)\u003c/a> proposed an adaptive deep residual neural network algorithm for the classification of three european wheat diseases (septoria leaf blotch, brown spot, and rust) in real-world scenarios, which effectively improved the classification accuracy of wheat diseases. \u003ca href=\"#b13\">kim and ahn (2021)\u003c/a> employed deep cnn architectures such as resnet, xception, and densenet, combined with transfer learning and fine-tuning, to classify 9 categories of pests, diseases, and healthy states in tomatoes. although densenet combined with the rmsprop algorithm achieved an accuracy of 98.63%, single cnn architectures have clear bottlenecks in handling complex plant lesions and overlapping multiple diseases, including insufficient specificity in feature extraction and limited ability to distinguish subtle differences between lesions. \u003ca href=\"#b28\">shi et al. (2023)\u003c/a> analyzed the application of cnns in evaluating the severity of plant diseases, pointing out that hybrid architectures such as classical cnns, improved cnns, and segmentation networks have limitations in handling complex plant lesions: classification errors caused by concurrent multiple diseases, as well as constraints on model generalization ability due to unbalanced datasets and insufficient annotation accuracy. these issues significantly restrict their precise application in practical agricultural scenarios.\u003c/p>\u003cp class=\"mb15\">although significant advancements have been made in plant lesion segmentation and disease classification using deep learning, the grading task for walnut leaf brown spot disease&#x2014;characterized by blurred edge representation and complex small lesions&#x2014;still faces notable challenges. current mobilevit-based hybrid models fall into two categories: general architectures (mobilevitv1/v2/v3) focus on optimizing the cnn-transformer balance for natural images; domain-improved models (mobile-former/edgevit) are oriented toward medical/industrial tasks, with their task focus on localization rather than grading, which mismatches the pain points and fails to address the blurriness of agricultural lesions, tissue interference, and morphological variations. thus, the present study proposes a novel cogfuse-mobilevit model specifically designed for disease grading, with its specific contributions as follows:\u003c/p>\u003cp style=\"margin-top:1em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">1. a diverse field-collected walnut leaf dataset was constructed, with images divided into four distinct severity levels based on qualitative assessment of disease severity, ensuring the accuracy of disease severity grading.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">2. the hierarchical feature selection module (hfsm) enhances the fusion of local details (such as lesion texture and color) with global context through local and global attention mechanisms and task-driven feature selection, while suppressing interference from healthy regions.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">3. the edge convolution fusion module (ecfm) strengthens edge details through sobel convolution and residual connections, achieving effective integration of edge-specific features with general features, further enhancing edge details and enabling more precise capture of lesion contour details.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:1em;margin-left:1em;text-indent:-1em;text-align:left\">4. the adaptive multi-scale dilated dense inception convolution module (amsddicm) extracts differentiated features using multi-shaped convolution kernels and adaptively fuses multi-scale information via a dynamic weight mechanism, capturing lesion morphologies at different pathogenesis stages.\u003c/p>\u003ca id=\"h3\" name=\"h3\">\u003c/a>\u003ch2>2 materials and methods\u003c/h2>\u003ch3>2.1 characteristics of walnut leaf brown spot and classification of disease severity levels\u003c/h3>\u003cp class=\"mb0\">in the local standard technical regulations for prevention and control of walnut brown spot (\u003ca href=\"#b19\">mcgranahan and leslie, 1991\u003c/a>), walnut brown spot is divided into four severity levels. the severity of plant leaf diseases is generally determined using the spot coverage method. in this study, walnut leaves with different disease severities were processed to separate disease-infected spots from healthy leaf areas, and the percentage of lesion area to total leaf area was calculated. with reference to the local standard, the disease grades specified in the standard were re-determined through calculations in this study, as shown in \u003ca href=\"#t1\">table&#xa0;1\u003c/a>. classification of different severity levels of walnut leaf brown spot was conducted in accordance with technical regulations for prevention and control of walnut brown spot (\u003ca href=\"#b33\">wang et&#xa0;al., 2022\u003c/a>; \u003ca href=\"#b37\">yang et&#xa0;al., 2021\u003c/a>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;1\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" name=\"table&#xa0;1\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" alt=\"www.frontiersin.org\" id=\"t1\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;1\u003c/b>. walnut leaf brown spot disease severity grading criteria.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">walnut leaf brown spot is caused by infection with the fungus \u003ci>ophiognomonia leptostyla\u003c/i> (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>). in the early infection stage, near-circular or irregular small spots appear on the leaves, with a gray-brown center and dark yellow-green to purplish-brown edges, and diseased leaves tend to fall off prematurely. in the middle stage, elongated elliptical or irregular slightly sunken dark brown lesions form, with larger spot size and light brown edges; longitudinal cracks are often present in the center of the lesions. in the late stage, lesions often coalesce to form large scorched necrotic areas, surrounded by yellow to golden-yellow zones, and small black granules (conidiomata and conidia of the pathogen) are scattered on the surface of the diseased tissue (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>). according to the degree of color and texture feature changes in infected leaves, walnut leaf disease is divided into four stages: healthy, early, middle, and late stages, corresponding to disease severity levels: healthy (level 0), mild (level 1), moderate (level 2), and severe (level 3). the different severity levels of walnut leaf brown spot are shown in \u003ca href=\"#f1\">figure&#xa0;1\u003c/a>.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;1\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" alt=\"four leaves are displayed, labeled a to d. a shows a healthy leaf with no spots. b features a mildly infected leaf with a few small spots. c presents a moderately infected leaf with numerous spots. d shows a severely infected leaf covered with dark spots.\" id=\"f1\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;1\u003c/b>. walnut leaves infected with brown spot disease at different severity levels. \u003cb>(a)\u003c/b> healthy leaves; \u003cb>(b)\u003c/b> mildly infected leaves; \u003cb>(c)\u003c/b> moderately infected leaves; \u003cb>(d)\u003c/b> severely infected leaves.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">\u003ca href=\"#t1\">table&#xa0;1\u003c/a> shows the ratio of walnut leaf brown spot lesion area to total leaf area. level 0 (healthy): healthy leaves without symptoms. level 1 (mild): near-circular or irregular small white spots appear on leaves, accounting for less than 5% of the leaf area. level 2 (moderate): lesions expand into irregular dark brown spots, covering 5% to 30% of the leaf area. level 3 (severe): abundant lesion coalesce to form large scorched necrotic areas, exceeding 30% of the leaf area.\u003c/p>\u003ch3>2.2 calculation algorithm for walnut leaf brown spot disease severity levels\u003c/h3>\u003cp class=\"mb0\">after capturing walnut leaf brown spot samples using a camera, to minimize subjective bias, this study strictly adhered to the objective quantitative criteria defined in the technical regulations for the control of walnut brown leaf spot (\u003ca href=\"#t1\">table&#xa0;1\u003c/a>), utilizing the lesion area percentage (k) as the primary grading indicator. we re-determined the severity levels through computational analysis, with the specific determination process shown in \u003ca href=\"#f2\">figure&#xa0;2\u003c/a>.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;2\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" alt=\"flowchart illustrating a process for identifying disease areas on leaves. images of leaves are converted through morphological color space to grayscale. the grayscale is used to highlight disease area s1 and leaf area s2. the ratio s1/s2 is applied, producing a final analysis image showing leaf disease marked in red.\" id=\"f2\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;2\u003c/b>. computational method for different severity levels of brown spot infection in walnut leaves.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">first, a python-based color segmentation algorithm was employed to convert images from the bgr color space to the hsv color space for processing. the hsv color space offers inherent advantages in color segmentation tasks. by analyzing the hue, saturation, and value characteristics of diseased regions, the color threshold range in the hsv space was determined (\u003ca href=\"#b4\">chaudhary et&#xa0;al., 2012\u003c/a>). after generating an initial disease region mask based on this threshold range, morphological operations such as closing and opening were applied to optimize mask quality, effectively eliminating noise while preserving the integrity of lesion contours. subsequently, the original image was converted to grayscale mode, and the leaf region was extracted using an adaptive threshold binarization algorithm. leaf boundaries were localized via contour detection technology, and a complete leaf mask was generated through contour filling. pixel statistical analysis was performed on both the leaf mask and disease mask to calculate the total leaf area and diseased region area, respectively. when a valid leaf area (&gt;0) was detected, the disease severity index was calculated using the following formula: disease severity index \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im1\">\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmo mathsize=\"10.5pt\">%\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">s\u003c/mi>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003c/mrow>\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">s\u003c/mi>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmn mathsize=\"10.5pt\">100\u003c/mn>\u003cmo mathsize=\"10.5pt\">%\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;&#xa0;&#xa0;s\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>: total area of diseased regions \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im2\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>: total area of the complete leaf.\u003c/p>\u003ch3>2.3 dataset construction\u003c/h3>\u003cp class=\"mb0\">data were collected from the walnut plantation base of tarim university (alar, xinjiang) between may and october 2024. leaves from three locally dominant early-fruiting cultivars (&#x2018;wen 185&#x2019;, &#x2018;xinxin 2&#x2019;, &#x2018;zha 343&#x2019;)widely cultivated in southern xinjiang were vertically photographed using an iphone 13. this genotypic diversity ensures model generalizability by encompassing varied disease phenotypes. the orchard follows standardized cultivation with 4m plant spacing, 5m row spacing (&#x2248;50 plants/mu), and conventional management practices. sampled trees were in full fruiting stage. to capture disease traits across microenvironments, samples were collected from safely accessible crown layers. the dataset includes healthy and brown spot-infected leaves three severity levels, spanning varied time periods, light conditions, and angles. after removing duplicates and invalid images,5,120 high-quality jpgs were retained for analysis.\u003c/p>\u003ch4>2.3.1 development of the walnut leaf brown spot dataset\u003c/h4>\u003cp class=\"mb0\">disease severity grading was established through quantitative lesion area analysis (\u003ca href=\"#f2\">figure&#xa0;2\u003c/a>). a representative subset of 512 images (10% of the full 5,120-image dataset) was selected via stratified random sampling, accurately preserving the original severity distribution. to ensure labeling rigor, two plant protection specialists (&gt;5 years&#x2019; expertise in walnut pathology, certified in local protocols) executed standardized annotation: pre-annotation training unified diagnostic criteria for ambiguous cases, with formal annotation commencing only after achieving pre-calibration inter-rater agreement (kappa coefficient &#x2265;0.75). as validated in \u003ca href=\"#t2\">table&#xa0;2\u003c/a>, dual statistical metrics confirmed gold-standard reliability&#x2014;inter-rater cohen&#x2019;s kappa reached 0.71, meeting landis &amp; koch&#x2019;s &#x201c;substantial agreement&#x201d; threshold; algorithm-expert consensus attained a weighted fleiss&#x2019; kappa of 0.77, substantially outperforming conventional cohen&#x2019;s kappa in multi-annotator scenarios. a three-stage standardization pipeline eliminated preprocessing discrepancies. the final dataset, partitioned into training/testing sets (8:2 ratio), strictly adheres to plant disease survey protocols and provides benchmarked data for deep learning model development (\u003ca href=\"#t3\">table&#xa0;3\u003c/a>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;2\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" name=\"table&#xa0;2\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" alt=\"www.frontiersin.org\" id=\"t2\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;2\u003c/b>. algorithm vs. expert consensus agreement evaluation.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;3\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" name=\"table&#xa0;3\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" alt=\"www.frontiersin.org\" id=\"t3\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;3\u003c/b>. walnut leaf brown spot disease image acquisition data.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>2.4 construction of walnut leaf brown spot disease severity grading model\u003c/h3>\u003ch4>2.4.1 the optimized mobilevitv3 network: cogfuse-mobilevit\u003c/h4>\u003cp class=\"mb15\">to address the challenge of difficult feature capture for small lesions in walnut leaf brown spot severity grading, this study proposes an innovative model, cogfuse-mobilevit. mobilevitv3 was selected as the backbone network due to its suitability for walnut brown spot grading. its hybrid mobilenet-vit architecture integrates cnn-based local feature extraction essential for lesion detail capture with transformer-enabled global contextual modeling critical for analyzing lesion spatial distribution. this design aligns with pathological requirements for severity grading, necessitating concurrent attention to local lesion characteristics and global infection patterns. the lightweight architecture further supports real-time processing on embedded devices, enabling future field deployment.\u003c/p>\u003cp class=\"mb0\">the model embodies a &#x201c;grading task-driven&#x201d; design principle. conceptually, it establishes a disease-specific framework of &#x201c;hierarchical screening to edge enhancement to dynamic fusion.&#x201d; this framework elevates general feature extraction to targeted solutions addressing three major agricultural challenges: suppressing interference from healthy tissues via hierarchical screening resolving feature confusion; strengthening blurred lesion contours through edge enhancement overcoming the bottleneck of edge blurriness; adaptively handling multi-stage morphological variations using dynamic fusion addressing morphological variation challenges. structurally, as illustrated in \u003ca href=\"#f3\">figure&#xa0;3\u003c/a>, the network implements a progressive feature extraction strategy. the hierarchical feature selection module (hfsm) first fuses shallow and middle-layer features. it employs a hierarchical attention mechanism to enhance semantic consistency while preserving spatial details. the edge convolution fusion module (ecfm) then processes these features. it utilizes learnable sobel operators to extract edge information, which is fused with conventional convolutional features via residual connections to augment edge perception capability. finally, the adaptive multi-scale dilated inception convolution module (amsddicm) enables adaptive processing of multi-scale edge features. this module is capable of both capturing fine edge changes in minute lesions and grasping the overall contour structure of large lesions, thereby comprehensively covering edge features across different developmental stages. based on these enhanced edge features, the network accurately outputs the final disease severity grade.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;3\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" alt=\"diagram illustrating a deep learning process for leaf classification. input images of leaves undergo convolution and mobilevit blocks, reducing dimensions and processing through hfsm, ecfm, and amsddicm modules. outputs are pooled and linearly transformed to logits.\" id=\"f3\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;3\u003c/b>. cogfuse-mobilevit architecture diagram (hfsm suppresses healthy regions through dynamic masks, ecfm explicitly extracts edges via sobel convolution, and amsddicm fuses multi-scale features through dynamic weights).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>2.4.2 the hierarchical feature selection module\u003c/h4>\u003cp class=\"mb0\">the hfsm (hierarchical feature selection module) is an innovative architecture proposed in this study. unlike mobilevit&#x2019;s direct feature concatenation, hfsm utilizes learnable prompt vectors (\u003ca href=\"#eq1\">equation 1\u003c/a>) to generate spatial masks, dynamically suppressing non-lesion regions. such task-driven selection is crucial for tiny objects. in the grading task of walnut leaf brown spot disease, this module utilizes a local attention mechanism to focus on micro-regions of walnut leaves, fusing shallow and middle-layer features. meanwhile, the hierarchical feature selection mechanism enables precise capture of local detail features such as lesion texture and color, effectively suppressing interference from healthy leaf regions and enhancing disease features. this provides high-discriminative feature representations for brown spot disease grading while preparing for subsequent lesion edge processing. as depicted in \u003ca href=\"#f4\">figure&#xa0;4\u003c/a>, the module processes two hierarchical feature maps. initial 1&#xd7;1 convolutions reduce both maps&#x2019; channels to half the output dimension curtailing computational load. one reduced map undergoes bilinear upsampling for spatial alignment with the other. these aligned features are summed and processed by a 3&#xd7;3 convolution to generate base-path features. simultaneously, the original reduced map and upsampled map are fed into dual local-global attention modules for parallel processing (\u003ca href=\"#b36\">xu, 2024\u003c/a>). each group contains local and global branches. during branch processing, the feature map is partitioned into p&#xd7;p non-overlapping patches \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im3\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">p\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmi mathsize=\"10.5pt\">j\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math> via the unfold operation. after calculating the mean of pixel features within each patch, the result is processed using the following core formula: attention distribution generation.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;4\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" alt=\"diagram illustrating a neural network architecture with multiple components. the upper section shows two convolution layers, lga modules, concatenation, and rep blocks. the lower section features processes like unfold, mean calculation, softmax, and feature selection, highlighting token and channel selection. various operations and components like multiplication, addition, and dropout are labeled, with symbols explaining their functions.\" id=\"f4\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;4\u003c/b>. architecture diagram of the hierarchical feature selection module (hfsm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">z\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ml\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">layernor\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ml\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">mean\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>1\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">a\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">softmax\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">z\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>2\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq1\">equations 1\u003c/a> and \u003ca href=\"#eq2\">2\u003c/a> convert the mean value of patch features into a high-dimensional feature vector through multi-layer perceptrons (mlp) and layer normalization (layernorm) (wherein(\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im4\">\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">mlp\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;and&#xa0;&#xa0;mlp\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>) \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im5\">\u003cmrow>\u003cmsup>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">achieve&#xa0;dimensional&#xa0;transformation&#xa0;p\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msup>\u003cmo mathsize=\"10.5pt\">&#x2192;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ouc\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmo mathsize=\"10.5pt\">&#x2192;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ouc\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/mrow>\u003c/math>)), and then generates the attention distribution via the softmax function \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im6\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">a\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmi mathsize=\"10.5pt\">j\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>. this mechanism enables the model to focus on the key features of lesion regions, weaken the interference from healthy leaf areas, enhance the specificity of feature selection, and provide high-quality features for subsequent precise grading. after the local and global features output by the two modules are spliced along the channel dimension, they are combined with the basic path features to generate the output features of the final processing module.\u003c/p>\u003cp class=\"mb0\">for leaf images with over 80% healthy tissue, dynamic masks are generated via learnable prompt vectors to suppress green texture features while enhancing the gray-brown features of lesions (\u003ca href=\"#b19\">mcgranahan and leslie, 1991\u003c/a>).\u003c/p>\u003ch4>2.4.3 ecfm edge convolutional fusion module\u003c/h4>\u003cp class=\"mb0\">in the walnut leaf brown spot grading task, lesion edge features are crucial for accurately determining disease severity. the standard mobilevit relies on cnn-transformer blocks to implicitly learn edges, whereas the ecfm (edge convolution fusion module) incorporates sobel convolutions, conventional convolutions, and residual connections, effectively fusing edge features with general features and thereby enhancing edge details. as shown in \u003ca href=\"#f5\">figure&#xa0;5\u003c/a>, the sobel branch employs sobel convolution to extract image edge information, accurately capturing the contour details of brown spot lesions; the convolutional branch captures general features such as leaf color and texture through standard convolution. the two realize feature fusion via the following core formula (\u003ca href=\"#eq3\">equation 3\u003c/a>):\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;5\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" alt=\"diagram showing a network flow. input x is split into two paths: one goes through a sobel filter and the other through a convolution (conv) layer. outputs are concatenated (concat) and followed by another conv layer. the result is summed with a bypass connection and passed through a final conv layer.\" id=\"f5\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;5\u003c/b>. architecture diagram of the edge convolutional fusion module (ecfm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">sobelconv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>3\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">processing the input feature map \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im7\">\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmi mathsize=\"10.5pt\">x\u003c/mi>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> through the sobel convolution operator (sobelconv) specifically extracts edge information of lesions (such as the boundary contours between lesions and healthy tissues, and the edge textures of small lesions). for the commonly seen blurred edges in brown spot disease, sobel convolution can enhance edge gradient changes, making the originally blurred lesion contours clearer, thus providing key edge feature support for subsequent grading. conventional feature extraction and fusion (\u003ca href=\"#eq4\">equation 4\u003c/a>):\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>4\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im8\">\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;c\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> extracting the overall features of leaves (such as the color distribution of lesions and the overall texture of leaves) through standard convolution forms a complement to edge features, preventing the model from focusing only on local edges while ignoring global lesion information.\u003c/p>\u003cp class=\"mb15\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im9\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> concatenating the edge feature s and the conventional feature c along the channel dimension achieves the initial fusion of edge details and global features. this fusion enables the model to both identify the fine contours of lesions and judge the disease condition by combining the overall color and texture changes of lesions, thereby improving the accuracy of grading. the module also introduces feature addition and subsequent convolution operations: the fused features are first processed by the first convolution layer \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im10\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math>, the corresponding operation results are added to the original input, and the final output is generated through the second convolution layer \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im11\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">final\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math>. in this process, the residual connection retains the original feature information, further strengthens the integration of edge features and conventional features, and ensures the effective transmission and enhancement of multi-level features.\u003c/p>\u003cp class=\"mb0\">the edge gradient information extracted by sobel convolution (such as grayscale differences between diseased spots and healthy tissues) and the texture features from conventional convolution (such as the roughness of necrotic areas) are fused via residual connections. this not only preserves the blurred edges of early-stage lesions but also prevents edge features from being disconnected from the overall texture (\u003ca href=\"#b8\">ghaiwat and arora, 2014\u003c/a>).\u003c/p>\u003ch4>2.4.4 amsddicm adaptive multi-scale dilated depthwise inseparable convolution module\u003c/h4>\u003cp class=\"mb0\">this module differs from mobilevit&#x2019;s single-scale convolution, leverages multi-shaped convolution kernels to accurately extract lesion features of circular, irregular, and other shapes from multiple dimensions, enabling the identification of subtle differences in lesion edges and internal colors to enrich feature dimensions. meanwhile, depth wise separable convolutions are employed to accommodate lesion scales at different stages of disease development. specifically, the input features are first split into two groups, each entering the amsddicm module to extract features via multi-scale depth wise separable convolutions. a dynamic weighting mechanism (global pooling + convolution + softmax) is used to generate weights for fusing multi-scale features. the processed features from the two groups are concatenated, and finally, cross-channel fusion is completed via 1&#xd7;1 convolution to output the final optimized features. the entire process integrates multi-scale convolution, dynamic weight allocation, and feature fusion to enhance the model&#x2019;s ability to capture complex features, as shown in \u003ca href=\"#f6\">figure&#xa0;6\u003c/a>. in response to the morphological differences between early-stage small spots (1-3mm in diameter) and late-stage fused spots (over 20mm in diameter), the dynamic weight mechanism enables the model to adaptively allocate the contributions of 3&#xd7;3 kernels for capturing local textures and 11&#xd7;1 kernels for extracting strip-shaped spreading features. this addresses the limitation of fixed weights in traditional inception architectures in adapting to multi-scale morphologies.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;6\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" alt=\"diagram showing a neural network process. the top section involves splitting input channels, applying a dms 2d convolution, concatenation, and another convolution. the bottom section includes a pooling layer, convolution, rearrangement, softmax, summation, batch normalization, and activation. the process results in output \\(x^{''}\\).\" id=\"f6\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;6\u003c/b>. architecture diagram of the adaptive multi-scale dilated depth wise inseparable convolution module (amsddicm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">when the input feature tensor x with the number of channels c enters the amsddicm module, it first undergoes a feature grouping operation: the input features are evenly split into two groups along the channel dimension, with each group containing half of the original number of channels (c//2). this grouping strategy not only reduces computational complexity but also creates conditions for subsequent multi-scale feature extraction. each feature group is fed into an independent dmsconv2d module, which adopts different configurations such as 3&#xd7;3, 5&#xd7;5 square convolution kernels and 1&#xd7;11, 11&#xd7;1 strip-shaped convolution kernels &#x2014; the square kernels capture local textures, while the strip-shaped kernels extract direction-sensitive long-range dependencies. this dynamic weight allocation draws inspiration from adaptive strategies in agricultural iot systems. similar to how salinity-aware ets models prioritize soil conductivity features under salt stress (\u003ca href=\"#b40\">zhang et&#xa0;al., 2023\u003c/a>), our mechanism selectively amplifies lesion morphological features critical for severity grading. the dynamic weighting mechanism of dmsconv2d generates weights through global average pooling and 1&#xd7;1 convolution, with the core formulas as follows:\u003c/p>\u003cp class=\"mb15\">weighted basic feature generation:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">rearrange\u003c/mtext>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003c/msub>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">avgpoo\u003c/mtext>\u003cmn mathsize=\"10.5pt\">l2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">d\u003c/mtext>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>5\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq5\">equation 5\u003c/a> compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information (such as the overall distribution characteristics of lesions at different scales), and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmo mathsize=\"10.5pt\">.\u003c/mo>\u003cmtext mathsize=\"10.5pt\">softmax\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>6\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq6\">equation 6\u003c/a> compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information, such as the overall distribution characteristics of lesions at different scales, and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">0\u003c/mn>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msubsup>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">dwcon\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">v\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>7\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb0\">\u003ca href=\"#eq7\">equation 7\u003c/a> is based on dynamic weights \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im12\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>) for different convolution kernels \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im13\">\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;dwconv\u003c/mtext>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> weighted fusion is performed on the extracted features. for walnut brown spot lesions exhibiting size heterogeneity and morphological complexity&#x2014;ranging from early-stage circular micro-lesions to late-stage coalesced irregular lesions&#x2014;this mechanism adaptively modulates multi-scale feature contributions. it prioritizes retention of morphology-discriminative features, local textures in small lesions or directional distributions in large lesions, thereby comprehensively capturing pathological characteristics across developmental stages. the processed features undergo channel-wise concatenation to generate optimized outputs (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>).\u003c/p>\u003ch3>2.5 experimental process for severity grading of walnut leaf brown spot disease\u003c/h3>\u003cp class=\"mb0\">\u003ca href=\"#f7\">figure&#xa0;7\u003c/a> is the overall flow chart of severity grading of walnut leaf spot disease. first is the image acquisition stage, where raw images of walnut leaves are captured and collected to construct an image database containing pictures to be classified (\u003ca href=\"#b29\">singh et&#xa0;al., 2019\u003c/a>). next is the image preprocessing stage, which involves sequentially calculating and classifying disease severity levels, establishing image labels, and performing image preprocessing operations. subsequently, the processed data are used to train the constructed dataset. finally, in the model training and performance evaluation stage, the preprocessed images are input into the model for classifying walnut leaf brown spot disease, after which performance evaluation is conducted on the model&#x2019;s classification results to determine the model&#x2019;s effectiveness and accuracy.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;7\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" alt=\"flowchart illustrating a process for walnut leaf brown spot disease classification. it consists of three main phases: image acquisition, preprocessing, and model training and evaluation. the image acquisition phase involves collecting walnut leaf images and storing them in a database. the preprocessing phase includes classification of disease grades, establishing image labels, and training the dataset. finally, the model classifies the disease, resulting in output images and a performance evaluation matrix for accuracy verification. the chart uses labeled boxes, arrows, and image samples for illustration.\" id=\"f7\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;7\u003c/b>. overall flow chart for severity grading of walnut leaf brown spot disease.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>2.6 experimental parameters and evaluation metrics\u003c/h3>\u003ch4>2.6.1 test environment and hyperparameter setting\u003c/h4>\u003cp class=\"mb15\">the experimental setup of this study is based on deep learning technology and leverages high-performance computing resources. all experiments were conducted on the windows 10 operating system. the hardware platform consists of an amd ryzen 7 3700x processor and an nvidia rtx 2080ti graphics card. the software environment was built using python 3.8, the pytorch 1.13.0 deep learning framework, and the cuda 11.3 parallel computing platform. additionally, pytorch 1.13.0&#x2014;a widely adopted open-source deep learning library renowned for its high flexibility&#x2014;was selected, making it well-suited for research and development.\u003c/p>\u003cp class=\"mb0\">during model training, multiple adjustments were made to the hyperparameters to compare test results and select the optimal hyperparameter combination. the model accepts input images sized at 224&#xd7;224 pixels, with a configured batch size of 32 during training and a maximum of 100 training epochs. the optimizer employs an initial learning rate of 0.003.\u003c/p>\u003ch4>2.6.2 evaluation metrics\u003c/h4>\u003cp class=\"mb15\">this paper aims to evaluate the performance of the model and verify the effectiveness of the improvement measures. we selected multiple evaluation metrics, and all subsequent experimental results adopt the method of calculating averages via 5-fold cross-validation, aiming to comprehensively and reliably evaluate the model performance. including precision (p), recall (r), etc. (\u003ca href=\"#eq8\">equations 8\u003c/a>&#x2013;\u003ca href=\"#eq16\">16\u003c/a>) these metrics can be calculated using the following formulas.\u003c/p>\u003cp class=\"mb15\">the arithmetic mean of the metric values across the five folds:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmover accent=\"true\">\u003cmtext mathsize=\"10.5pt\">m\u003c/mtext>\u003cmo mathsize=\"10.5pt\">&#xaf;\u003c/mo>\u003c/mover>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmn mathsize=\"10.5pt\">5\u003c/mn>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">5\u003c/mn>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">m\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>8\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">precision\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>9\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">recall\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fn\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>10\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">score\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003c/mrow>\u003cmrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fn\u003c/mtext>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>11\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">macro&#xa0;precision=\u003c/mtext>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>12\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">macro&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">r\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>13\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;precision=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&#xb7;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">precisio\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>14\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&#xb7;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">precisio\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>15\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmsubsup>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003cmtext mathsize=\"10.5pt\">t\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>16\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003ca id=\"h4\" name=\"h4\">\u003c/a>\u003ch2>3 experiments and results analysis\u003c/h2>\u003ch3>3.1 core module design and validity experimental verification\u003c/h3>\u003ch4>3.1.1 comparative test of necessity of hfsm module\u003c/h4>\u003cp class=\"mb0\">to validate the necessity of the hierarchical feature selection module (hfsm) within the cogfuse-mobilevit framework, this study conducted comparative tests against two mainstream lightweight attention modules: se (squeeze-and-excitation) and cbam (convolutional block attention module). as illustrated in \u003ca href=\"#t4\">table&#xa0;4\u003c/a>, the hfsm module achieves a significantly higher accuracy of 86.61%, outperforming the se module and cbam module by 7.38% and 4.46%, respectively. this demonstrates that its hierarchical feature selection mechanism more effectively captures discriminative features. in terms of computational efficiency, the parameters and flops of hfsm remain comparable with those of the se module and cbam module, indicating that its performance breakthrough stems from innovative structural design under equivalent lightweight constraints. comprehensive results confirm that replacing hfsm with se or cbam modules would incur a performance degradation exceeding 4%, thereby validating the indispensable value of hfsm in enabling efficient feature selection within the cogfuse-mobilevit framework.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;4\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" name=\"table&#xa0;4\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" alt=\"www.frontiersin.org\" id=\"t4\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;4\u003c/b>. performance comparison of attention modules within the cogfuse-mobilevit framework.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.2 convolutional kernel selection in amsddicm\u003c/h4>\u003cp class=\"mb0\">rigorous validation via kernel combination ablation studies (\u003ca href=\"#t5\">table&#xa0;5\u003c/a>) demonstrates. the hybrid configuration (3&#xd7;3 \u003cb>+\u003c/b> 5&#xd7;5 \u003cb>+\u003c/b> 1&#xd7;11) significantly outperformed square-only kernels (3&#xd7;3 \u003cb>+\u003c/b> 5&#xd7;5) accuracy 86.61% and 82.15% stage-specific recall gains. mid-stage lesions (level 2) raise 9.4 percentage points. late-stage lesions (level 3) raise 6.1 percentage points. this empirically validates the necessity of 1&#xd7;11 rectangular kernels for capturing linear pathological features, overcoming the limitation of isotropic kernels in detecting anisotropic structures.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;5\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" name=\"table&#xa0;5\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" alt=\"www.frontiersin.org\" id=\"t5\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;5\u003c/b>. comparison of different convolution kernels in amsddicm.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.3 the impact of new modules on computational complexity\u003c/h4>\u003cp class=\"mb0\">\u003ca href=\"#f8\">figure&#xa0;8\u003c/a> shows that the hfsm module incurs a 169% flops increase to achieve a breakthrough improvement in early-stage lesion detection&#x2014;reducing level 0/1 misclassification by 24%, thereby establishing the pathological foundation for severity grading. the ecfm module contributes a mere 3% flops increment yet drives a 3.68 percentage point accuracy gain through enhanced edge feature representation. with only a 0.028g flops overhead 1.3%, the amsddicm module enables adaptive fusion of multiscale pathological deformations, culminating in a 7.80-pp accuracy leap. these modules form a cascaded optimization paradigm: hfsm&#x2019;s substantial cost resolves the core pathological bottleneck, while subsequent modules deliver superlinear returns&#x2014;harvesting 6.85-pp accuracy gain with just 14% additional flops&#x2014;collectively establishing the globally optimal computation-performance equilibrium.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;8\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" alt=\"bar graphs comparing different configurations of mobilevit models across three metrics. panel a shows accuracy percentages, with values from 78.81% to 86.61%. panel b displays flops, ranging from 0.74g to 2.1g. panel c illustrates parameters in millions, with values between 1.94m and 2.02m. each graph compares mobilevit, mobilevit plus hfsm, mobilevit plus hfsm plus ecfm, and cogfuse-mobilevit.\" id=\"f8\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;8\u003c/b>. comparison of the impact of each newly added module on computational complexity \u003cb>(a)\u003c/b> accuracy (%); \u003cb>(b)\u003c/b> flops (g); \u003cb>(c)\u003c/b> params (m). measured on nvidia rtx 2080ti gpu (pytorch 1.13.0, cuda 11.3) with 224&#xd7;224 input.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.4 comparison of the influence of different module fusion on model performance\u003c/h4>\u003cp class=\"mb0\">to validate the effect of module fusion, \u003ca href=\"#t6\">table&#xa0;6\u003c/a> compares model performances with different combinations. when only hfsm (hierarchical feature selection module) is introduced, precision increases from the baseline of 82.23% to 83.77%, but recall decreases by 3 percentage points to 72.31%, causing the f1 score to slightly decline to 78.04%. accuracy rises to 82.15%, indicating improved overall classification correctness of the model, but with potential risk of missed detections.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;6\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" name=\"table&#xa0;6\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" alt=\"www.frontiersin.org\" id=\"t6\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;6\u003c/b>. impact of fusion of different modules on model performance.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">when hfsm and ecfm (edge-context feature module) are synergistically introduced, precision increases to 86.34%, recall recovers to 74.69%, and both f1 score and accuracy are significantly optimized. hfsm fuses shallow and middle-layer features through a hierarchical attention mechanism, laying the foundation for subsequent edge feature processing; ecfm enhances lesion edge features. the combination of the two effectively improves the integrity of feature representation and the clarity of lesion boundaries.\u003c/p>\u003cp class=\"mb15\">the combination of hfsm and amsddicm (adaptive multi-scale dilated depthwise inseparable convolution module) further pushes recall to 76.24%, accuracy to 83.94%, and f1 score to 80.79%, outperforming the hfsm+ecfm combination. amsddicm compensates for hfsm&#x2019;s deficiency in local feature refinement through attention-guided multi-scale detail fusion, which integrates multi-layer detail feature weights, especially suitable for scenarios with small targets or blurred features.\u003c/p>\u003cp class=\"mb0\">when the three modules work synergistically, all indicators reach optimal levels: precision, recall, f1 score, and accuracy increase by 12.19%, 7.67%, 9.62%, and 10.00% respectively compared with the baseline. among them, hfsm lays the foundation for cross-layer feature fusion, ecfm effectively integrates edge-specific features with general features to enhance image edge information, and amsddicm adaptively fuses multi-scale and multi-type features through an attention mechanism, forming a progressive optimization chain from &#x201c;hierarchical feature extraction&#x201d; to &#x201c;edge semantic enhancement&#x201d; and then to &#x201c;multi-layer detail fusion&#x201d;. the experimental results show a balanced improvement in both precision and recall, indicating that the fusion of the three modules enables the model to focus more on learning the features of small brown spot lesions, thereby improving its classification performance. this synergistic task-specific design, combining hierarchical selection, explicit edge enhancement, and adaptive multi-scale fusion, fundamentally distinguishes cogfuse-mobilevit from prior mobilevit hybrids designed for general vision or localization tasks.\u003c/p>\u003ch4>3.1.5 influence of different module combinations on f1-score of level (0-3)\u003c/h4>\u003cp class=\"mb0\">in order to verify the targeted improvement of each module for a specific disease level, we conducted the following comparative experiments as shown in the \u003ca href=\"#t6\">table&#xa0;6\u003c/a>, when hfsm is introduced alone, the f1-score of level 0 increases from 83.10% to 85.60%, effectively reducing feature confusion between healthy leaves and early-stage lesions. after adding ecfm, the f1-score of level 1 rises from 73.20% (with hfsm alone) to 79.80%, significantly mitigating the recognition bias caused by blurred edges of small lesions. amsddicm increases the f1-score of level 3 from 80.37% to 85.54%, enhancing the adaptability to the morphology of large-scale fused scorched areas. with the collaboration of the three modules, the f1-scores of level 0&#x2013;3 reach 93.99%, 82.57%, 84.28%, and 85.54% respectively, achieving balanced optimization of performance across all levels.\u003c/p>\u003ch3>3.2 results comparison of different algorithms and statistical significance verification\u003c/h3>\u003ch4>3.2.1 comparison of grading results for different classification models\u003c/h4>\u003cp class=\"mb0\">to verify the effectiveness of the cogfuse-mobilevit model, we selected 9 commonly used classification models to compare with the optimized cogfuse-mobilevit model, and the results are the average of 5-fold cross-validation over 120 epochs. as shown in \u003ca href=\"#t7\">table&#xa0;7\u003c/a>, the proposed cogfuse-mobilevit model achieved the highest grading performance in terms of precision, recall, and f1-score for identifying walnut leaf brown spot disease at different severity levels among all compared models. the improved cogfuse-mobilevit model exhibits an accuracy of 86.61%, representing a 7.80-percentage-point improvement over the original model. overall, the experimental results highlight that the enhanced cogfuse-mobilevit model is more conducive to focusing on lesion edge details and accurately learning the features of different severity levels of walnut leaf brown spot disease, thereby improving the model&#x2019;s classification performance.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;7\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" name=\"table&#xa0;7\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" alt=\"www.frontiersin.org\" id=\"t7\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;7\u003c/b>. comparison of grading results for different classification models.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.2.2 comparison of performance and reliability validation of different algorithms\u003c/h4>\u003cp class=\"mb0\">in model training, to quantify the reliability of results and avoid random biases from single experiments, this study employs 5-fold cross-validation to generate 95% confidence intervals, as shown in the \u003ca href=\"#f9\">figure&#xa0;9\u003c/a>. the results demonstrate that cogfuse-mobilevit takes a significant lead with an accuracy of 86.61% (95% ci: [85.24%, 87.89%]). its narrowest confidence interval range indicates the strongest generalization capability. among the comparative models, mobilevitv3 has a 95% ci of [77.20%, 80.42%]; its lower bound is significantly lower than that of cogfuse-mobilevit, confirming that the 7.8% performance improvement is not due to random fluctuations. furthermore, the confidence intervals constructed through 5-fold cross-validation further highlight the reliability of the experimental results.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;9\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" alt=\"bar chart showing confidence intervals for accuracy rates of different classification models. models with higher accuracy include cogfuse-mobilevit at 86.61 and mobilevitv3 at 78.81. other models range from 73.63 to 68.72. each model is color-coded, with a corresponding legend on the right.\" id=\"f9\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;9\u003c/b>. confidence intervals of accuracy for different classification models in 5-fold cross-validation.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.2.3 statistical significance verification of model improvement\u003c/h4>\u003cp class=\"mb0\">to statistically evaluate the performance improvement of cogfuse-mobilevit over the baseline mobilevitv3, an independent samples t-test was conducted (\u003ca href=\"#t8\">table&#xa0;8\u003c/a>). for each model architecture, five independent training runs. the null hypothesis stated that the mean accuracy of cogfuse-mobilevit was equal to that of mobilevitv3 \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im14\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">h\u003c/mi>\u003cmn mathsize=\"10.5pt\">0\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">:\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">c\u003c/mi>\u003cmtext mathsize=\"10.5pt\">og\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">m\u003c/mi>\u003cmi mathsize=\"10.5pt\">o\u003c/mi>\u003cmi mathsize=\"10.5pt\">b\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>), while the alternative hypothesis stated that cogfuse-mobilevit had a higher mean accuracy (\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im15\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">h\u003c/mi>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">:\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">c\u003c/mi>\u003cmtext mathsize=\"10.5pt\">og\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&gt;\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">m\u003c/mi>\u003cmi mathsize=\"10.5pt\">o\u003c/mi>\u003cmi mathsize=\"10.5pt\">b\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>)cogfuse-mobilevit achieved a mean accuracy, significantly outperforming mobilevitv3. the independent samples t-test confirmed this improvement (t(8)=18.92,p=3.7&#xd7;10 \u003csup>&#x2212;8\u003c/sup>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;8\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" name=\"table&#xa0;8\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" alt=\"www.frontiersin.org\" id=\"t8\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;8\u003c/b>. comparison of accuracy results between mobilevitv3 and cogfuse-mobilevit using independent samples t-test.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">under 5-fold cross-validation (\u003ca href=\"#f10\">figure&#xa0;10\u003c/a>), cogfuse-mobilevit achieves rapid convergence within the first 20 epochs, demonstrating more efficient feature learning capability. upon entering the steady-state phase, its validation loss is reduced by over 5-fold compared to the baseline mobilevitv3, directly confirming smaller prediction errors and superior generalization capability. meanwhile, the smooth and minimally fluctuating loss curve of cogfuse-mobilevit reflects strong robustness against data noise and distribution variations. combined with the previous statistical findings from accuracy confidence intervals and independent t-tests, these results collectively validate the statistical significance of the improved model.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;10\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" alt=\"line graph showing validation loss over epochs for two models: cogfuse-mobilevit (blue) and mobilevitv3 (orange). loss decreases sharply initially, then stabilizes, with cogfuse-mobilevit exhibiting a consistently lower loss.\" id=\"f10\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;10\u003c/b>. comparison of loss curves between the cogfuse-mobilevit model and the original model in 5-fold cross-validation within 120 epochs.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.3 model result analysis performance comparison of different models\u003c/h3>\u003cp class=\"mb0\">\u003ca href=\"#f11\">figure&#xa0;11a\u003c/a> shows that in the walnut leaf brown spot disease severity grading task, the classification accuracy of all models gradually improved and stabilized with the increase of training epochs, indicating that the models continuously optimized during learning until convergence. cogfuse-mobilevit stood out prominently: it achieved rapid accuracy improvement, reached a high level in relatively early training epochs with minimal subsequent fluctuations, and stably maintained the highest accuracy, demonstrating fast convergence and strong generalization ability to efficiently extract features distinguishing different disease levels. densenet also maintained high accuracy in the late training stage, but its accuracy improvement was slower, with slightly inferior convergence speed and final stability compared to cogfuse-mobilevit. models like resnet and regnet showed limited accuracy gains with gentle upward trends, and their final stable accuracy values were significantly lower, reflecting insufficient feature extraction capabilities possibly due to network architecture or parameter optimization efficiency.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;11\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" alt=\"panel a shows a line graph comparing the accuracy of different neural networks over 100 epochs. cogfuse-mobilevit reaches the highest accuracy, achieving over 90 percent. panel b displays a line graph comparing loss over epochs. cogfuse-mobilevit has the lowest loss, declining swiftly to under 0.2. the legend lists networks like densenet, efficientnet, and mobilevitv3.\" id=\"f11\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;11\u003c/b>. comparison of accuracy and loss across different models \u003cb>(a)\u003c/b> accuracy line chart; \u003cb>(b)\u003c/b> loss line chart.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">in \u003ca href=\"#f11\">figure&#xa0;11b\u003c/a>, all models showed high initial loss values that dropped rapidly and then stabilized, following the typical learning process of iterative optimization. cogfuse-mobilevit achieved significant early loss decline and stabilized near the minimum, indicating efficient feature learning and strong fitting capability. while densenet also reached a relatively low loss level, it remained slightly higher than cogfuse-mobilevit. other models had higher late-stage loss values: some showed fast initial decline but converged to higher levels, indicating shortcomings in capturing key disease features.\u003c/p>\u003ch3>3.4 analysis of detection results for different classification models\u003c/h3>\u003cp class=\"mb0\">the confusion matrix is a key indicator for evaluating classification models: the higher the diagonal values, the higher the classification accuracy for the corresponding category, and the lower the off-diagonal values, the fewer misjudgments. \u003ca href=\"#f12\">figure&#xa0;12\u003c/a> shows the classification results of different models for the four disease grades (0&#x2013;3) of walnut leaf brown spot disease. it is clearly evident that the overall classification performance, particularly the accuracy of cogfuse-mobilevit across all categories, is remarkably high with minimal misjudgments, demonstrating that the model has enhanced discrimination ability for disease grades and performs optimally in distinguishing the four disease levels.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;12\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" alt=\"six normalized confusion matrices labeled a to f show predicted versus true values ranging from zero to three. each matrix illustrates classification performance with varying accuracy per class, indicated by different shades of red. brighter reds denote higher accuracy, while lighter shades indicate lower accuracy or misclassification.\" id=\"f12\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;12\u003c/b>. confusion matrices of the improved cogfuse-mobilevit model and traditional classification models \u003cb>(a)\u003c/b> densenet; \u003cb>(b)\u003c/b> efficientnet; \u003cb>(c)\u003c/b> efficientnetv2; \u003cb>(d)\u003c/b> swin transformer; \u003cb>(e)\u003c/b> mobilevitv3; \u003cb>(f)\u003c/b> cogfuse-mobilevit.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">the edge-enhanced feature module (ecfm) effectively addressed level 0 and level 1 misclassification by capturing incipient lesion features. baseline analysis revealed substantial false negatives for early-stage lesions, with level 1 and level 0 misclassification reaching 32%. ecfm reduced this rate to 8%, demonstrating its capacity to resolve texture confusion through enhanced edge contour delineation. similarly, the adaptive multi-scale dilated convolution module (amsddicm) significantly mitigated level 2 and level 3 mutual misclassification. amsddicm drastically reduced both errors by enhancing local fine-grained features in mid-stage lesions (level 2) while strengthening global fusion features in late-stage coalesced lesions (level 3). this resolves grading ambiguity arising from the morphological continuum of lesion progression.\u003c/p>\u003cp class=\"mb0\">to establish a comprehensive performance evaluation framework and quantify the model&#x2019;s overall discriminative capability, \u003ca href=\"#f13\">figure&#xa0;13\u003c/a> presents the receiver operating characteristic (roc) curves of cogfuse-mobilevit across four severity levels. these curves compare the true positive rate (tpr) against the false positive rate (fpr) by dynamically adjusting the classification threshold. the area under the curve (auc) serves as the primary performance metric, where a higher value indicates stronger classification ability. notably, the auc values for all categories significantly exceed the level of random guessing (auc=0.5), fully demonstrating that the model possesses robust discriminative capability across different severity levels.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;13\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" alt=\"receiver operating characteristic (roc) curve chart showing true positive rate versus false positive rate for four classes: class 0 (blue), class 1 (orange), class 2 (green), and class 3 (red), along with an average roc curve (dashed purple). the curves are above the diagonal, indicating good model performance.\" id=\"f13\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;13\u003c/b>. roc curves of the four different severity levels for cogfuse-mobilevit.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.5 tsne visualization of features extracted by different models\u003c/h3>\u003cp class=\"mb0\">the tsne visualization of features extracted from the models is shown in \u003ca href=\"#f14\">figures&#xa0;14a&#x2013;d\u003c/a>. this study analyzed the features of the original model at the 20th, 50th, and 100th training epochs, as well as the improved model at the 100th epoch. after 20 epochs of training, the original model showed a highly discrete feature distribution. limited by the number of training epochs, the model failed to fully learn discriminative features, resulting in insufficient distinction between categories and significant overlap among different classes. this indicates that under this training intensity, the original model&#x2019;s ability to capture meaningful patterns was relatively limited. when the original model was trained to 50 epochs, compared with (a), the feature aggregation significantly improved. however, inter-class overlap still existed, suggesting that although prolonged training aided feature learning, the original model&#x2019;s architecture had inherent defects in achieving clear feature separation. at 100 epochs of training, the original model exhibited more prominent feature clustering. nevertheless, some regions still lacked clear separation between different categories, indicating that even after prolonged training, the original model faced challenges in maximizing inter-class distance and intra-class compactness. in figure (d), the improved model after 100 epochs of training demonstrated a more superior feature distribution: each category was tightly clustered, achieving high intra-class compactness, while distinct boundaries between different categories were established, resulting in significant inter-class distance. this suggests that the model improvements effectively enhanced its ability to extract discriminative features, greatly reduced feature ambiguity, and improved feature discriminative power. compared with the original model, it showed stronger feature discrimination and optimization potential.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;14\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" alt=\"four scatter plots labeled a, b, c, and d illustrate data points grouped by colors representing levels zero to three. each plot displays varying cluster formations. plot a shows two distinct clusters. plot b has dispersed clusters, with one elongated group. plot c features overlapping clusters, and plot d displays four well-separated clusters.\" id=\"f14\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;14\u003c/b>. tsne visualization of features extracted by different models. after the three modules work synergistically, the boundaries of feature clustering between level 0 (healthy) and level 1 (early-stage) are significantly clearer, which verifies the effectiveness of edge-texture collaborative learning \u003cb>(a)\u003c/b> the original model was trained for 20 epochs; \u003cb>(b)\u003c/b> the original model was trained for 50 epochs; \u003cb>(c)\u003c/b> the original model was trained for 100 epochs; \u003cb>(d)\u003c/b> the improved model was trained for 100 epochs.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.6 radar chart for comparison of classification performance between original and improved models\u003c/h3>\u003cp class=\"mb0\">as shown in \u003ca href=\"#f15\">figure&#xa0;15\u003c/a>, a radar chart compares the performance of the original model and the improved cogfuse-mobilevit model across multiple evaluation metrics. in terms of accuracy, cogfuse-mobilevit exhibits significantly higher values than the original mobilevitv3 model, indicating that the improved model achieves overall higher classification accuracy. macro-average precision, which considers the average precision of each category, clearly shows that cogfuse-mobilevit also performs better, meaning it has superior precision across all categories. meanwhile, cogfuse-mobilevit also demonstrates better macro-average recall. when examining weighted-average precision and weighted-average recall&#x2014;metrics that account for class imbalance&#x2014;cogfuse-mobilevit outperforms mobilevitv3 in both, indicating that in practical applications, even with uneven class distribution, the cogfuse-mobilevit model can deliver better performance. these results further demonstrate the model&#x2019;s reliability and practicality.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;15\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" alt=\"radar chart comparing two models: mobilevitv3 (red) and cogfuse-mobilevit (blue). metrics include accuracy, macro average precision, macro average recall, weighted average precision, and weighted average recall. each axis ranges from fifty to one hundred.\" id=\"f15\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;15\u003c/b>. multi-metric comparison between mobilevitv3 and improved cogfuse-mobilevit model.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.7 public data set experiment\u003c/h3>\u003cp class=\"mb15\">to further validate the efficacy and generalizability of the proposed cogfuse-mobilevit model, experiments were conducted on the public dataset appleleaf9 (\u003ca href=\"#b38\">yang et&#xa0;al., 2022\u003c/a>). this dataset comprises healthy apple leaves and eight categories of apple leaf diseases captured in field environments without restrictions on imaging angles or noise interference. the dataset was partitioned into training and test sets at an 8:2 ratio. all images were resized to 224&#xd7;224 pixels to optimize deep learning model training efficiency. following the hyperparameter configurations specified in &#x201c;experimental parameters and evaluation metrics&#x201d;, both the baseline mobilevitv3 and cogfuse-mobilevit models were trained and evaluated.\u003c/p>\u003cp class=\"mb0\">cross-species validation demonstrates that the cogfuse-mobilevit model delivers exceptional performance on the appleleaf9 dataset. as presented in \u003ca href=\"#t9\">table&#xa0;9\u003c/a>, the model comprehensively surpasses the baseline mobilevitv3 across all four core metrics: precision increases by 0.16 percentage points, recall achieves a breakthrough improvement of 3.45 percentage points, f1-score rises by 1.18 percentage points, and accuracy elevates by 1.14 percentage points. the synergistic enhancement in both precision and recall signifies that performance gains stem from strengthened feature discriminability rather than metric trade-off compromises. the remarkable percentage points recall gain substantially mitigates leaf disease omission rates, while the holistic advance in f1-score further attests to the model&#x2019;s robustness. these results collectively validate the generalizability of cogfuse-mobilevit&#x2019;s core innovations in agricultural fine-grained disease grading, establishing a transferable paradigm for small-target pathology identification across plant species.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;9\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" name=\"table&#xa0;9\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" alt=\"www.frontiersin.org\" id=\"t9\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;9\u003c/b>. experimental results of the original model and cogfuse-mobilevit model on appleleaf9 data set.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ca id=\"h5\" name=\"h5\">\u003c/a>\u003ch2>4 conclusion\u003c/h2>\u003cp class=\"mb0\">this study addresses the challenges in precise grading of walnut leaf brown spot disease. by adopting dynamic feature filtering, edge gradient reinforcement, and multi-scale morphological adaptation, it effectively resolves three key limitations of existing hybrid architectures and the baseline model mobilevitv3 in plant disease grading weak capability in capturing blurred edges, poor multi-scale adaptability, and interference from healthy tissues. experimental results show that the model achieves an 86.61% grading accuracy on a dataset encompassing diverse lighting conditions, cultivars, and lesion stages, representing a 7.80% improvement over the baseline mobilevitv3 and outperforming nine state-of-the-art models. at the same time, this study confirmed that the performance improvement of cogfuse-mobilevit was statistically significant and stable through strict statistical tests, providing a reliable method for accurate classification of walnut leaf spot disease. the constructed image dataset and proposed grading re-measurement method lay the foundation for accuracy. this approach provides a new paradigm for small-target disease grading, with core modules transferable to multi-crop disease recognition. beyond disease classification, context-aware ai frameworks demonstrate significant efficacy in agricultural management (\u003ca href=\"#b1\">acharya et&#xa0;al., 2022\u003c/a>). iot systems dynamically adjust fertilization recommendations by integrating real-time soil-crop data, while salinity-corrected evapotranspiration (ets) models optimize irrigation strategies for saline-alkali soils (\u003ca href=\"#b15\">li et&#xa0;al., 2023\u003c/a>; \u003ca href=\"#b30\">su et&#xa0;al., 2023\u003c/a>). similarly, multimodal fusion techniques enhance reference evapotranspiration (eto) prediction accuracy, facilitating precision water allocation (\u003ca href=\"#b11\">jo et&#xa0;al., 2021\u003c/a>; \u003ca href=\"#b27\">rustia et&#xa0;al., 2023\u003c/a>). these breakthroughs collectively validate the robust capability of adaptive feature processing in complex agricultural environments&#x2014;a core principle that resonates with our dynamic weighting strategy for disease feature extraction. future research should therefore integrate cross-modal and cross-domain capabilities to establish multidimensional assessment systems and regional dynamic monitoring frameworks, thereby delivering comprehensive technical solutions for small-target disease control.\u003c/p>\u003ca id=\"h6\" name=\"h6\">\u003c/a>\u003ch2>5 discussion and future work\u003c/h2>\u003cp class=\"mb15\">compared with existing methods, this study overcomes the limitation of traditional deep learning models relying on fixed convolution kernels, enabling adaptive capture of lesion features with multi-shaped convolution kernels to accurately extract edge textures of circular micro-lesions and regional contours of irregular lesions. although the constructed multi-source dataset covers diverse lighting conditions, cultivars, and lesion development stages, the homogeneous internal features of severe diseases lead to limited recall rate. meanwhile, when lesions are severely overlapped or mixed with mechanical damage, insect damage, or other types of injuries, the discriminative accuracy of the model is affected. for ultra-small lesions, the local feature information is too weak and easily overlooked, and the computational efficiency still needs optimization on some hardware platforms. furthermore, the adaptability of the current model in extreme field scenarios, such as high-density occlusion and compound damage from pests and diseases, remains to be further verified. in these scenarios, the coupling of complex interference factors exacerbates feature confusion, affecting grading reliability.\u003c/p>\u003cp class=\"mb15\">to address these issues, future work will focus on the following research directions. future research will focus on enhancing the model&#x2019;s performance in complex field environments through multiple synergistic strategies. this includes constructing multi-interference factor coupled datasets that incorporating insect holes, lesions, and soil adhesion to strengthen robustness against extreme field disturbances; introducing attention-based multi-damage feature decoupling modules and dedicated micro-lesion enhancement modules combining super-resolution and feature interpolation to improve discriminability in complex scenarios and for tiny lesions;&#xa0;optimizing dynamic weight calculations via approximate computation or hardware-friendly reconstruction, while developing lightweight real-time deployment frameworks integrated with uav near-ground sensing technology to boost efficiency and practical monitoring coverage; and establishing cross-crop pathological transfer learning mechanisms, extending the model to multi-crop disease recognition tasks with self-supervised pre-training to enhance algorithm universality and low-contrast lesion feature mining capabilities. through the above research, it is expected to further improve the applicability and practicality of the model in complex field environments, promoting the development of intelligent plant disease grading technology towards more accurate, efficient, and universal directions.\u003c/p>\u003ca id=\"h7\" name=\"h7\">\u003c/a>\u003ch2>data availability statement\u003c/h2>\u003cp class=\"mb0\">the raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.\u003c/p>\u003ca id=\"h8\" name=\"h8\">\u003c/a>\u003ch2>author contributions\u003c/h2>\u003cp class=\"mb0\">yw: writing &#x2013; original draft. dz: writing &#x2013; original draft. lz:&#xa0;writing &#x2013; review &amp; editing.\u003c/p>\u003ca id=\"h9\" name=\"h9\">\u003c/a>\u003ch2>funding\u003c/h2>\u003cp class=\"mb0\">the author(s) declare financial support was received for the research and/or publication of this article. this work was supported in part by the science and technology key project of the xinjiang production and construction corps (2023ab063) development and application demonstration of green technology for online monitoring of fresh fruits and cold chain logistics in xinjiang.\u003c/p>\u003ca id=\"h10\" name=\"h10\">\u003c/a>\u003ch2>conflict of interest\u003c/h2>\u003cp class=\"mb0\">the authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\u003c/p>\u003ca id=\"h11\" name=\"h11\">\u003c/a>\u003ch2>generative ai statement\u003c/h2>\u003cp class=\"mb15\">the author(s) declare that no generative ai was used in the creation of this manuscript.\u003c/p>\u003cp class=\"mb0\">any alternative text (alt text) provided alongside figures in this article has been generated by frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. if you identify any issues, please contact us.\u003c/p>\u003ca id=\"h12\" name=\"h12\">\u003c/a>\u003ch2>publisher&#x2019;s note\u003c/h2>\u003cp class=\"mb15\">all claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.\u003c/p>\u003ca id=\"h13\" name=\"h13\">\u003c/a>\u003ch2>references\u003c/h2>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b1\" id=\"b1\">\u003c/a> acharya, p., burgers, t., and nguyen, k.-d. (2022). ai-enabled droplet detection and tracking for agricultural spraying systems. \u003ci>computers and electronics in agriculture\u003c/i>, 202, 107325. doi:&#xa0;10.1016/j.compag.2022.107325\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2022.107325\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=p.+acharya&amp;author=t.+burgers&amp;author=k.-d.+nguyen&amp;publication_year=2022&amp;title=ai-enabled%20droplet%20detection%20and%20tracking%20for%20agricultural%20spraying%20systems&amp;journal=computers+and+electronics+in+agriculture&amp;volume=202&amp;pages=107325\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b2\" id=\"b2\">\u003c/a> adaskaveg, j., f&#xf6;rster, h., thompson, d., driever, g., connell, j., buchner, r., et al. (2009). epidemiology and management of walnut blight. \u003ci>walnut res. rep\u003c/i>, 94, 225&#x2013;256. doi:&#xa0;10.1016/j.inpa.2016.10.005\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.inpa.2016.10.005\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.+adaskaveg&amp;author=h.+f%c3%b6rster&amp;author=d.+thompson&amp;author=g.+driever&amp;author=j.+connell&amp;author=r.+buchner&amp;publication_year=2009&amp;title=epidemiology%20and%20management%20of%20walnut%20blight&amp;journal=walnut+res.+rep&amp;volume=94&amp;pages=225\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b3\" id=\"b3\">\u003c/a> arivazhagan, s., shebiah, r. n., ananthi, s., and varthini, s. v. (2013). detection of unhealthy region of plant leaves and classification of plant leaf diseases using texture features. \u003ci>agricultural engineering international: cigr journal\u003c/i>, 15, 211&#x2013;217.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+arivazhagan&amp;author=r.%20n.+shebiah&amp;author=s.+ananthi&amp;author=s.%20v.+varthini&amp;publication_year=2013&amp;title=detection%20of%20unhealthy%20region%20of%20plant%20leaves%20and%20classification%20of%20plant%20leaf%20diseases%20using%20texture%20features&amp;journal=agricultural+engineering+international:+cigr+journal&amp;volume=15&amp;pages=211\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b4\" id=\"b4\">\u003c/a> chaudhary, p., chaudhari, a. k., cheeran, a., and godara, s. (2012). color transform based approach for disease spot detection on plant leaf. \u003ci>international journal of computer science and telecommunications\u003c/i>, 3, 65&#x2013;70.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=p.+chaudhary&amp;author=a.%20k.+chaudhari&amp;author=a.+cheeran&amp;author=s.+godara&amp;publication_year=2012&amp;title=color%20transform%20based%20approach%20for%20disease%20spot%20detection%20on%20plant%20leaf&amp;journal=international+journal+of+computer+science+and+telecommunications&amp;volume=3&amp;pages=65\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b5\" id=\"b5\">\u003c/a> chen, s., zhang, k., zhao, y., sun, y., ban, w., chen, y., et al. (2021). an approach for rice bacterial leaf streak disease segmentation and disease severity estimation. \u003ci>agriculture\u003c/i>, 11, 420. doi:&#xa0;10.3390/agriculture11050420\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agriculture11050420\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+chen&amp;author=k.+zhang&amp;author=y.+zhao&amp;author=y.+sun&amp;author=w.+ban&amp;author=y.+chen&amp;publication_year=2021&amp;title=an%20approach%20for%20rice%20bacterial%20leaf%20streak%20disease%20segmentation%20and%20disease%20severity%20estimation&amp;journal=agriculture&amp;volume=11&amp;pages=420\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b6\" id=\"b6\">\u003c/a> chiang, k.-s., bock, c., el jarroudi, m., delfosse, p., lee, i., and liu, h. (2016). effects of rater bias and assessment method on disease severity estimation with regard to hypothesis testing. \u003ci>phytopathology\u003c/i>, 65, 523&#x2013;535. doi:&#xa0;10.1111/ppa.12435\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1111/ppa.12435\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=k.-s.+chiang&amp;author=c.+bock&amp;author=m.+el%20jarroudi&amp;author=p.+delfosse&amp;author=i.+lee&amp;author=h.+liu&amp;publication_year=2016&amp;title=effects%20of%20rater%20bias%20and%20assessment%20method%20on%20disease%20severity%20estimation%20with%20regard%20to%20hypothesis%20testing&amp;journal=phytopathology&amp;volume=65&amp;pages=523\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b7\" id=\"b7\">\u003c/a> cooke, b. (2006). &#x201c;disease assessment and yield loss,&#x201d; in \u003ci>the epidemiology of plant diseases\u003c/i> (dordrecht: springer), 43&#x2013;80.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=b.+cooke&amp;publication_year=2006&amp;title=disease%20assessment%20and%20yield%20loss&amp;book=the+epidemiology+of+plant+diseases&amp;pages=43\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b8\" id=\"b8\">\u003c/a> ghaiwat, s. n. and arora, p. (2014). detection and classification of plant leaf diseases using image processing techniques: a review. \u003ci>international journal of recent advances in engineering &amp; technology\u003c/i>, 2, 1&#x2013;7.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20n.+ghaiwat&amp;author=p.+arora&amp;publication_year=2014&amp;title=detection%20and%20classification%20of%20plant%20leaf%20diseases%20using%20image%20processing%20techniques%3a%20a%20review&amp;journal=international+journal+of+recent+advances+in+engineering+&amp;+technology&amp;volume=2&amp;pages=1\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b9\" id=\"b9\">\u003c/a> hu, g., wang, h., zhang, y., and wan, m. (2021). detection and severity analysis of tea leaf blight based on deep learning. \u003ci>computers &amp; electrical engineering\u003c/i>, 90, 107023. doi:&#xa0;10.1016/j.compeleceng.2021.107023\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compeleceng.2021.107023\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=g.+hu&amp;author=h.+wang&amp;author=y.+zhang&amp;author=m.+wan&amp;publication_year=2021&amp;title=detection%20and%20severity%20analysis%20of%20tea%20leaf%20blight%20based%20on%20deep%20learning&amp;journal=computers+&amp;+electrical+engineering&amp;volume=90&amp;pages=107023\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b10\" id=\"b10\">\u003c/a> jadhav, s. b. and patil, s. b. (2016). grading of soybean leaf disease based on segmented image using k-means clustering. \u003ci>iaes international journal of artificial intelligence\u003c/i>, 5, 13&#x2013;13. doi:&#xa0;10.11591/ijai.v5.i1.pp13-21\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.11591/ijai.v5.i1.pp13-21\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20b.+jadhav&amp;author=s.%20b.+patil&amp;publication_year=2016&amp;title=grading%20of%20soybean%20leaf%20disease%20based%20on%20segmented%20image%20using%20k-means%20clustering&amp;journal=iaes+international+journal+of+artificial+intelligence&amp;volume=5&amp;pages=13\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b11\" id=\"b11\">\u003c/a> jo, w. j., kim, d. s., sim, h. s., ahn, s. r., lee, h. j., moon, y. h., et al. (2021). estimation of evapotranspiration and water requirements of strawberry plants in greenhouses using environmental data. \u003ci>frontiers in sustainable food systems\u003c/i>, 5, 684808. doi:&#xa0;10.3389/fsufs.2021.684808\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3389/fsufs.2021.684808\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=w.%20j.+jo&amp;author=d.%20s.+kim&amp;author=h.%20s.+sim&amp;author=s.%20r.+ahn&amp;author=h.%20j.+lee&amp;author=y.%20h.+moon&amp;publication_year=2021&amp;title=estimation%20of%20evapotranspiration%20and%20water%20requirements%20of%20strawberry%20plants%20in%20greenhouses%20using%20environmental%20data&amp;journal=frontiers+in+sustainable+food+systems&amp;volume=5&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b12\" id=\"b12\">\u003c/a> khan, m. a., ali, m., shah, m., mahmood, t., ahmad, m., jhanjhi, n., et al. (2021). machine learning-based detection and classification of walnut fungi diseases. \u003ci>intelligent automation &amp; soft computing\u003c/i>, 30, 771&#x2013;785. doi:&#xa0;10.32604/iasc.2021.018039\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.32604/iasc.2021.018039\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=m.%20a.+khan&amp;author=m.+ali&amp;author=m.+shah&amp;author=t.+mahmood&amp;author=m.+ahmad&amp;author=n.+jhanjhi&amp;publication_year=2021&amp;title=machine%20learning-based%20detection%20and%20classification%20of%20walnut%20fungi%20diseases&amp;journal=intelligent+automation+&amp;+soft+computing&amp;volume=30&amp;pages=771\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b13\" id=\"b13\">\u003c/a> kim, s.-k. and ahn, j.-g. (2021). tomato crop diseases classification models using deep cnn-based architectures. \u003ci>j korea acad-ind coop soc\u003c/i>, 22, 7&#x2013;14. doi:&#xa0;10.5762/kais.2021.22.5.7\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.5762/kais.2021.22.5.7\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.-k.+kim&amp;author=j.-g.+ahn&amp;publication_year=2021&amp;title=tomato%20crop%20diseases%20classification%20models%20using%20deep%20cnn-based%20architectures&amp;journal=j+korea+acad-ind+coop+soc&amp;volume=22&amp;pages=7\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b14\" id=\"b14\">\u003c/a> lamichhane, j. r. (2014). xanthomonas arboricola diseases of stone fruit, almond, and walnut trees: progress toward understanding and management. \u003ci>plant disease\u003c/i>, 98, 1600&#x2013;1610. doi:&#xa0;10.1094/pdis-08-14-0831-fe\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30703892/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-08-14-0831-fe\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.%20r.+lamichhane&amp;publication_year=2014&amp;title=xanthomonas%20arboricola%20diseases%20of%20stone%20fruit%2c%20almond%2c%20and%20walnut%20trees%3a%20progress%20toward%20understanding%20and%20management&amp;journal=plant+disease&amp;volume=98&amp;pages=1600\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b15\" id=\"b15\">\u003c/a> li, x., zha, t., liu, p., bourque, c. p.-a., jia, x., and tian, y. (2023). interannual variation in gross ecosystem production and evapotranspiration in a temperate semiarid grassland undergoing vegetation recovery. \u003ci>agricultural and forest meteorology\u003c/i>, 341, 109672. doi:&#xa0;10.1016/j.agrformet.2023.109672\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.agrformet.2023.109672\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=x.+li&amp;author=t.+zha&amp;author=p.+liu&amp;author=c.%20p.-a.+bourque&amp;author=x.+jia&amp;author=y.+tian&amp;publication_year=2023&amp;title=interannual%20variation%20in%20gross%20ecosystem%20production%20and%20evapotranspiration%20in%20a%20temperate%20semiarid%20grassland%20undergoing%20vegetation%20recovery&amp;journal=agricultural+and+forest+meteorology&amp;volume=341&amp;pages=109672\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b16\" id=\"b16\">\u003c/a> lin, k., gong, l., huang, y., liu, c., and pan, j. (2019). deep learning-based segmentation and quantification of cucumber powdery mildew using convolutional neural network. \u003ci>frontiers in plant science\u003c/i>, 10, 155. doi:&#xa0;10.3389/fpls.2019.00155\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30891048/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.3389/fpls.2019.00155\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=k.+lin&amp;author=l.+gong&amp;author=y.+huang&amp;author=c.+liu&amp;author=j.+pan&amp;publication_year=2019&amp;title=deep%20learning-based%20segmentation%20and%20quantification%20of%20cucumber%20powdery%20mildew%20using%20convolutional%20neural%20network&amp;journal=frontiers+in+plant+science&amp;volume=10&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b17\" id=\"b17\">\u003c/a> ma, j., du, k., zhang, l., zheng, f., chu, j., and sun, z. (2017). a segmentation method for greenhouse vegetable foliar disease spots images using color information and region growing. \u003ci>computers and electronics in agriculture\u003c/i>, 142, 110&#x2013;117. doi:&#xa0;10.1016/j.compag.2017.08.023\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2017.08.023\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.+ma&amp;author=k.+du&amp;author=l.+zhang&amp;author=f.+zheng&amp;author=j.+chu&amp;author=z.+sun&amp;publication_year=2017&amp;title=a%20segmentation%20method%20for%20greenhouse%20vegetable%20foliar%20disease%20spots%20images%20using%20color%20information%20and%20region%20growing&amp;journal=computers+and+electronics+in+agriculture&amp;volume=142&amp;pages=110\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b18\" id=\"b18\">\u003c/a> mao, r., wang, z., li, f., zhou, j., chen, y., and hu, x. (2023). gseyolox-s: an improved lightweight network for identifying the severity of wheat fusarium head blight. \u003ci>agronomy\u003c/i>, 13, 242. doi:&#xa0;10.3390/agronomy13010242\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agronomy13010242\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=r.+mao&amp;author=z.+wang&amp;author=f.+li&amp;author=j.+zhou&amp;author=y.+chen&amp;author=x.+hu&amp;publication_year=2023&amp;title=gseyolox-s%3a%20an%20improved%20lightweight%20network%20for%20identifying%20the%20severity%20of%20wheat%20fusarium%20head%20blight&amp;journal=agronomy&amp;volume=13&amp;pages=242\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b19\" id=\"b19\">\u003c/a> mcgranahan, g. and leslie, c. (1991). walnuts (juglans). \u003ci>molecules\u003c/i>, 907&#x2013;924. doi:&#xa0;10.17660/actahortic.1991.290.20\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.17660/actahortic.1991.290.20\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=g.+mcgranahan&amp;author=c.+leslie&amp;publication_year=1991&amp;title=walnuts%20%28juglans%29&amp;journal=molecules&amp;pages=907\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b20\" id=\"b20\">\u003c/a> mircetich, s. m., sanborn, r., and ramos, d. (1980). natural spread, graft-transmission, and possible etiology of walnut blackline disease. \u003ci>phytopathology\u003c/i>, 70, 962&#x2013;968. doi:&#xa0;10.1094/phyto-70-962\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/phyto-70-962\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20m.+mircetich&amp;author=r.+sanborn&amp;author=d.+ramos&amp;publication_year=1980&amp;title=natural%20spread%2c%20graft-transmission%2c%20and%20possible%20etiology%20of%20walnut%20blackline%20disease&amp;journal=phytopathology&amp;volume=70&amp;pages=962\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b21\" id=\"b21\">\u003c/a> moragrega, c., matias, j., alet&#xe0;, n., montesinos, e., and rovira, m. (2011). apical necrosis and premature drop of persian (english) walnut fruit caused by xanthomonas arboricola pv. juglandis. \u003ci>plant disease\u003c/i>, 95, 1565&#x2013;1570. doi:&#xa0;10.1094/pdis-03-11-0259\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30732020/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-03-11-0259\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=c.+moragrega&amp;author=j.+matias&amp;author=n.+alet%c3%a0&amp;author=e.+montesinos&amp;author=m.+rovira&amp;publication_year=2011&amp;title=apical%20necrosis%20and%20premature%20drop%20of%20persian%20%28english%29%20walnut%20fruit%20caused%20by%20xanthomonas%20arboricola%20pv.%20juglandis&amp;journal=plant+disease&amp;volume=95&amp;pages=1565\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b22\" id=\"b22\">\u003c/a> ngugi, l. c., abdelwahab, m., and abo-zahhad, m. (2020). tomato leaf segmentation algorithms for mobile phone applications using deep learning. \u003ci>computers and electronics in agriculture\u003c/i>, 178, 105788. doi:&#xa0;10.1016/j.compag.2020.105788\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2020.105788\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=l.%20c.+ngugi&amp;author=m.+abdelwahab&amp;author=m.+abo-zahhad&amp;publication_year=2020&amp;title=tomato%20leaf%20segmentation%20algorithms%20for%20mobile%20phone%20applications%20using%20deep%20learning&amp;journal=computers+and+electronics+in+agriculture&amp;volume=178&amp;pages=105788\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b23\" id=\"b23\">\u003c/a> ozturk, o., sarica, b., and seker, d. z. (2025). interpretable and robust ensemble deep learning framework for tea leaf disease classification. \u003ci>horticulturae\u003c/i>, 11, 437. doi:&#xa0;10.3390/horticulturae11040437\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/horticulturae11040437\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=o.+ozturk&amp;author=b.+sarica&amp;author=d.%20z.+seker&amp;publication_year=2025&amp;title=interpretable%20and%20robust%20ensemble%20deep%20learning%20framework%20for%20tea%20leaf%20disease%20classification&amp;journal=horticulturae&amp;volume=11&amp;pages=437\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b24\" id=\"b24\">\u003c/a> parashar, n., johri, p., khan, a. a., gaur, n., and kadry, s. (2024). an integrated analysis of yield prediction models: a comprehensive review of advancements and challenges. \u003ci>computers, materials &amp; continua\u003c/i>, 80 (1). doi:&#xa0;10.32604/cmc.2024.050240\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.32604/cmc.2024.050240\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=n.+parashar&amp;author=p.+johri&amp;author=a.%20a.+khan&amp;author=n.+gaur&amp;author=s.+kadry&amp;publication_year=2024&amp;title=an%20integrated%20analysis%20of%20yield%20prediction%20models%3a%20a%20comprehensive%20review%20of%20advancements%20and%20challenges&amp;journal=computers,+materials+&amp;+continua&amp;volume=80&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b25\" id=\"b25\">\u003c/a> picon, a., alvarez-gila, a., seitz, m., ortiz-barredo, a., echazarra, j., and johannes, a. (2019). deep convolutional neural networks for mobile capture device-based crop disease classification in the wild. \u003ci>computers and electronics in agriculture\u003c/i>, 161, 280&#x2013;290. doi:&#xa0;10.1016/j.compag.2018.04.002\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2018.04.002\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=a.+picon&amp;author=a.+alvarez-gila&amp;author=m.+seitz&amp;author=a.+ortiz-barredo&amp;author=j.+echazarra&amp;author=a.+johannes&amp;publication_year=2019&amp;title=deep%20convolutional%20neural%20networks%20for%20mobile%20capture%20device-based%20crop%20disease%20classification%20in%20the%20wild&amp;journal=computers+and+electronics+in+agriculture&amp;volume=161&amp;pages=280\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b26\" id=\"b26\">\u003c/a> rastogi, a., arora, r., and sharma, s. (2015). &#x201c;leaf disease detection and grading using computer vision technology &amp; fuzzy logic,&#x201d; in paper presented at the 2015 2nd international conference on signal processing and integrated networks (spin).\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=a.+rastogi&amp;author=r.+arora&amp;author=s.+sharma&amp;publication_year=2015&amp;title=leaf%20disease%20detection%20and%20grading%20using%20computer%20vision%20technology%20%26%20fuzzy%20logic&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b27\" id=\"b27\">\u003c/a> rustia, d. j. a., lee, w.-c., lu, c.-y., wu, y.-f., shih, p.-y., and chen, s.-k. (2023). edge-based wireless imaging system for continuous monitoring of insect pests in a remote outdoor mango orchard. \u003ci>computers and electronics in agriculture\u003c/i>, 211, 108019. doi:&#xa0;10.1016/j.compag.2023.108019\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2023.108019\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=d.%20j.%20a.+rustia&amp;author=w.-c.+lee&amp;author=c.-y.+lu&amp;author=y.-f.+wu&amp;author=p.-y.+shih&amp;author=s.-k.+chen&amp;publication_year=2023&amp;title=edge-based%20wireless%20imaging%20system%20for%20continuous%20monitoring%20of%20insect%20pests%20in%20a%20remote%20outdoor%20mango%20orchard&amp;journal=computers+and+electronics+in+agriculture&amp;volume=211&amp;pages=108019\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b28\" id=\"b28\">\u003c/a> shi, t., liu, y., zheng, x., hu, k., huang, h., liu, h., et al. (2023). recent advances in plant disease severity assessment using convolutional neural networks. \u003ci>scientific reports\u003c/i>, 13, 2336. doi:&#xa0;10.1038/s41598-023-29230-7\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/36759626/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1038/s41598-023-29230-7\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=t.+shi&amp;author=y.+liu&amp;author=x.+zheng&amp;author=k.+hu&amp;author=h.+huang&amp;author=h.+liu&amp;publication_year=2023&amp;title=recent%20advances%20in%20plant%20disease%20severity%20assessment%20using%20convolutional%20neural%20networks&amp;journal=scientific+reports&amp;volume=13&amp;pages=2336\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b29\" id=\"b29\">\u003c/a> singh, u. p., chouhan, s. s., jain, s., and jain, s. (2019). multilayer convolution neural network for the classification of mango leaves infected by anthracnose disease. \u003ci>ieee access\u003c/i>, 7, 43721&#x2013;43729. doi:&#xa0;10.1109/access.2019.2907383\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/access.2019.2907383\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=u.%20p.+singh&amp;author=s.%20s.+chouhan&amp;author=s.+jain&amp;author=s.+jain&amp;publication_year=2019&amp;title=multilayer%20convolution%20neural%20network%20for%20the%20classification%20of%20mango%20leaves%20infected%20by%20anthracnose%20disease&amp;journal=ieee+access&amp;volume=7&amp;pages=43721\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b30\" id=\"b30\">\u003c/a> su, y., wang, j., li, j., wang, l., wang, k., li, a., et al. (2023). spatiotemporal changes and driving factors of reference evapotranspiration and crop evapotranspiration for cotton production in china from 1960 to 2019. \u003ci>frontiers in environmental science\u003c/i>, 11, 1251789. doi:&#xa0;10.3389/fenvs.2023.1251789\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3389/fenvs.2023.1251789\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=y.+su&amp;author=j.+wang&amp;author=j.+li&amp;author=l.+wang&amp;author=k.+wang&amp;author=a.+li&amp;publication_year=2023&amp;title=spatiotemporal%20changes%20and%20driving%20factors%20of%20reference%20evapotranspiration%20and%20crop%20evapotranspiration%20for%20cotton%20production%20in%20china%20from%201960%20to%202019&amp;journal=frontiers+in+environmental+science&amp;volume=11&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b31\" id=\"b31\">\u003c/a> tripathi, r. (2021). &#x201c;a deep learning approach for plant material disease identification,&#x201d; in paper presented at the iop conference series: materials science and engineering.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=r.+tripathi&amp;publication_year=2021&amp;title=a%20deep%20learning%20approach%20for%20plant%20material%20disease%20identification&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b32\" id=\"b32\">\u003c/a> vishnoi, v. k., kumar, k., kumar, b., mohan, s., and khan, a. a. (2022). detection of apple plant diseases using leaf images through convolutional neural network . \u003ci>ieee access\u003c/i>, 11, 6594&#x2013;6609. doi:&#xa0;10.1109/access.2022.3232917\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/access.2022.3232917\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=v.%20k.+vishnoi&amp;author=k.+kumar&amp;author=b.+kumar&amp;author=s.+mohan&amp;author=a.%20a.+khan&amp;publication_year=2022&amp;title=detection%20of%20apple%20plant%20diseases%20using%20leaf%20images%20through%20convolutional%20neural%20network&amp;journal=ieee+access&amp;volume=11&amp;pages=6594\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b33\" id=\"b33\">\u003c/a> wang, f., dun, c., tang, t., duan, y., guo, x., and you, j. (2022). boeremia exigua causes leaf spot of walnut trees (juglans regia) in china. \u003ci>plant disease\u003c/i>, 106, 1993. doi:&#xa0;10.1094/pdis-10-21-2304-pdn\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/pdis-10-21-2304-pdn\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=f.+wang&amp;author=c.+dun&amp;author=t.+tang&amp;author=y.+duan&amp;author=x.+guo&amp;author=j.+you&amp;publication_year=2022&amp;title=boeremia%20exigua%20causes%20leaf%20spot%20of%20walnut%20trees%20%28juglans%20regia%29%20in%20china&amp;journal=plant+disease&amp;volume=106&amp;pages=1993\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b34\" id=\"b34\">\u003c/a> wang, q.-h., fan, k., li, d.-w., han, c.-m., qu, y.-y., qi, y.-k., et al. (2020). identification, virulence and fungicide sensitivity of colletotrichum gloeosporioides ss responsible for walnut anthracnose disease in china. \u003ci>plant disease\u003c/i>, 104, 1358&#x2013;1368. doi:&#xa0;10.1094/pdis-12-19-2569-re\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/32196416/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-12-19-2569-re\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=q.-h.+wang&amp;author=k.+fan&amp;author=d.-w.+li&amp;author=c.-m.+han&amp;author=y.-y.+qu&amp;author=y.-k.+qi&amp;publication_year=2020&amp;title=identification%2c%20virulence%20and%20fungicide%20sensitivity%20of%20colletotrichum%20gloeosporioides%20ss%20responsible%20for%20walnut%20anthracnose%20disease%20in%20china&amp;journal=plant+disease&amp;volume=104&amp;pages=1358\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b35\" id=\"b35\">\u003c/a> weber, b. c. (1980). \u003ci>how to diagnose black walnut damage\u003c/i> vol. 57 (north central forest experiment station, forest service, us department of agriculture).\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=b.%20c.+weber&amp;publication_year=1980&amp;book=how+to+diagnose+black+walnut+damage&amp;volume=57&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b36\" id=\"b36\">\u003c/a> xu, w. (2024). hcf-net: hierarchicalcontextfusion network forinfrared small object detection. \u003ci>arxiv\u003c/i>. arxiv, 2403.10778. doi:&#xa0;10.1109/icme57554.2024.10687431\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/icme57554.2024.10687431\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=w.+xu&amp;publication_year=2024&amp;title=hcf-net%3a%20hierarchicalcontextfusion%20network%20forinfrared%20small%20object%20detection&amp;journal=arxiv&amp;volume=arxiv&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b37\" id=\"b37\">\u003c/a> yang, c., deng, y., wang, f., yang, h., xu, x., zeng, q., et al. (2021). brown leaf spot on \u003ci>juglans sigillata\u003c/i> caused by \u003ci>ophiognomonia leptostyla\u003c/i> in sichuan, china. \u003ci>plant disease\u003c/i>, 105, 4160. doi:&#xa0;10.1094/pdis-02-21-0344-pdn\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/pdis-02-21-0344-pdn\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=c.+yang&amp;author=y.+deng&amp;author=f.+wang&amp;author=h.+yang&amp;author=x.+xu&amp;author=q.+zeng&amp;publication_year=2021&amp;title=brown%20leaf%20spot%20on%20juglans%20sigillata%20caused%20by%20ophiognomonia%20leptostyla%20in%20sichuan%2c%20china&amp;journal=plant+disease&amp;volume=105&amp;pages=4160\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b38\" id=\"b38\">\u003c/a> yang, q., duan, s., and wang, l. (2022). efficient identification of apple leaf diseases in the wild using convolutional neural networks. \u003ci>agronomy\u003c/i>, 12, 2784. doi:&#xa0;10.3390/agronomy12112784\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agronomy12112784\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=q.+yang&amp;author=s.+duan&amp;author=l.+wang&amp;publication_year=2022&amp;title=efficient%20identification%20of%20apple%20leaf%20diseases%20in%20the%20wild%20using%20convolutional%20neural%20networks&amp;journal=agronomy&amp;volume=12&amp;pages=2784\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b39\" id=\"b39\">\u003c/a> zarei, s., taghavi, s. m., banihashemi, z., hamzehzarghani, h., and osdaghi, e. (2019). etiology of leaf spot and fruit canker symptoms on stone fruits and nut trees in iran. \u003ci>journal of plant pathology\u003c/i>, 101, 1133&#x2013;1142. doi:&#xa0;10.1007/s42161-019-00283-w\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1007/s42161-019-00283-w\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+zarei&amp;author=s.%20m.+taghavi&amp;author=z.+banihashemi&amp;author=h.+hamzehzarghani&amp;author=e.+osdaghi&amp;publication_year=2019&amp;title=etiology%20of%20leaf%20spot%20and%20fruit%20canker%20symptoms%20on%20stone%20fruits%20and%20nut%20trees%20in%20iran&amp;journal=journal+of+plant+pathology&amp;volume=101&amp;pages=1133\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b40\" id=\"b40\">\u003c/a> zhang, y., li, x., &#x160;im&#x16f;nek, j., shi, h., chen, n., and hu, q. (2023). quantifying water and salt movement in a soil-plant system of a corn field using hydrus (2d/3d) and the stable isotope method. \u003ci>agricultural water management\u003c/i>, 288, 108492. doi:&#xa0;10.1016/j.agwat.2023.108492\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.agwat.2023.108492\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=y.+zhang&amp;author=x.+li&amp;author=j.+%c5%a0im%c5%afnek&amp;author=h.+shi&amp;author=n.+chen&amp;author=q.+hu&amp;publication_year=2023&amp;title=quantifying%20water%20and%20salt%20movement%20in%20a%20soil-plant%20system%20of%20a%20corn%20field%20using%20hydrus%20%282d%2f3d%29%20and%20the%20stable%20isotope%20method&amp;journal=agricultural+water+management&amp;volume=288&amp;pages=108492\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003c/div>\u003cdiv class=\"thinlinem20\">\u003c/div>\u003cdiv class=\"abstractsummary\">\u003cp>\u003cspan>keywords:\u003c/span> walnut, brown spot disease (\u003ci>ophiognomonia leptostyla\u003c/i>), hierarchical feature selection, edge features perception, adaptive multi-scale dilated convolution, disease grading\u003c/p>\u003cp>\u003cspan>citation:\u003c/span> wei y, zeng d and zheng l (2025) a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution. \u003ci>front. plant sci.\u003c/i> 16:1641677. doi: 10.3389/fpls.2025.1641677\u003c/p>\u003cp id=\"timestamps\">\u003cspan>received:\u003c/span> 05 june 2025; \u003cspan>accepted:\u003c/span> 09 september 2025;\u003cbr>\u003cspan>published:\u003c/span> 03 october 2025.\u003c/p>\u003cdiv>\u003cp>edited by:\u003c/p>\u003ca href=\"https://loop.frontiersin.org/people/1037951\">ravinder kumar\u003c/a>, indian agricultural research institute (icar), india\u003c/div>\u003cdiv>\u003cp>reviewed by:\u003c/p>\u003ca href=\"https://loop.frontiersin.org/people/2082015\">geza bujdoso\u003c/a>, hungarian university of agricultural and life sciences, hungary\u003cbr>\r\n\u003ca href=\"https://loop.frontiersin.org/people/3078480\">vasudha vedula\u003c/a>, university of texas of the permian basin, united states\u003c/div>\u003cp>\u003cspan>copyright\u003c/span> &#xa9; 2025 wei, zeng and zheng. this is an open-access article distributed under the terms of the \u003ca rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\" target=\"_blank\">creative commons attribution license (cc by)\u003c/a>. the use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. no use, distribution or reproduction is permitted which does not comply with these terms.\u003c/p>\u003cp>\u003cspan>*correspondence:\u003c/span> liangfang zheng, \u003ca id=\"encmail\">mtgxnjaynta0othamtyzlmnvbq==\u003c/a>\u003c/p>\u003cp>\u003cspan>\u003csup>&#x2020;\u003c/sup>\u003c/span>these authors have contributed equally to this work\u003c/p>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>","\u003cul class=\"flyoutjournal\">\u003cli>\u003ca href=\"#h1\">abstract\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h2\">1 introduction\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h3\">2 materials and methods\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h4\">3 experiments and results analysis\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h5\">4 conclusion\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h6\">5 discussion and future work\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h7\">data availability statement\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h8\">author contributions\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h9\">funding\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h10\">conflict of interest\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h11\">generative ai statement\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h12\">publisher&#x2019;s note\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h13\">references\u003c/a>\u003c/li>\u003c/ul>",[629,636,642],{"name":630,"fileserverpackageentryid":19,"fileserverid":631,"fileserverversionnumber":632,"type":633},"epub.epub","1641677/epub",2,{"code":634,"name":635},"epub","epub",{"name":637,"fileserverpackageentryid":637,"fileserverid":638,"fileserverversionnumber":632,"type":639},"fpls-16-1641677.xml","1641677/xml",{"code":640,"name":641},"nlm_xml","xml",{"name":643,"fileserverpackageentryid":19,"fileserverid":644,"fileserverversionnumber":632,"type":645},"publishers-proof.pdf","1641677/publishers-proof",{"code":646,"name":647},"pdf","pdf","v3",{"title":650,"link":651,"meta":655,"script":748},"frontiers | a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution",[652],{"rel":653,"href":654},"canonical","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/full",[656,659,662,664,667,671,673,677,680,683,686,688,690,692,694,696,699,702,704,707,709,711,714,717,720,723,726,730,734,737,740,743,745],{"hid":657,"property":657,"name":657,"content":658},"description","walnut leaf brown spot disease, caused by ophiognomonia leptostyla, is among the most destructive fungal diseases in walnut cultivation. in the development o...",{"hid":660,"property":660,"name":661,"content":650},"og:title","title",{"hid":663,"property":663,"name":657,"content":658},"og:description",{"hid":665,"name":665,"content":666},"keywords","walnut,brown spot disease (ophiognomonia leptostyla),hierarchical feature selection,edge features perception,adaptive multi-scale dilated convolution,disease grading",{"hid":668,"property":668,"name":669,"content":670},"og:site_name","site_name","frontiers",{"hid":672,"property":672,"name":385,"content":404},"og:image",{"hid":674,"property":674,"name":675,"content":676},"og:type","type","article",{"hid":678,"property":678,"name":679,"content":654},"og:url","url",{"hid":681,"name":681,"content":682},"twitter:card","summary_large_image",{"hid":684,"name":684,"content":685},"citation_volume","16",{"hid":687,"name":687,"content":116},"citation_journal_title",{"hid":689,"name":689,"content":670},"citation_publisher",{"hid":691,"name":691,"content":611},"citation_journal_abbrev",{"hid":693,"name":693,"content":612},"citation_issn",{"hid":695,"name":695,"content":538},"citation_doi",{"hid":697,"name":697,"content":698},"citation_firstpage","1641677",{"hid":700,"name":700,"content":701},"citation_language","english",{"hid":703,"name":703,"content":539},"citation_title",{"hid":705,"name":705,"content":706},"citation_keywords","walnut; brown spot disease (ophiognomonia leptostyla); hierarchical feature selection; edge features perception; adaptive multi-scale dilated convolution; disease grading",{"hid":708,"name":708,"content":544},"citation_abstract",{"hid":710,"name":710,"content":553},"citation_article_type",{"hid":712,"name":712,"content":713},"citation_pdf_url","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/pdf",{"hid":715,"name":715,"content":716},"citation_xml_url","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/xml",{"hid":718,"name":718,"content":719},"citation_fulltext_world_readable","yes",{"hid":721,"name":721,"content":722},"citation_online_date","2025/09/09",{"hid":724,"name":724,"content":725},"citation_publication_date","2025/10/03",{"hid":727,"name":728,"content":729},"citation_author_0","citation_author","wei, yuting ",{"hid":731,"name":732,"content":733},"citation_author_institution_0","citation_author_institution","college of information engineering, tarim university, china",{"hid":735,"name":728,"content":736},"citation_author_1","zeng, debin ",{"hid":738,"name":732,"content":739},"citation_author_institution_1","key laboratory of tarim oasis agriculture, ministry of education, tarim university, china",{"hid":741,"name":728,"content":742},"citation_author_2","zheng, liangfang ",{"hid":744,"name":732,"content":739},"citation_author_institution_2",{"hid":746,"name":746,"content":747},"dc.identifier","doi:10.3389/fpls.2025.1641677",[749,752,754,756,758],{"src":750,"body":13,"type":751,"async":13},"https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6","text/javascript",{"src":753,"body":13,"type":751,"async":13},"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/mathjax.js?config=tex-mml-am_chtml",{"src":755,"body":13,"type":751,"async":13},"https://d1bxh8uas1mnw7.cloudfront.net/assets/altmetric_badges-f0bc9b243ff5677d05460c1eb71834ca998946d764eb3bc244ab4b18ba50d21e.js",{"src":757,"body":13,"type":751,"async":13},"https://api.altmetric.com/v1/doi/10.3389/fpls.2025.1641677?callback=_altmetric.embed_callback&domain=www.frontiersin.org&key=3c130976ca2b8f2e88f8377633751ba1&cache_until=14-15",{"src":759,"body":13,"type":751,"async":13},"https://crossmark-cdn.crossref.org/widget/v2.0/widget.js",{"articlehubslug":19,"articlehubpage":761,"articlehubarticleslist":762,"canjournalhasarticlehub":367,"articledoilist":763},{},[],[],{"title":19,"image":-1,"breadcrumbs":765,"linkscollection":766,"metricscollection":768},[],{"total":389,"items":767},[],{"total":389,"items":769},[]] window.__nuxt__={};window.__nuxt__.config={public:{isdevmode:false,appname:"article-pages-2024",baseurl:"https://www.frontiersin.org",domain:"frontiersin.org",loopurl:"https://loop.frontiersin.org",ssmaindomain:"frontiersin.org",contentfulurl:"https://graphql.contentful.com/content/v1/spaces",spaceid:1,spacename:"frontiers",livespaceid:1,tenantlogourl:"",frontiersgraphurl:"https://apollo-federation-gateway.frontiersin.org",registrationapiurl:"https://onboarding-ui.frontiersin.org",emaildigestapiurl:"https://api-email-digest.frontiersin.org",journalfilesapiurl:"https://files-journal-api.frontiersin.org",searchapiurl:"https://search-api.frontiersin.org",productionforumapiurl:"https://production-forum-api-v2.frontiersin.org",gtmserverurl:"https://tag-manager.frontiersin.org",gtmid:"gtm-m322fv2",gtmauth:"owvbwxfajr21yqv1fe1caq",gtmpreview:"env-1",figsharepluginurl:"https://widgets.figshare.com/static/figshare.js",figshareapiurl:"https://api.figshare.com/v2/collections/search",figsharetimeout:3000,crossmarkpublisheddate:"2015/08/24",googlerecaptchasitekey:"6ldg3i0uaaaaaoc4quh35ubhgjotehp_stxhgr_v",googlerecaptchakeyname:"frontiersrecaptchav2",faviconsize16:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_16-tenantfavicon-frontiers.png",faviconsize32:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_32-tenantfavicon-frontiers.png",faviconsize180:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_180-tenantfavicon-frontiers.png",faviconsize192:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_192-tenantfavicon-frontiers.png",faviconsize512:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_512-tenantfavicon-frontiers.png",socialmediaimg:"https://brand.frontiersin.org/m/1c8bcb536c789e11/guidelines-frontiers_logo_1200x628_1-91to1.png",linkedarticlecopytext:"'{\"articletypecopytext\":[{\"articletypeid\":0,\"originalarticlecopytext\":\"part of this article's content has been mentioned in:\",\"linkedarticlecopytext\":\"this article mentions parts of:\"},{\"articletypeid\":122,\"originalarticlecopytext\":\"parts of this article's content have been modified or rectified in:\",\"linkedarticlecopytext\":\"this article is an erratum on:\"},{\"articletypeid\":129,\"originalarticlecopytext\":\"parts of this article's content have been modified or rectified in:\",\"linkedarticlecopytext\":\"this article is an addendum to:\"},{\"articletypeid\":128,\"originalarticlecopytext\":\"a correction has been applied to this article in:\",\"linkedarticlecopytext\":\"this article is a correction to:\"},{\"articletypeid\":134,\"originalarticlecopytext\":\"a retraction of this article was approved in:\",\"linkedarticlecopytext\":\"this article is a retraction of:\"},{\"articletypeid\":29,\"originalarticlecopytext\":\"a commentary has been posted on this article:\",\"linkedarticlecopytext\":\"this article is a commentary on:\"},{\"articletypeid\":30,\"originalarticlecopytext\":\"a commentary has been posted on this article:\",\"linkedarticlecopytext\":\"this article is a commentary on:\"}],\"articleidcopytext\":[]}'\n",acceptedarticleexcludedjournalids:"'[2766,979]'\n",articletypeconfigurablelabel:"\u003c\u003carticle-type:uppercase>> article",settingsfeaturesswitchers:"'{\"displaytitlepilllabels\":true,\"displayrelatedarticlesbox\":true,\"showeditors\":true,\"showreviewers\":true,\"showloopimpactlink\":true,\"enablefigshare\":false,\"usexmlimages\":true}'\n",journalswitharticlehub:"'{\"strings\":[\"science\"]}'\n",terminologysettings:"'{\"terms\":[{\"sequencenumber\":1,\"key\":\"frontiers\",\"tenantterm\":\"frontiers\",\"frontiersdefaultterm\":\"frontiers\",\"category\":\"customer\"},{\"sequencenumber\":2,\"key\":\"submission_system\",\"tenantterm\":\"submission system\",\"frontiersdefaultterm\":\"submission system\",\"category\":\"product\"},{\"sequencenumber\":3,\"key\":\"public_pages\",\"tenantterm\":\"public pages\",\"frontiersdefaultterm\":\"public pages\",\"category\":\"product\"},{\"sequencenumber\":4,\"key\":\"my_frontiers\",\"tenantterm\":\"my frontiers\",\"frontiersdefaultterm\":\"my frontiers\",\"category\":\"product\"},{\"sequencenumber\":5,\"key\":\"digital_editorial_office\",\"tenantterm\":\"digital editorial office\",\"frontiersdefaultterm\":\"digital editorial office\",\"category\":\"product\"},{\"sequencenumber\":6,\"key\":\"deo\",\"tenantterm\":\"deo\",\"frontiersdefaultterm\":\"deo\",\"category\":\"product\"},{\"sequencenumber\":7,\"key\":\"digital_editorial_office_for_chiefs\",\"tenantterm\":\"digital editorial office for chiefs\",\"frontiersdefaultterm\":\"digital editorial office for chiefs\",\"category\":\"product\"},{\"sequencenumber\":8,\"key\":\"digital_editorial_office_for_eof\",\"tenantterm\":\"digital editorial office for eof\",\"frontiersdefaultterm\":\"digital editorial office for eof\",\"category\":\"product\"},{\"sequencenumber\":9,\"key\":\"editorial_office\",\"tenantterm\":\"editorial office\",\"frontiersdefaultterm\":\"editorial office\",\"category\":\"product\"},{\"sequencenumber\":10,\"key\":\"eof\",\"tenantterm\":\"eof\",\"frontiersdefaultterm\":\"eof\",\"category\":\"product\"},{\"sequencenumber\":11,\"key\":\"research_topic_management\",\"tenantterm\":\"research topic management\",\"frontiersdefaultterm\":\"research topic management\",\"category\":\"product\"},{\"sequencenumber\":12,\"key\":\"review_forum\",\"tenantterm\":\"review forum\",\"frontiersdefaultterm\":\"review forum\",\"category\":\"product\"},{\"sequencenumber\":13,\"key\":\"accounting_office\",\"tenantterm\":\"accounting office\",\"frontiersdefaultterm\":\"accounting office\",\"category\":\"product\"},{\"sequencenumber\":14,\"key\":\"aof\",\"tenantterm\":\"aof\",\"frontiersdefaultterm\":\"aof\",\"category\":\"product\"},{\"sequencenumber\":15,\"key\":\"publishing_office\",\"tenantterm\":\"publishing office\",\"frontiersdefaultterm\":\"publishing office\",\"category\":\"product\"},{\"sequencenumber\":16,\"key\":\"production_office\",\"tenantterm\":\"production office forum\",\"frontiersdefaultterm\":\"production office forum\",\"category\":\"product\"},{\"sequencenumber\":17,\"key\":\"pof\",\"tenantterm\":\"pof\",\"frontiersdefaultterm\":\"pof\",\"category\":\"product\"},{\"sequencenumber\":18,\"key\":\"book_office_forum\",\"tenantterm\":\"book office forum\",\"frontiersdefaultterm\":\"book office forum\",\"category\":\"product\"},{\"sequencenumber\":19,\"key\":\"bof\",\"tenantterm\":\"bof\",\"frontiersdefaultterm\":\"bof\",\"category\":\"product\"},{\"sequencenumber\":20,\"key\":\"aira\",\"tenantterm\":\"aira\",\"frontiersdefaultterm\":\"aira\",\"category\":\"product\"},{\"sequencenumber\":21,\"key\":\"editorial_board_management\",\"tenantterm\":\"editorial board management\",\"frontiersdefaultterm\":\"editorial board management\",\"category\":\"product\"},{\"sequencenumber\":22,\"key\":\"ebm\",\"tenantterm\":\"ebm\",\"frontiersdefaultterm\":\"ebm\",\"category\":\"product\"},{\"sequencenumber\":23,\"key\":\"domain\",\"tenantterm\":\"domain\",\"frontiersdefaultterm\":\"domain\",\"category\":\"taxonomy\"},{\"sequencenumber\":24,\"key\":\"journal\",\"tenantterm\":\"journal\",\"frontiersdefaultterm\":\"journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":25,\"key\":\"section\",\"tenantterm\":\"section\",\"frontiersdefaultterm\":\"section\",\"category\":\"taxonomy\"},{\"sequencenumber\":26,\"key\":\"domains\",\"tenantterm\":\"domains\",\"frontiersdefaultterm\":\"domains\",\"category\":\"taxonomy\"},{\"sequencenumber\":27,\"key\":\"specialty_section\",\"tenantterm\":\"specialty section\",\"frontiersdefaultterm\":\"specialty section\",\"category\":\"taxonomy\"},{\"sequencenumber\":28,\"key\":\"specialty_journal\",\"tenantterm\":\"specialty journal\",\"frontiersdefaultterm\":\"specialty journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":29,\"key\":\"journals\",\"tenantterm\":\"journals\",\"frontiersdefaultterm\":\"journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":30,\"key\":\"sections\",\"tenantterm\":\"sections\",\"frontiersdefaultterm\":\"sections\",\"category\":\"taxonomy\"},{\"sequencenumber\":31,\"key\":\"specialty_sections\",\"tenantterm\":\"specialty sections\",\"frontiersdefaultterm\":\"specialty sections\",\"category\":\"taxonomy\"},{\"sequencenumber\":32,\"key\":\"specialty_journals\",\"tenantterm\":\"specialty journals\",\"frontiersdefaultterm\":\"specialty journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":33,\"key\":\"manuscript\",\"tenantterm\":\"manuscript\",\"frontiersdefaultterm\":\"manuscript\",\"category\":\"core\"},{\"sequencenumber\":34,\"key\":\"manuscripts\",\"tenantterm\":\"manuscripts\",\"frontiersdefaultterm\":\"manuscripts\",\"category\":\"core\"},{\"sequencenumber\":35,\"key\":\"article\",\"tenantterm\":\"article\",\"frontiersdefaultterm\":\"article\",\"category\":\"core\"},{\"sequencenumber\":36,\"key\":\"articles\",\"tenantterm\":\"articles\",\"frontiersdefaultterm\":\"articles\",\"category\":\"core\"},{\"sequencenumber\":37,\"key\":\"article_type\",\"tenantterm\":\"article type\",\"frontiersdefaultterm\":\"article type\",\"category\":\"core\"},{\"sequencenumber\":38,\"key\":\"article_types\",\"tenantterm\":\"article types\",\"frontiersdefaultterm\":\"article types\",\"category\":\"core\"},{\"sequencenumber\":39,\"key\":\"author\",\"tenantterm\":\"author\",\"frontiersdefaultterm\":\"author\",\"category\":\"label (role)\"},{\"sequencenumber\":40,\"key\":\"authors\",\"tenantterm\":\"authors\",\"frontiersdefaultterm\":\"authors\",\"category\":\"label (role)\"},{\"sequencenumber\":41,\"key\":\"authoring\",\"tenantterm\":\"authoring\",\"frontiersdefaultterm\":\"authoring\",\"category\":\"core\"},{\"sequencenumber\":42,\"key\":\"authored\",\"tenantterm\":\"authored\",\"frontiersdefaultterm\":\"authored\",\"category\":\"core\"},{\"sequencenumber\":43,\"key\":\"accept\",\"tenantterm\":\"accept\",\"frontiersdefaultterm\":\"accept\",\"category\":\"process\"},{\"sequencenumber\":44,\"key\":\"accepted\",\"tenantterm\":\"accepted\",\"frontiersdefaultterm\":\"accepted\",\"category\":\"process\"},{\"sequencenumber\":45,\"key\":\"assistant_field_chief_editor\",\"tenantterm\":\"assistant field chief editor\",\"frontiersdefaultterm\":\"assistant field chief editor\",\"description\":\"an editorial role on a field journal that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":46,\"key\":\"assistant_specialty_chief_editor\",\"tenantterm\":\"assistant specialty chief editor\",\"frontiersdefaultterm\":\"assistant specialty chief editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":47,\"key\":\"assistant_specialty_chief_editors\",\"tenantterm\":\"assistant specialty chief editors\",\"frontiersdefaultterm\":\"assistant specialty chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":48,\"key\":\"associate_editor\",\"tenantterm\":\"associate editor\",\"frontiersdefaultterm\":\"associate editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":49,\"key\":\"specialty_chief_editor\",\"tenantterm\":\"specialty chief editor\",\"frontiersdefaultterm\":\"specialty chief editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":50,\"key\":\"specialty_chief_editors\",\"tenantterm\":\"specialty chief editors\",\"frontiersdefaultterm\":\"specialty chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":51,\"key\":\"chief_editor\",\"tenantterm\":\"chief editor\",\"frontiersdefaultterm\":\"chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":52,\"key\":\"chief_editors\",\"tenantterm\":\"chief editors\",\"frontiersdefaultterm\":\"chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":53,\"key\":\"call_for_participation\",\"tenantterm\":\"call for participation\",\"frontiersdefaultterm\":\"call for participation\",\"category\":\"process\"},{\"sequencenumber\":54,\"key\":\"citation\",\"tenantterm\":\"citation\",\"frontiersdefaultterm\":\"citation\",\"category\":\"misc.\"},{\"sequencenumber\":55,\"key\":\"citations\",\"tenantterm\":\"citations\",\"frontiersdefaultterm\":\"citations\",\"category\":\"misc.\"},{\"sequencenumber\":56,\"key\":\"contributor\",\"tenantterm\":\"contributor\",\"frontiersdefaultterm\":\"contributor\",\"category\":\"label (role)\"},{\"sequencenumber\":57,\"key\":\"contributors\",\"tenantterm\":\"contributors\",\"frontiersdefaultterm\":\"contributors\",\"category\":\"label (role)\"},{\"sequencenumber\":58,\"key\":\"corresponding_author\",\"tenantterm\":\"corresponding author\",\"frontiersdefaultterm\":\"corresponding author\",\"category\":\"label (role)\"},{\"sequencenumber\":59,\"key\":\"corresponding_authors\",\"tenantterm\":\"corresponding authors\",\"frontiersdefaultterm\":\"corresponding authors\",\"category\":\"label (role)\"},{\"sequencenumber\":60,\"key\":\"decline\",\"tenantterm\":\"decline\",\"frontiersdefaultterm\":\"decline\",\"category\":\"process\"},{\"sequencenumber\":61,\"key\":\"declined\",\"tenantterm\":\"declined\",\"frontiersdefaultterm\":\"declined\",\"category\":\"process\"},{\"sequencenumber\":62,\"key\":\"reject\",\"tenantterm\":\"reject\",\"frontiersdefaultterm\":\"reject\",\"category\":\"process\"},{\"sequencenumber\":63,\"key\":\"rejected\",\"tenantterm\":\"rejected\",\"frontiersdefaultterm\":\"rejected\",\"category\":\"process\"},{\"sequencenumber\":64,\"key\":\"publish\",\"tenantterm\":\"publish\",\"frontiersdefaultterm\":\"publish\",\"category\":\"core\"},{\"sequencenumber\":65,\"key\":\"published\",\"tenantterm\":\"published\",\"frontiersdefaultterm\":\"published\",\"category\":\"core\"},{\"sequencenumber\":66,\"key\":\"publication\",\"tenantterm\":\"publication\",\"frontiersdefaultterm\":\"publication\",\"category\":\"core\"},{\"sequencenumber\":67,\"key\":\"peer_review\",\"tenantterm\":\"peer review\",\"frontiersdefaultterm\":\"peer review\",\"category\":\"peer review process\"},{\"sequencenumber\":68,\"key\":\"peer_reviewed\",\"tenantterm\":\"peer reviewed\",\"frontiersdefaultterm\":\"peer reviewed\",\"category\":\"peer review process\"},{\"sequencenumber\":69,\"key\":\"initial_validation\",\"tenantterm\":\"initial validation\",\"frontiersdefaultterm\":\"initial validation\",\"category\":\"peer review process\"},{\"sequencenumber\":70,\"key\":\"editorial_assignment\",\"tenantterm\":\"editorial assignment\",\"frontiersdefaultterm\":\"editorial assignment\",\"category\":\"peer review process\"},{\"sequencenumber\":71,\"key\":\"independent_review\",\"tenantterm\":\"independent review\",\"frontiersdefaultterm\":\"independent review\",\"category\":\"peer review process\"},{\"sequencenumber\":72,\"key\":\"interactive_review\",\"tenantterm\":\"interactive review\",\"frontiersdefaultterm\":\"interactive review\",\"category\":\"peer review process\"},{\"sequencenumber\":73,\"key\":\"review\",\"tenantterm\":\"review\",\"frontiersdefaultterm\":\"review\",\"category\":\"peer review process\"},{\"sequencenumber\":74,\"key\":\"reviewing\",\"tenantterm\":\"reviewing\",\"frontiersdefaultterm\":\"reviewing\",\"category\":\"peer review process\"},{\"sequencenumber\":75,\"key\":\"reviewer\",\"tenantterm\":\"reviewer\",\"frontiersdefaultterm\":\"reviewer\",\"category\":\"label (role)\"},{\"sequencenumber\":76,\"key\":\"reviewers\",\"tenantterm\":\"reviewers\",\"frontiersdefaultterm\":\"reviewers\",\"category\":\"label (role)\"},{\"sequencenumber\":77,\"key\":\"review_finalized\",\"tenantterm\":\"review finalized\",\"frontiersdefaultterm\":\"review finalized\",\"category\":\"peer review process\"},{\"sequencenumber\":78,\"key\":\"final_decision\",\"tenantterm\":\"final decision\",\"frontiersdefaultterm\":\"final decision\",\"category\":\"peer review process\"},{\"sequencenumber\":79,\"key\":\"final_validation\",\"tenantterm\":\"final validation\",\"frontiersdefaultterm\":\"final validation\",\"category\":\"peer review process\"},{\"sequencenumber\":80,\"key\":\"ae_accept_manuscript\",\"tenantterm\":\"recommend to accept manuscript\",\"frontiersdefaultterm\":\"accept manuscript\",\"category\":\"process\"},{\"sequencenumber\":81,\"key\":\"fee\",\"tenantterm\":\"fee\",\"frontiersdefaultterm\":\"fee\",\"category\":\"accounting\"},{\"sequencenumber\":82,\"key\":\"fees\",\"tenantterm\":\"fees\",\"frontiersdefaultterm\":\"fees\",\"category\":\"accounting\"},{\"sequencenumber\":83,\"key\":\"guest_associate_editor\",\"tenantterm\":\"guest associate editor\",\"frontiersdefaultterm\":\"guest associate editor\",\"category\":\"label (role)\"},{\"sequencenumber\":84,\"key\":\"guest_associate_editors\",\"tenantterm\":\"guest associate editors\",\"frontiersdefaultterm\":\"guest associate editors\",\"category\":\"label (role)\"},{\"sequencenumber\":85,\"key\":\"in_review\",\"tenantterm\":\"in review\",\"frontiersdefaultterm\":\"in review\",\"category\":\"peer review process\"},{\"sequencenumber\":86,\"key\":\"institutional_member\",\"tenantterm\":\"institutional partner\",\"frontiersdefaultterm\":\"institutional partner\",\"category\":\"accounting\"},{\"sequencenumber\":87,\"key\":\"institutional_membership\",\"tenantterm\":\"institutional partnership\",\"frontiersdefaultterm\":\"institutional partnership\",\"category\":\"accounting\"},{\"sequencenumber\":88,\"key\":\"article_processing_charge\",\"tenantterm\":\"article processing charge\",\"frontiersdefaultterm\":\"article processing charge\",\"category\":\"accounting\"},{\"sequencenumber\":89,\"key\":\"article_processing_charges\",\"tenantterm\":\"article processing charges\",\"frontiersdefaultterm\":\"article processing charges\",\"category\":\"accounting\"},{\"sequencenumber\":90,\"key\":\"apcs\",\"tenantterm\":\"apcs\",\"frontiersdefaultterm\":\"apcs\",\"category\":\"accounting\"},{\"sequencenumber\":91,\"key\":\"apc\",\"tenantterm\":\"apc\",\"frontiersdefaultterm\":\"apc\",\"category\":\"accounting\"},{\"sequencenumber\":92,\"key\":\"received\",\"tenantterm\":\"received\",\"frontiersdefaultterm\":\"received\",\"description\":\"date manuscript was received on.\",\"category\":\"core\"},{\"sequencenumber\":93,\"key\":\"transferred\",\"tenantterm\":\"transferred\",\"frontiersdefaultterm\":\"transferred\",\"category\":\"core\"},{\"sequencenumber\":94,\"key\":\"transfer\",\"tenantterm\":\"transfer\",\"frontiersdefaultterm\":\"transfer\",\"category\":\"core\"},{\"sequencenumber\":95,\"key\":\"research_topic\",\"tenantterm\":\"research topic\",\"frontiersdefaultterm\":\"research topic\",\"category\":\"core\"},{\"sequencenumber\":96,\"key\":\"research_topics\",\"tenantterm\":\"research topics\",\"frontiersdefaultterm\":\"research topics\",\"category\":\"core\"},{\"sequencenumber\":97,\"key\":\"topic_editor\",\"tenantterm\":\"topic editor\",\"frontiersdefaultterm\":\"topic editor\",\"category\":\"label (role)\"},{\"sequencenumber\":98,\"key\":\"review_editor\",\"tenantterm\":\"community reviewer\",\"frontiersdefaultterm\":\"review editor\",\"category\":\"label (role)\"},{\"sequencenumber\":99,\"key\":\"title\",\"tenantterm\":\"title\",\"frontiersdefaultterm\":\"title\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":100,\"key\":\"running_title\",\"tenantterm\":\"running title\",\"frontiersdefaultterm\":\"running title\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":101,\"key\":\"submit\",\"tenantterm\":\"submit\",\"frontiersdefaultterm\":\"submit\",\"category\":\"process\"},{\"sequencenumber\":102,\"key\":\"submitted\",\"tenantterm\":\"submitted\",\"frontiersdefaultterm\":\"submitted\",\"category\":\"process\"},{\"sequencenumber\":103,\"key\":\"submitting\",\"tenantterm\":\"submitting\",\"frontiersdefaultterm\":\"submitting\",\"category\":\"process\"},{\"sequencenumber\":104,\"key\":\"t_e\",\"tenantterm\":\"te\",\"frontiersdefaultterm\":\"te\",\"category\":\"label (role)\"},{\"sequencenumber\":105,\"key\":\"topic\",\"tenantterm\":\"topic\",\"frontiersdefaultterm\":\"topic\",\"category\":\"process\"},{\"sequencenumber\":106,\"key\":\"topic_summary\",\"tenantterm\":\"topic summary\",\"frontiersdefaultterm\":\"topic summary\",\"category\":\"process\"},{\"sequencenumber\":107,\"key\":\"figure\",\"tenantterm\":\"figure\",\"frontiersdefaultterm\":\"figure\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":108,\"key\":\"figures\",\"tenantterm\":\"figures\",\"frontiersdefaultterm\":\"figures\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":109,\"key\":\"editorial_file\",\"tenantterm\":\"editorial file\",\"frontiersdefaultterm\":\"editorial file\",\"category\":\"core\"},{\"sequencenumber\":110,\"key\":\"editorial_files\",\"tenantterm\":\"editorial files\",\"frontiersdefaultterm\":\"editorial files\",\"category\":\"core\"},{\"sequencenumber\":111,\"key\":\"e_book\",\"tenantterm\":\"e-book\",\"frontiersdefaultterm\":\"e-book\",\"category\":\"core\"},{\"sequencenumber\":112,\"key\":\"organization\",\"tenantterm\":\"organization\",\"frontiersdefaultterm\":\"organization\",\"category\":\"core\"},{\"sequencenumber\":113,\"key\":\"institution\",\"tenantterm\":\"institution\",\"frontiersdefaultterm\":\"institution\",\"category\":\"core\"},{\"sequencenumber\":114,\"key\":\"reference\",\"tenantterm\":\"reference\",\"frontiersdefaultterm\":\"reference\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":115,\"key\":\"references\",\"tenantterm\":\"references\",\"frontiersdefaultterm\":\"references\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":116,\"key\":\"sce\",\"tenantterm\":\"sce\",\"frontiersdefaultterm\":\"sce\",\"description\":\"abbreviation for specialty chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":117,\"key\":\"submission\",\"tenantterm\":\"submission\",\"frontiersdefaultterm\":\"submission\",\"category\":\"process\"},{\"sequencenumber\":118,\"key\":\"submissions\",\"tenantterm\":\"submissions\",\"frontiersdefaultterm\":\"submissions\",\"category\":\"process\"},{\"sequencenumber\":119,\"key\":\"editing\",\"tenantterm\":\"editing\",\"frontiersdefaultterm\":\"editing\",\"category\":\"process\"},{\"sequencenumber\":120,\"key\":\"in_preparation\",\"tenantterm\":\"in preparation\",\"frontiersdefaultterm\":\"in preparation\",\"category\":\"process\"},{\"sequencenumber\":121,\"key\":\"country_region\",\"tenantterm\":\"country/region\",\"frontiersdefaultterm\":\"country/region\",\"description\":\"because of political issues, some of the country listings are actually classified as `regions` and we need to include this. however other clients may not want to do this.\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":122,\"key\":\"countries_regions\",\"tenantterm\":\"countries/regions\",\"frontiersdefaultterm\":\"countries/regions\",\"description\":\"because of political issues, some of the country listings are actually classified as `regions` and we need to include this. however other clients may not want to do this.\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":123,\"key\":\"specialty\",\"tenantterm\":\"specialty\",\"frontiersdefaultterm\":\"specialty\",\"category\":\"core\"},{\"sequencenumber\":124,\"key\":\"specialties\",\"tenantterm\":\"specialties\",\"frontiersdefaultterm\":\"specialties\",\"category\":\"core\"},{\"sequencenumber\":125,\"key\":\"associate_editors\",\"tenantterm\":\"associate editors\",\"frontiersdefaultterm\":\"associate editors\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":126,\"key\":\"reviewed\",\"tenantterm\":\"reviewed\",\"frontiersdefaultterm\":\"reviewed\",\"category\":\"peer review process\"},{\"sequencenumber\":127,\"key\":\"institutional_members\",\"tenantterm\":\"institutional partners\",\"frontiersdefaultterm\":\"institutional partners\",\"category\":\"accounting\"},{\"sequencenumber\":128,\"key\":\"institutional_memberships\",\"tenantterm\":\"institutional partnerships\",\"frontiersdefaultterm\":\"institutional partnerships\",\"category\":\"accounting\"},{\"sequencenumber\":129,\"key\":\"assistant_field_chief_editors\",\"tenantterm\":\"assistant field chief editors\",\"frontiersdefaultterm\":\"assistant field chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":130,\"key\":\"publications\",\"tenantterm\":\"publications\",\"frontiersdefaultterm\":\"publications\",\"category\":\"process\"},{\"sequencenumber\":131,\"key\":\"ae_accepted\",\"tenantterm\":\"recommended acceptance\",\"frontiersdefaultterm\":\"accepted\",\"category\":\"process\"},{\"sequencenumber\":132,\"key\":\"field_journal\",\"tenantterm\":\"field journal\",\"frontiersdefaultterm\":\"field journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":133,\"key\":\"field_journals\",\"tenantterm\":\"field journals\",\"frontiersdefaultterm\":\"field journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":134,\"key\":\"program_manager\",\"tenantterm\":\"program manager\",\"frontiersdefaultterm\":\"program manager\",\"category\":\"label (role)\"},{\"sequencenumber\":135,\"key\":\"journal_manager\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"journal manager\",\"category\":\"label (role)\"},{\"sequencenumber\":136,\"key\":\"journal_specialist\",\"tenantterm\":\"journal specialist\",\"frontiersdefaultterm\":\"journal specialist\",\"category\":\"label (role)\"},{\"sequencenumber\":137,\"key\":\"program_managers\",\"tenantterm\":\"program managers\",\"frontiersdefaultterm\":\"program managers\",\"category\":\"label (role)\"},{\"sequencenumber\":138,\"key\":\"journal_managers\",\"tenantterm\":\"journal managers\",\"frontiersdefaultterm\":\"journal managers\",\"category\":\"label (role)\"},{\"sequencenumber\":139,\"key\":\"journal_specialists\",\"tenantterm\":\"journal specialists\",\"frontiersdefaultterm\":\"journal specialists\",\"category\":\"label (role)\"},{\"sequencenumber\":140,\"key\":\"cover_letter\",\"tenantterm\":\"manuscript contribution to the field\",\"frontiersdefaultterm\":\"manuscript contribution to the field\",\"category\":\"process\"},{\"sequencenumber\":141,\"key\":\"ae_accepted_manuscript\",\"tenantterm\":\"recommended to accept manuscript\",\"frontiersdefaultterm\":\"accepted manuscript\",\"category\":\"process\"},{\"sequencenumber\":142,\"key\":\"recommend_for_rejection\",\"tenantterm\":\"recommend for rejection\",\"frontiersdefaultterm\":\"recommend for rejection\",\"category\":\"process\"},{\"sequencenumber\":143,\"key\":\"recommended_for_rejection\",\"tenantterm\":\"recommended for rejection\",\"frontiersdefaultterm\":\"recommended for rejection\",\"category\":\"process\"},{\"sequencenumber\":144,\"key\":\"ae\",\"tenantterm\":\"ae\",\"frontiersdefaultterm\":\"ae\",\"description\":\"associate editor - board member\",\"category\":\"label (role)\"},{\"sequencenumber\":145,\"key\":\"re\",\"tenantterm\":\"re\",\"frontiersdefaultterm\":\"re\",\"description\":\"review editor\",\"category\":\"label (role)\"},{\"sequencenumber\":146,\"key\":\"rev\",\"tenantterm\":\"rev\",\"frontiersdefaultterm\":\"rev\",\"description\":\"reviewer\",\"category\":\"label (role)\"},{\"sequencenumber\":147,\"key\":\"aut\",\"tenantterm\":\"aut\",\"frontiersdefaultterm\":\"aut\",\"description\":\"author\",\"category\":\"label (role)\"},{\"sequencenumber\":148,\"key\":\"coraut\",\"tenantterm\":\"coraut\",\"frontiersdefaultterm\":\"coraut\",\"description\":\"corresponding author\",\"category\":\"label (role)\"},{\"sequencenumber\":149,\"key\":\"saut\",\"tenantterm\":\"saut\",\"frontiersdefaultterm\":\"saut\",\"description\":\"submitting author\",\"category\":\"label (role)\"},{\"sequencenumber\":150,\"key\":\"coaut\",\"tenantterm\":\"coaut\",\"frontiersdefaultterm\":\"coaut\",\"description\":\"co-author\",\"category\":\"label (role)\"},{\"sequencenumber\":151,\"key\":\"tsof\",\"tenantterm\":\"tsof\",\"frontiersdefaultterm\":\"tsof\",\"description\":\"typesetter\",\"category\":\"label (role)\"},{\"sequencenumber\":152,\"key\":\"typesetting_office\",\"tenantterm\":\"typesetting office\",\"frontiersdefaultterm\":\"typesetting office\",\"category\":\"product\"},{\"sequencenumber\":153,\"key\":\"config\",\"tenantterm\":\"config\",\"frontiersdefaultterm\":\"config\",\"description\":\"configuration office role\",\"category\":\"label (role)\"},{\"sequencenumber\":154,\"key\":\"jm\",\"tenantterm\":\"jm\",\"frontiersdefaultterm\":\"jm\",\"description\":\"journal manager\",\"category\":\"label (role)\"},{\"sequencenumber\":155,\"key\":\"rte\",\"tenantterm\":\"rte\",\"frontiersdefaultterm\":\"rte\",\"description\":\"research topic editor\",\"category\":\"label (role)\"},{\"sequencenumber\":156,\"key\":\"organizations\",\"tenantterm\":\"organizations\",\"frontiersdefaultterm\":\"organizations\",\"category\":\"core\"},{\"sequencenumber\":157,\"key\":\"publishing\",\"tenantterm\":\"publishing\",\"frontiersdefaultterm\":\"publishing\",\"category\":\"core\"},{\"sequencenumber\":158,\"key\":\"acceptance\",\"tenantterm\":\"acceptance\",\"frontiersdefaultterm\":\"acceptance\",\"category\":\"process\"},{\"sequencenumber\":159,\"key\":\"preferred_associate_editor\",\"tenantterm\":\"preferred associate editor\",\"frontiersdefaultterm\":\"preferred associate editor\",\"category\":\"label (role)\"},{\"sequencenumber\":160,\"key\":\"topic_editors\",\"tenantterm\":\"topic editors\",\"frontiersdefaultterm\":\"topic editors\",\"category\":\"label (role)\"},{\"sequencenumber\":161,\"key\":\"institutions\",\"tenantterm\":\"institutions\",\"frontiersdefaultterm\":\"institutions\",\"category\":\"core\"},{\"sequencenumber\":162,\"key\":\"author(s)\",\"tenantterm\":\"author(s)\",\"frontiersdefaultterm\":\"author(s)\",\"category\":\"label (role)\"},{\"sequencenumber\":163,\"key\":\"figure(s)\",\"tenantterm\":\"figure(s)\",\"frontiersdefaultterm\":\"figure(s)\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":164,\"key\":\"co-authors\",\"tenantterm\":\"co-authors\",\"frontiersdefaultterm\":\"co-authors\",\"description\":\"co-authors\",\"category\":\"label (role)\"},{\"sequencenumber\":165,\"key\":\"editorial_board_members\",\"tenantterm\":\"editorial board members\",\"frontiersdefaultterm\":\"editorial board members\",\"description\":\"editorial board members\",\"category\":\"label (role)\"},{\"sequencenumber\":166,\"key\":\"editorial_board\",\"tenantterm\":\"editorial board\",\"frontiersdefaultterm\":\"editorial board\",\"description\":\"editorial board\",\"category\":\"product\"},{\"sequencenumber\":167,\"key\":\"co-authorship\",\"tenantterm\":\"co-authorship\",\"frontiersdefaultterm\":\"co-authorship\",\"description\":\"co-authorship\",\"category\":\"misc.\"},{\"sequencenumber\":168,\"key\":\"role_id_1\",\"tenantterm\":\"registration office\",\"frontiersdefaultterm\":\"registration office\",\"category\":\"user role\"},{\"sequencenumber\":169,\"key\":\"role_id_2\",\"tenantterm\":\"editorial office\",\"frontiersdefaultterm\":\"editorial office\",\"category\":\"user role\"},{\"sequencenumber\":170,\"key\":\"role_id_7\",\"tenantterm\":\"field chief editor\",\"frontiersdefaultterm\":\"field chief editor\",\"category\":\"user role\"},{\"sequencenumber\":171,\"key\":\"role_id_8\",\"tenantterm\":\"assistant field chief editor\",\"frontiersdefaultterm\":\"assistant field chief editor\",\"category\":\"user role\"},{\"sequencenumber\":172,\"key\":\"role_id_9\",\"tenantterm\":\"specialty chief editor\",\"frontiersdefaultterm\":\"specialty chief editor\",\"category\":\"user role\"},{\"sequencenumber\":173,\"key\":\"role_id_10\",\"tenantterm\":\"assistant specialty chief editor\",\"frontiersdefaultterm\":\"assistant specialty chief editor\",\"category\":\"user role\"},{\"sequencenumber\":174,\"key\":\"role_id_11\",\"tenantterm\":\"associate editor\",\"frontiersdefaultterm\":\"associate editor\",\"category\":\"user role\"},{\"sequencenumber\":175,\"key\":\"role_id_12\",\"tenantterm\":\"guest associate editor\",\"frontiersdefaultterm\":\"guest associate editor\",\"category\":\"user role\"},{\"sequencenumber\":176,\"key\":\"role_id_13\",\"tenantterm\":\"review editor\",\"frontiersdefaultterm\":\"review editor\",\"category\":\"user role\"},{\"sequencenumber\":177,\"key\":\"role_id_14\",\"tenantterm\":\"reviewer\",\"frontiersdefaultterm\":\"reviewer\",\"category\":\"user role\"},{\"sequencenumber\":178,\"key\":\"role_id_15\",\"tenantterm\":\"author\",\"frontiersdefaultterm\":\"author\",\"category\":\"user role\"},{\"sequencenumber\":179,\"key\":\"role_id_16\",\"tenantterm\":\"corresponding author\",\"frontiersdefaultterm\":\"corresponding author\",\"category\":\"user role\"},{\"sequencenumber\":180,\"key\":\"role_id_17\",\"tenantterm\":\"submitting author\",\"frontiersdefaultterm\":\"submitting author\",\"category\":\"user role\"},{\"sequencenumber\":181,\"key\":\"role_id_18\",\"tenantterm\":\"co-author\",\"frontiersdefaultterm\":\"co-author\",\"category\":\"user role\"},{\"sequencenumber\":182,\"key\":\"role_id_20\",\"tenantterm\":\"production office\",\"frontiersdefaultterm\":\"production office\",\"category\":\"user role\"},{\"sequencenumber\":183,\"key\":\"role_id_22\",\"tenantterm\":\"typesetting office (typesetter)\",\"frontiersdefaultterm\":\"typesetting office (typesetter)\",\"category\":\"user role\"},{\"sequencenumber\":184,\"key\":\"role_id_24\",\"tenantterm\":\"registered user\",\"frontiersdefaultterm\":\"registered user\",\"category\":\"user role\"},{\"sequencenumber\":185,\"key\":\"role_id_35\",\"tenantterm\":\"job office\",\"frontiersdefaultterm\":\"job office\",\"category\":\"user role\"},{\"sequencenumber\":186,\"key\":\"role_id_41\",\"tenantterm\":\"special event administrator\",\"frontiersdefaultterm\":\"special event administrator\",\"category\":\"user role\"},{\"sequencenumber\":187,\"key\":\"role_id_42\",\"tenantterm\":\"special event reviewer\",\"frontiersdefaultterm\":\"special event reviewer\",\"category\":\"user role\"},{\"sequencenumber\":188,\"key\":\"role_id_43\",\"tenantterm\":\"submit abstract\",\"frontiersdefaultterm\":\"submit abstract\",\"category\":\"user role\"},{\"sequencenumber\":189,\"key\":\"role_id_52\",\"tenantterm\":\"events office\",\"frontiersdefaultterm\":\"events office\",\"category\":\"user role\"},{\"sequencenumber\":190,\"key\":\"role_id_53\",\"tenantterm\":\"event administrator\",\"frontiersdefaultterm\":\"event administrator\",\"category\":\"user role\"},{\"sequencenumber\":191,\"key\":\"role_id_89\",\"tenantterm\":\"content management office\",\"frontiersdefaultterm\":\"content management office\",\"category\":\"user role\"},{\"sequencenumber\":192,\"key\":\"role_id_98\",\"tenantterm\":\"accounting office\",\"frontiersdefaultterm\":\"accounting office\",\"category\":\"user role\"},{\"sequencenumber\":193,\"key\":\"role_id_99\",\"tenantterm\":\"projects\",\"frontiersdefaultterm\":\"projects\",\"category\":\"user role\"},{\"sequencenumber\":194,\"key\":\"role_id_103\",\"tenantterm\":\"configuration office\",\"frontiersdefaultterm\":\"configuration office\",\"category\":\"user role\"},{\"sequencenumber\":195,\"key\":\"role_id_104\",\"tenantterm\":\"beta user\",\"frontiersdefaultterm\":\"beta user\",\"category\":\"user role\"},{\"sequencenumber\":196,\"key\":\"role_id_106\",\"tenantterm\":\"wfconf\",\"frontiersdefaultterm\":\"wfconf\",\"category\":\"user role\"},{\"sequencenumber\":197,\"key\":\"role_id_107\",\"tenantterm\":\"rt management beta user\",\"frontiersdefaultterm\":\"rt management beta user\",\"category\":\"user role\"},{\"sequencenumber\":198,\"key\":\"role_id_108\",\"tenantterm\":\"deo beta user\",\"frontiersdefaultterm\":\"deo beta user\",\"category\":\"user role\"},{\"sequencenumber\":199,\"key\":\"role_id_109\",\"tenantterm\":\"search beta user\",\"frontiersdefaultterm\":\"search beta user\",\"category\":\"user role\"},{\"sequencenumber\":200,\"key\":\"role_id_110\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"journal manager\",\"category\":\"user role\"},{\"sequencenumber\":201,\"key\":\"role_id_111\",\"tenantterm\":\"myfrontiers beta user\",\"frontiersdefaultterm\":\"myfrontiers beta user\",\"category\":\"user role\"},{\"sequencenumber\":202,\"key\":\"role_id_21\",\"tenantterm\":\"copy editor\",\"frontiersdefaultterm\":\"copy editor\",\"category\":\"user role\"},{\"sequencenumber\":203,\"key\":\"role_id_1_abr\",\"tenantterm\":\"rof\",\"frontiersdefaultterm\":\"rof\",\"category\":\"user role\"},{\"sequencenumber\":204,\"key\":\"role_id_2_abr\",\"tenantterm\":\"eof\",\"frontiersdefaultterm\":\"eof\",\"category\":\"user role\"},{\"sequencenumber\":205,\"key\":\"role_id_7_abr\",\"tenantterm\":\"fce\",\"frontiersdefaultterm\":\"fce\",\"category\":\"user role\"},{\"sequencenumber\":206,\"key\":\"role_id_8_abr\",\"tenantterm\":\"afce\",\"frontiersdefaultterm\":\"afce\",\"category\":\"user role\"},{\"sequencenumber\":207,\"key\":\"role_id_9_abr\",\"tenantterm\":\"sce\",\"frontiersdefaultterm\":\"sce\",\"category\":\"user role\"},{\"sequencenumber\":208,\"key\":\"role_id_10_abr\",\"tenantterm\":\"asce\",\"frontiersdefaultterm\":\"asce\",\"category\":\"user role\"},{\"sequencenumber\":209,\"key\":\"role_id_11_abr\",\"tenantterm\":\"ae\",\"frontiersdefaultterm\":\"ae\",\"category\":\"user role\"},{\"sequencenumber\":210,\"key\":\"role_id_12_abr\",\"tenantterm\":\"gae\",\"frontiersdefaultterm\":\"gae\",\"category\":\"user role\"},{\"sequencenumber\":211,\"key\":\"role_id_13_abr\",\"tenantterm\":\"re\",\"frontiersdefaultterm\":\"re\",\"category\":\"user role\"},{\"sequencenumber\":212,\"key\":\"role_id_14_abr\",\"tenantterm\":\"rev\",\"frontiersdefaultterm\":\"rev\",\"category\":\"user role\"},{\"sequencenumber\":213,\"key\":\"role_id_15_abr\",\"tenantterm\":\"aut\",\"frontiersdefaultterm\":\"aut\",\"category\":\"user role\"},{\"sequencenumber\":214,\"key\":\"role_id_16_abr\",\"tenantterm\":\"coraut\",\"frontiersdefaultterm\":\"coraut\",\"category\":\"user role\"},{\"sequencenumber\":215,\"key\":\"role_id_17_abr\",\"tenantterm\":\"saut\",\"frontiersdefaultterm\":\"saut\",\"category\":\"user role\"},{\"sequencenumber\":216,\"key\":\"role_id_18_abr\",\"tenantterm\":\"coaut\",\"frontiersdefaultterm\":\"coaut\",\"category\":\"user role\"},{\"sequencenumber\":217,\"key\":\"role_id_20_abr\",\"tenantterm\":\"pof\",\"frontiersdefaultterm\":\"pof\",\"category\":\"user role\"},{\"sequencenumber\":218,\"key\":\"role_id_22_abr\",\"tenantterm\":\"tsof\",\"frontiersdefaultterm\":\"tsof\",\"category\":\"user role\"},{\"sequencenumber\":219,\"key\":\"role_id_24_abr\",\"tenantterm\":\"ru\",\"frontiersdefaultterm\":\"ru\",\"category\":\"user role\"},{\"sequencenumber\":220,\"key\":\"role_id_35_abr\",\"tenantterm\":\"jof\",\"frontiersdefaultterm\":\"jof\",\"category\":\"user role\"},{\"sequencenumber\":221,\"key\":\"role_id_41_abr\",\"tenantterm\":\"se-adm\",\"frontiersdefaultterm\":\"se-adm\",\"category\":\"user role\"},{\"sequencenumber\":222,\"key\":\"role_id_42_abr\",\"tenantterm\":\"se-rev\",\"frontiersdefaultterm\":\"se-rev\",\"category\":\"user role\"},{\"sequencenumber\":223,\"key\":\"role_id_43_abr\",\"tenantterm\":\"se-aut\",\"frontiersdefaultterm\":\"se-aut\",\"category\":\"user role\"},{\"sequencenumber\":224,\"key\":\"role_id_52_abr\",\"tenantterm\":\"evof\",\"frontiersdefaultterm\":\"evof\",\"category\":\"user role\"},{\"sequencenumber\":225,\"key\":\"role_id_53_abr\",\"tenantterm\":\"ev-adm\",\"frontiersdefaultterm\":\"ev-adm\",\"category\":\"user role\"},{\"sequencenumber\":226,\"key\":\"role_id_89_abr\",\"tenantterm\":\"comof\",\"frontiersdefaultterm\":\"comof\",\"category\":\"user role\"},{\"sequencenumber\":227,\"key\":\"role_id_98_abr\",\"tenantterm\":\"aof\",\"frontiersdefaultterm\":\"aof\",\"category\":\"user role\"},{\"sequencenumber\":228,\"key\":\"role_id_99_abr\",\"tenantterm\":\"projects\",\"frontiersdefaultterm\":\"projects\",\"category\":\"user role\"},{\"sequencenumber\":229,\"key\":\"role_id_103_abr\",\"tenantterm\":\"config\",\"frontiersdefaultterm\":\"config\",\"category\":\"user role\"},{\"sequencenumber\":230,\"key\":\"role_id_104_abr\",\"tenantterm\":\"beta\",\"frontiersdefaultterm\":\"beta\",\"category\":\"user role\"},{\"sequencenumber\":231,\"key\":\"role_id_106_abr\",\"tenantterm\":\"wfconf\",\"frontiersdefaultterm\":\"wfconf\",\"category\":\"user role\"},{\"sequencenumber\":232,\"key\":\"role_id_107_abr\",\"tenantterm\":\"rtbeta\",\"frontiersdefaultterm\":\"rtbeta\",\"category\":\"user role\"},{\"sequencenumber\":233,\"key\":\"role_id_108_abr\",\"tenantterm\":\"deobeta\",\"frontiersdefaultterm\":\"deobeta\",\"category\":\"user role\"},{\"sequencenumber\":234,\"key\":\"role_id_109_abr\",\"tenantterm\":\"searchbeta\",\"frontiersdefaultterm\":\"searchbeta\",\"category\":\"user role\"},{\"sequencenumber\":235,\"key\":\"role_id_110_abr\",\"tenantterm\":\"jm\",\"frontiersdefaultterm\":\"jm\",\"category\":\"user role\"},{\"sequencenumber\":236,\"key\":\"role_id_111_abr\",\"tenantterm\":\"mfbeta\",\"frontiersdefaultterm\":\"mfbeta\",\"category\":\"user role\"},{\"sequencenumber\":237,\"key\":\"role_id_21_abr\",\"tenantterm\":\"coped\",\"frontiersdefaultterm\":\"coped\",\"category\":\"user role\"},{\"sequencenumber\":238,\"key\":\"reviewer_editorial_board\",\"tenantterm\":\"editorial board\",\"frontiersdefaultterm\":\"editorial board\",\"description\":\"this is the label for the review editorial board\",\"category\":\"label\"},{\"sequencenumber\":239,\"key\":\"field_chief_editor\",\"tenantterm\":\"field chief editor\",\"frontiersdefaultterm\":\"field chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":240,\"key\":\"field_chief_editors\",\"tenantterm\":\"field chief editors\",\"frontiersdefaultterm\":\"field chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":241,\"key\":\"editor\",\"tenantterm\":\"editor\",\"frontiersdefaultterm\":\"editor\",\"category\":\"label (role)\"},{\"sequencenumber\":242,\"key\":\"editors\",\"tenantterm\":\"editors\",\"frontiersdefaultterm\":\"editors\",\"category\":\"label (role)\"},{\"sequencenumber\":243,\"key\":\"board\",\"tenantterm\":\"board\",\"frontiersdefaultterm\":\"board\",\"category\":\"label\"},{\"sequencenumber\":244,\"key\":\"boards\",\"tenantterm\":\"boards\",\"frontiersdefaultterm\":\"boards\",\"category\":\"label\"},{\"sequencenumber\":245,\"key\":\"article_collection\",\"tenantterm\":\"article collection\",\"frontiersdefaultterm\":\"article collection\",\"category\":\"label\"},{\"sequencenumber\":246,\"key\":\"article_collections\",\"tenantterm\":\"article collections\",\"frontiersdefaultterm\":\"article collections\",\"category\":\"label\"},{\"sequencenumber\":247,\"key\":\"handling_editor\",\"tenantterm\":\"handling editor\",\"frontiersdefaultterm\":\"associate editor\",\"description\":\"this terminology key is for the person assigned to edit a manuscript. it is a label for the temporary handling editor assignment.\",\"category\":\"label (role)\"},{\"sequencenumber\":248,\"key\":\"handling_editors\",\"tenantterm\":\"handling editors\",\"frontiersdefaultterm\":\"associate editors\",\"description\":\"this terminology key is for the person assigned to edit a manuscript. it is a label for the temporary handling editor assignment.\",\"category\":\"label (role)\"},{\"sequencenumber\":249,\"key\":\"ae_accept\",\"tenantterm\":\"recommend acceptance\",\"frontiersdefaultterm\":\"accept\",\"category\":\"process\"},{\"sequencenumber\":250,\"key\":\"rtm\",\"tenantterm\":\"rtm\",\"frontiersdefaultterm\":\"rtm\",\"category\":\"product\"},{\"sequencenumber\":251,\"key\":\"frontiers_media_sa\",\"tenantterm\":\"frontiers media s.a\",\"frontiersdefaultterm\":\"frontiers media s.a\",\"category\":\"customer\"},{\"sequencenumber\":252,\"key\":\"review_editors\",\"tenantterm\":\"community reviewers\",\"frontiersdefaultterm\":\"review editors\",\"category\":\"label (role)\"},{\"sequencenumber\":253,\"key\":\"journal_card_chief_editor\",\"tenantterm\":\"chief editor\",\"frontiersdefaultterm\":\"chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":254,\"key\":\"journal_card_chief_editors\",\"tenantterm\":\"chief editors\",\"frontiersdefaultterm\":\"chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":255,\"key\":\"call_for_papers\",\"tenantterm\":\"call for papers\",\"frontiersdefaultterm\":\"call for papers\",\"category\":\"label\"},{\"sequencenumber\":256,\"key\":\"calls_for_papers\",\"tenantterm\":\"calls for papers\",\"frontiersdefaultterm\":\"calls for papers\",\"category\":\"label\"},{\"sequencenumber\":257,\"key\":\"supervising_editor\",\"tenantterm\":\"supervising editor\",\"frontiersdefaultterm\":\"supervising editor\",\"description\":\"a chief or assistant chief editor who is assigned to a manuscript to supervise.\",\"category\":\"role\",\"externalkey\":\"supervising_editor\"},{\"sequencenumber\":258,\"key\":\"supervising_editors\",\"tenantterm\":\"supervising editors\",\"frontiersdefaultterm\":\"supervising editors\",\"description\":\"a chief or assistant chief editor who is assigned to a manuscript to supervise.\",\"category\":\"role\",\"externalkey\":\"supervising_editors\"},{\"sequencenumber\":259,\"key\":\"reviewer_endorse\",\"tenantterm\":\"endorse\",\"frontiersdefaultterm\":\"endorse\",\"category\":\"label\"},{\"sequencenumber\":260,\"key\":\"reviewer_endorsed\",\"tenantterm\":\"endorsed\",\"frontiersdefaultterm\":\"endorsed\",\"category\":\"label\"},{\"sequencenumber\":261,\"key\":\"reviewer_endorse_publication\",\"tenantterm\":\"endorse publication\",\"frontiersdefaultterm\":\"endorse publication\",\"category\":\"label\"},{\"sequencenumber\":262,\"key\":\"reviewer_endorsed_publication\",\"tenantterm\":\"endorsed publication\",\"frontiersdefaultterm\":\"endorsed publication\",\"category\":\"label\"},{\"sequencenumber\":263,\"key\":\"editor_role\",\"tenantterm\":\"editor role\",\"frontiersdefaultterm\":\"editor role\",\"category\":\"label\"},{\"sequencenumber\":264,\"key\":\"editor_roles\",\"tenantterm\":\"editor roles\",\"frontiersdefaultterm\":\"editor roles\",\"category\":\"label\"},{\"sequencenumber\":265,\"key\":\"editorial_role\",\"tenantterm\":\"editorial role\",\"frontiersdefaultterm\":\"editorial role\",\"category\":\"label\"},{\"sequencenumber\":266,\"key\":\"editorial_roles\",\"tenantterm\":\"editorial roles\",\"frontiersdefaultterm\":\"editorial roles\",\"category\":\"label\"},{\"sequencenumber\":267,\"key\":\"call_for_paper\",\"tenantterm\":\"call for paper\",\"frontiersdefaultterm\":\"call for paper\",\"category\":\"label\"},{\"sequencenumber\":268,\"key\":\"research_topic_abstract\",\"tenantterm\":\"manuscript summary\",\"frontiersdefaultterm\":\"manuscript summary\",\"category\":\"process\"},{\"sequencenumber\":269,\"key\":\"research_topic_abstracts\",\"tenantterm\":\"manuscript summaries\",\"frontiersdefaultterm\":\"manuscript summaries\",\"category\":\"process\"},{\"sequencenumber\":270,\"key\":\"submissions_team_manager\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"content manager\",\"category\":\"process\"},{\"sequencenumber\":271,\"key\":\"submissions_team\",\"tenantterm\":\"journal team\",\"frontiersdefaultterm\":\"content team\",\"category\":\"process\"},{\"sequencenumber\":272,\"key\":\"topic_coordinator\",\"tenantterm\":\"topic coordinator\",\"frontiersdefaultterm\":\"topic coordinator\",\"category\":\"process\"},{\"sequencenumber\":273,\"key\":\"topic_coordinators\",\"tenantterm\":\"topic coordinators\",\"frontiersdefaultterm\":\"topic coordinators\",\"category\":\"process\"},{\"sequencenumber\":274,\"key\":\"frontiers_journal_team\",\"tenantterm\":\"frontiers journal team\",\"frontiersdefaultterm\":\"frontiers journal team\",\"category\":\"label (role)\"},{\"sequencenumber\":275,\"key\":\"research_topic_ic_submission\",\"tenantterm\":\"rt:ic submission\",\"frontiersdefaultterm\":\"rt:ic submission\",\"category\":\"label (role)\"},{\"sequencenumber\":276,\"key\":\"research_topic_ic_submission_participant\",\"tenantterm\":\"rt:ic submission participant\",\"frontiersdefaultterm\":\"rt:ic submission participant\",\"category\":\"label (role)\"}]}'\n",impactgtmid:"gtm-njf2ml9b",impactgtmauth:"fyo-7nodm9w37qgfms03sa",impactgtmpreview:"env-1",impactgooglemapsapikey:"aizasyctehx2eu8pwfi86gw9fi00ftcykk6ifr8",qualtricsv4tov3:"'{\"enabled\":true,\"interceptid\":\"si_er0e2ze69oz8yn4\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",qualtricsumux:"'{\"enabled\":false,\"interceptid\":\"si_89fphy3etxqbwtu\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",qualtricscontinuousbrand:"'{\"enabled\":true,\"interceptid\":\"si_9zut94ut81fcrh4\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",defaultarticletemplate:"v3"},app:{baseurl:"/",buildid:"f2d5b6d8-d01c-4534-90b9-d659f10f17ca",buildassetsdir:"ap-2024/_nuxt",cdnurl:""}}

## Results
demonstrate that the proposed model achieves an accuracy of 86.61% on the test set, representing an improvement of 7.8 percentage points compared with the original mobilevitv3 model and significantly outperforming other mainstream disease grading models. this study confirms that the cogfuse-mobilevit model can effectively resolve the issues of blurred edges and inefficient feature extraction in this disease, provides a reliable technical solution for its precision grading, and holds practical application value for the intelligent diagnosis of plant diseases in smart agriculture. 1 introduction walnut diseases pose a serious threat to walnut production in xinjiang. the spread of crop diseases significantly exacerbates the security risks of the walnut industry if timely prevention and control measures are not taken ( cooke, 2006 ; khan et&#xa0;al., 2021 ). as a core component of the disease prevention and control system, early precise grading plays a critical role in agricultural production management ( adaskaveg et&#xa0;al., 2009 ; chiang et&#xa0;al., 2016 ; lamichhane, 2014 ). walnut brown spot, caused primarily by the fungus ophiognomonia leptostyla , typically forms specific symptoms on organs such as leaves, flowers, and fruits. among them, leaves, as the primary carrier of plant diseases, exhibit characteristics such as lesion morphology and color changes on their surfaces, which often serve as important bases for disease grading ( ghaiwat and arora, 2014 ; weber, 1980 ; zarei et&#xa0;al., 2019 ). traditional disease identification relies on manual experience ( moragrega et&#xa0;al., 2011 ; wang et&#xa0;al., 2020 ), a method that is not only time- color system to reduce interference from light and leaf veins, and lesion edge detection using the sobel operator, ultimately achieving fast and accurate grading based on the ratio of lesion area to leaf area. jadhav and patil (2016) developed a leaf image partitioning technique based on k-means clustering and squared euclidean distance, automatically quantifying damaged leaf area through the pixel ratio of lesions to leaves for disease grading. arivazhagan et al. (2013) proposed a four-step processing system involving color conversion, green pixel removal, segmentation, and texture feature classification for leaf disease grading. however, traditional methods for acquiring disease information suffer from insufficient segmentation accuracy for plant leaf disease images with complex textures and unclear lesion boundaries, thereby resulting in poor disease severity grading performance. with the continuous advancement of computational power, deep learning has increasingly become a key technology for addressing complex lesion segmentation, primarily due to its superior capability in automatic feature extraction ( mao et&#xa0;al., 2023 ). chen et al. (2021) proposed the blsnet method based on the unet semantic segmentation network, enhancing lesion segmentation accuracy through the introduction of attention mechanisms and multi-scale feature fusion. experiments showed that its segmentation and classification accuracy outperformed benchmark models such as deeplabv3+ and unet, preliminarily verifying the reliability of this method in automatic assessment of bls disease severity. ngugi et al. (2020) developed the kijaninet segmentation network based on fully convolutional neural networks, demonstrating excellent performance in tomato leaf segmentation under complex backgrounds; lin et al. (2019) developed a cnn semantic segmentation model that achieved high-precision pixel-level segmentation of cucumber powdery mildew on leaves. additionally, some studies have fused traditional features with deep learning: tripathi (2021) proposed a convolutional neural network method based on alexnet, achieving disease grading by fusing features extracted by the model with external leaf segmentation features; ma et al. (2017) introduced comprehensive color features (ccf) combining hyper-red index, hsv/h and lab/b components, and achieved lesion segmentation via interactive region growing, with experiments confirming that this method enables accurate grading of disease images such as cucumber downy mildew under actual field conditions. however, although such methods have improved grading accuracy to some extent, they still suffer from insufficient feature capture of blurred disease edges, making it difficult to achieve fine-grained differentiation of subtle differences between disease grades ( rastogi et&#xa0;al., 2015 ). in recent years, the application of deep learning in plant disease classification has continued to expand. parashar et al. (2024) systematically validated the capability of complex feature modeling for crop yield prediction, further substantiating the critical role of adaptive feature extraction in agricultural intelligent decision-making. concurrently, vishnoi et al. (2022) enhanced small-target detection precision in apple disease identification through a spatial attention mechanism-based cnn architecture for leaf disease diagnosis. ozturk et al. (2025) constructed an ensemble learning classification model based on resnet50, mobile net, efficientnetb0, and densenet121, enhancing generalization performance through statistical cross-validation and improving decision interpretability via grad-cam visualization. experimental results showed that the ensemble model achieved stable high-precision classification performance across multiple validation rounds. these studies provide robust and interpretable solutions for intelligent plant disease recognition through technical integration and architectural innovation. hu et al. (2021) integrated retinex enhancement, faster r-cnn detection, and vgg16 classification to improve the grading and detection accuracy of blurred diseased leaves. picon et al. (2019) proposed an adaptive deep residual neural network algorithm for the classification of three european wheat diseases (septoria leaf blotch, brown spot, and rust) in real-world scenarios, which effectively improved the classification accuracy of wheat diseases. kim and ahn (2021) employed deep cnn architectures such as resnet, xception, and densenet, combined with transfer learning and fine-tuning, to classify 9 categories of pests, diseases, and healthy states in tomatoes. although densenet combined with the rmsprop algorithm achieved an accuracy of 98.63%, single cnn architectures have clear bottlenecks in handling complex plant lesions and overlapping multiple diseases, including insufficient specificity in feature extraction and limited ability to distinguish subtle differences between lesions. shi et al. (2023) analyzed the application of cnns in evaluating the severity of plant diseases, pointing out that hybrid architectures such as classical cnns, improved cnns, and segmentation networks have limitations in handling complex plant lesions: classification errors caused by concurrent multiple diseases, as well as constraints on model generalization ability due to unbalanced datasets and insufficient annotation accuracy. these issues significantly restrict their precise application in practical agricultural scenarios. although significant advancements have been made in plant lesion segmentation and disease classification using deep learning, the grading task for walnut leaf brown spot disease&#x2014;characterized by blurred edge representation and complex small lesions&#x2014;still faces notable challenges. current mobilevit-based hybrid models fall into two categories: general architectures (mobilevitv1/v2/v3) focus on optimizing the cnn-transformer balance for natural images; domain-improved models (mobile-former/edgevit) are oriented toward medical/industrial tasks, with their task focus on localization rather than grading, which mismatches the pain points and fails to address the blurriness of agricultural lesions, tissue interference, and morphological variations. thus, the present study proposes a novel cogfuse-mobilevit model specifically designed for disease grading, with its specific contributions as follows: 1. a diverse field-collected walnut leaf dataset was constructed, with images divided into four distinct severity levels based on qualitative assessment of disease severity, ensuring the accuracy of disease severity grading. 2. the hierarchical feature selection module (hfsm) enhances the fusion of local details (such as lesion texture and color) with global context through local and global attention mechanisms and task-driven feature selection, while suppressing interference from healthy regions. 3. the edge convolution fusion module (ecfm) strengthens edge details through sobel convolution and residual connections, achieving effective integration of edge-specific features with general features, further enhancing edge details and enabling more precise capture of lesion contour details. 4. the adaptive multi-scale dilated dense inception convolution module (amsddicm) extracts differentiated features using multi-shaped convolution kernels and adaptively fuses multi-scale information via a dynamic weight mechanism, capturing lesion morphologies at different pathogenesis stages. 2 materials and methods 2.1 characteristics of walnut leaf brown spot and classification of disease severity levels in the local standard technical regulations for prevention and control of walnut brown spot ( mcgranahan and leslie, 1991 ), walnut brown spot is divided into four severity levels. the severity of plant leaf diseases is generally determined using the spot coverage method. in this study, walnut leaves with different disease severities were processed to separate disease-infected spots from healthy leaf areas, and the percentage of lesion area to total leaf area was calculated. with reference to the local standard, the disease grades specified in the standard were re-determined through calculations in this study, as shown in table&#xa0;1 . classification of different severity levels of walnut leaf brown spot was conducted in accordance with technical regulations for prevention and control of walnut brown spot ( wang et&#xa0;al., 2022 ; yang et&#xa0;al., 2021 ). table&#xa0;1 table&#xa0;1 . walnut leaf brown spot disease severity grading criteria. walnut leaf brown spot is caused by infection with the fungus ophiognomonia leptostyla ( mircetich et&#xa0;al., 1980 ). in the early infection stage, near-circular or irregular small spots appear on the leaves, with a gray-brown center and dark yellow-green to purplish-brown edges, and diseased leaves tend to fall off prematurely. in the middle stage, elongated elliptical or irregular slightly sunken dark brown lesions form, with larger spot size and light brown edges; longitudinal cracks are often present in the center of the lesions. in the late stage, lesions often coalesce to form large scorched necrotic areas, surrounded by yellow to golden-yellow zones, and small black granules (conidiomata and conidia of the pathogen) are scattered on the surface of the diseased tissue ( mircetich et&#xa0;al., 1980 ). according to the degree of color and texture feature changes in infected leaves, walnut leaf disease is divided into four stages: healthy, early, middle, and late stages, corresponding to disease severity levels: healthy (level 0), mild (level 1), moderate (level 2), and severe (level 3). the different severity levels of walnut leaf brown spot are shown in figure&#xa0;1 . figure&#xa0;1 figure&#xa0;1 . walnut leaves infected with brown spot disease at different severity levels. (a) healthy leaves; (b) mildly infected leaves; (c) moderately infected leaves; (d) severely infected leaves. table&#xa0;1 shows the ratio of walnut leaf brown spot lesion area to total leaf area. level 0 (healthy): healthy leaves without symptoms. level 1 (mild): near-circular or irregular small white spots appear on leaves, accounting for less than 5% of the leaf area. level 2 (moderate): lesions expand into irregular dark brown spots, covering 5% to 30% of the leaf area. level 3 (severe): abundant lesion coalesce to form large scorched necrotic areas, exceeding 30% of the leaf area. 2.2 calculation algorithm for walnut leaf brown spot disease severity levels after capturing walnut leaf brown spot samples using a camera, to minimize subjective bias, this study strictly adhered to the objective quantitative criteria defined in the technical regulations for the control of walnut brown leaf spot ( table&#xa0;1 ), utilizing the lesion area percentage (k) as the primary grading indicator. we re-determined the severity levels through computational analysis, with the specific determination process shown in figure&#xa0;2 . figure&#xa0;2 figure&#xa0;2 . computational method for different severity levels of brown spot infection in walnut leaves. first, a python-based color segmentation algorithm was employed to convert images from the bgr color space to the hsv color space for processing. the hsv color space offers inherent advantages in color segmentation tasks. by analyzing the hue, saturation, and value characteristics of diseased regions, the color threshold range in the hsv space was determined ( chaudhary et&#xa0;al., 2012 ). after generating an initial disease region mask based on this threshold range, morphological operations such as closing and opening were applied to optimize mask quality, effectively eliminating noise while preserving the integrity of lesion contours. subsequently, the original image was converted to grayscale mode, and the leaf region was extracted using an adaptive threshold binarization algorithm. leaf boundaries were localized via contour detection technology, and a complete leaf mask was generated through contour filling. pixel statistical analysis was performed on both the leaf mask and disease mask to calculate the total leaf area and diseased region area, respectively. when a valid leaf area (&gt;0) was detected, the disease severity index was calculated using the following formula: disease severity index ( % ) = s 1 s 2 &#xd7; 100 % &#xa0;&#xa0;&#xa0;s 1 : total area of diseased regions s 2 : total area of the complete leaf. 2.3 dataset construction data were collected from the walnut plantation base of tarim university (alar, xinjiang) between may and october 2024. leaves from three locally dominant early-fruiting cultivars (&#x2018;wen 185&#x2019;, &#x2018;xinxin 2&#x2019;, &#x2018;zha 343&#x2019;)widely cultivated in southern xinjiang were vertically photographed using an iphone 13. this genotypic diversity ensures model generalizability by encompassing varied disease phenotypes. the orchard follows standardized cultivation with 4m plant spacing, 5m row spacing (&#x2248;50 plants/mu), and conventional management practices. sampled trees were in full fruiting stage. to capture disease traits across microenvironments, samples were collected from safely accessible crown layers. the dataset includes healthy and brown spot-infected leaves three severity levels, spanning varied time periods, light conditions, and angles. after removing duplicates and invalid images,5,120 high-quality jpgs were retained for analysis. 2.3.1 development of the walnut leaf brown spot dataset disease severity grading was established through quantitative lesion area analysis ( figure&#xa0;2 ). a representative subset of 512 images (10% of the full 5,120-image dataset) was selected via stratified random sampling, accurately preserving the original severity distribution. to ensure labeling rigor, two plant protection specialists (&gt;5 years&#x2019; expertise in walnut pathology, certified in local protocols) executed standardized annotation: pre-annotation training unified diagnostic criteria for ambiguous cases, with formal annotation commencing only after achieving pre-calibration inter-rater agreement (kappa coefficient &#x2265;0.75). as validated in table&#xa0;2 , dual statistical metrics confirmed gold-standard reliability&#x2014;inter-rater cohen&#x2019;s kappa reached 0.71, meeting landis &amp; koch&#x2019;s &#x201c;substantial agreement&#x201d; threshold; algorithm-expert consensus attained a weighted fleiss&#x2019; kappa of 0.77, substantially outperforming conventional cohen&#x2019;s kappa in multi-annotator scenarios. a three-stage standardization pipeline eliminated preprocessing discrepancies. the final dataset, partitioned into training/testing sets (8:2 ratio), strictly adheres to plant disease survey protocols and provides benchmarked data for deep learning model development ( table&#xa0;3 ). table&#xa0;2 table&#xa0;2 . algorithm vs. expert consensus agreement evaluation. table&#xa0;3 table&#xa0;3 . walnut leaf brown spot disease image acquisition data. 2.4 construction of walnut leaf brown spot disease severity grading model 2.4.1 the optimized mobilevitv3 network: cogfuse-mobilevit to address the challenge of difficult feature capture for small lesions in walnut leaf brown spot severity grading, this study proposes an innovative model, cogfuse-mobilevit. mobilevitv3 was selected as the backbone network due to its suitability for walnut brown spot grading. its hybrid mobilenet-vit architecture integrates cnn-based local feature extraction essential for lesion detail capture with transformer-enabled global contextual modeling critical for analyzing lesion spatial distribution. this design aligns with pathological requirements for severity grading, necessitating concurrent attention to local lesion characteristics and global infection patterns. the lightweight architecture further supports real-time processing on embedded devices, enabling future field deployment. the model embodies a &#x201c;grading task-driven&#x201d; design principle. conceptually, it establishes a disease-specific framework of &#x201c;hierarchical screening to edge enhancement to dynamic fusion.&#x201d; this framework elevates general feature extraction to targeted solutions addressing three major agricultural challenges: suppressing interference from healthy tissues via hierarchical screening resolving feature confusion; strengthening blurred lesion contours through edge enhancement overcoming the bottleneck of edge blurriness; adaptively handling multi-stage morphological variations using dynamic fusion addressing morphological variation challenges. structurally, as illustrated in figure&#xa0;3 , the network implements a progressive feature extraction strategy. the hierarchical feature selection module (hfsm) first fuses shallow and middle-layer features. it employs a hierarchical attention mechanism to enhance semantic consistency while preserving spatial details. the edge convolution fusion module (ecfm) then processes these features. it utilizes learnable sobel operators to extract edge information, which is fused with conventional convolutional features via residual connections to augment edge perception capability. finally, the adaptive multi-scale dilated inception convolution module (amsddicm) enables adaptive processing of multi-scale edge features. this module is capable of both capturing fine edge changes in minute lesions and grasping the overall contour structure of large lesions, thereby comprehensively covering edge features across different developmental stages. based on these enhanced edge features, the network accurately outputs the final disease severity grade. figure&#xa0;3 figure&#xa0;3 . cogfuse-mobilevit architecture diagram (hfsm suppresses healthy regions through dynamic masks, ecfm explicitly extracts edges via sobel convolution, and amsddicm fuses multi-scale features through dynamic weights). 2.4.2 the hierarchical feature selection module the hfsm (hierarchical feature selection module) is an innovative architecture proposed in this study. unlike mobilevit&#x2019;s direct feature concatenation, hfsm utilizes learnable prompt vectors ( equation 1 ) to generate spatial masks, dynamically suppressing non-lesion regions. such task-driven selection is crucial for tiny objects. in the grading task of walnut leaf brown spot disease, this module utilizes a local attention mechanism to focus on micro-regions of walnut leaves, fusing shallow and middle-layer features. meanwhile, the hierarchical feature selection mechanism enables precise capture of local detail features such as lesion texture and color, effectively suppressing interference from healthy leaf regions and enhancing disease features. this provides high-discriminative feature representations for brown spot disease grading while preparing for subsequent lesion edge processing. as depicted in figure&#xa0;4 , the module processes two hierarchical feature maps. initial 1&#xd7;1 convolutions reduce both maps&#x2019; channels to half the output dimension curtailing computational load. one reduced map undergoes bilinear upsampling for spatial alignment with the other. these aligned features are summed and processed by a 3&#xd7;3 convolution to generate base-path features. simultaneously, the original reduced map and upsampled map are fed into dual local-global attention modules for parallel processing ( xu, 2024 ). each group contains local and global branches. during branch processing, the feature map is partitioned into p&#xd7;p non-overlapping patches p i , j via the unfold operation. after calculating the mean of pixel features within each patch, the result is processed using the following core formula: attention distribution generation. figure&#xa0;4 figure&#xa0;4 . architecture diagram of the hierarchical feature selection module (hfsm). z i , j = ml p 2 ( layernor ( ml p 1 ( mean ( p i , j ) ) ) ) ( 1 ) a i , j = softmax ( z i , j ) ( 2 ) equations 1 and 2 convert the mean value of patch features into a high-dimensional feature vector through multi-layer perceptrons (mlp) and layer normalization (layernorm) (wherein( mlp 1 &#xa0;and&#xa0;&#xa0;mlp 2 ) achieve&#xa0;dimensional&#xa0;transformation&#xa0;p 2 &#x2192; ouc / 2 &#x2192; ouc / 2 )), and then generates the attention distribution via the softmax function a i , j . this mechanism enables the model to focus on the key features of lesion regions, weaken the interference from healthy leaf areas, enhance the specificity of feature selection, and provide high-quality features for subsequent precise grading. after the local and global features output by the two modules are spliced along the channel dimension, they are combined with the basic path features to generate the output features of the final processing module. for leaf images with over 80% healthy tissue, dynamic masks are generated via learnable prompt vectors to suppress green texture features while enhancing the gray-brown features of lesions ( mcgranahan and leslie, 1991 ). 2.4.3 ecfm edge convolutional fusion module in the walnut leaf brown spot grading task, lesion edge features are crucial for accurately determining disease severity. the standard mobilevit relies on cnn-transformer blocks to implicitly learn edges, whereas the ecfm (edge convolution fusion module) incorporates sobel convolutions, conventional convolutions, and residual connections, effectively fusing edge features with general features and thereby enhancing edge details. as shown in figure&#xa0;5 , the sobel branch employs sobel convolution to extract image edge information, accurately capturing the contour details of brown spot lesions; the convolutional branch captures general features such as leaf color and texture through standard convolution. the two realize feature fusion via the following core formula ( equation 3 ): figure&#xa0;5 figure&#xa0;5 . architecture diagram of the edge convolutional fusion module (ecfm). s = sobelconv ( x ) &#xa0; ( 3 ) processing the input feature map ( x ) through the sobel convolution operator (sobelconv) specifically extracts edge information of lesions (such as the boundary contours between lesions and healthy tissues, and the edge textures of small lesions). for the commonly seen blurred edges in brown spot disease, sobel convolution can enhance edge gradient changes, making the originally blurred lesion contours clearer, thus providing key edge feature support for subsequent grading. conventional feature extraction and fusion ( equation 4 ): &#xa0; c = conv ( x ) , &#xa0; f concat = concat ( s , c ) ( 4 ) &#xa0;c = conv ( x ) extracting the overall features of leaves (such as the color distribution of lesions and the overall texture of leaves) through standard convolution forms a complement to edge features, preventing the model from focusing only on local edges while ignoring global lesion information. f concat = concat ( s , c ) concatenating the edge feature s and the conventional feature c along the channel dimension achieves the initial fusion of edge details and global features. this fusion enables the model to both identify the fine contours of lesions and judge the disease condition by combining the overall color and texture changes of lesions, thereby improving the accuracy of grading. the module also introduces feature addition and subsequent convolution operations: the fused features are first processed by the first convolution layer f 1 = conv 1 ( f concat ) , the corresponding operation results are added to the original input, and the final output is generated through the second convolution layer f final = conv 2 ( f 1 + x ) . in this process, the residual connection retains the original feature information, further strengthens the integration of edge features and conventional features, and ensures the effective transmission and enhancement of multi-level features. the edge gradient information extracted by sobel convolution (such as grayscale differences between diseased spots and healthy tissues) and the texture features from conventional convolution (such as the roughness of necrotic areas) are fused via residual connections. this not only preserves the blurred edges of early-stage lesions but also prevents edge features from being disconnected from the overall texture ( ghaiwat and arora, 2014 ). 2.4.4 amsddicm adaptive multi-scale dilated depthwise inseparable convolution module this module differs from mobilevit&#x2019;s single-scale convolution, leverages multi-shaped convolution kernels to accurately extract lesion features of circular, irregular, and other shapes from multiple dimensions, enabling the identification of subtle differences in lesion edges and internal colors to enrich feature dimensions. meanwhile, depth wise separable convolutions are employed to accommodate lesion scales at different stages of disease development. specifically, the input features are first split into two groups, each entering the amsddicm module to extract features via multi-scale depth wise separable convolutions. a dynamic weighting mechanism (global pooling + convolution + softmax) is used to generate weights for fusing multi-scale features. the processed features from the two groups are concatenated, and finally, cross-channel fusion is completed via 1&#xd7;1 convolution to output the final optimized features. the entire process integrates multi-scale convolution, dynamic weight allocation, and feature fusion to enhance the model&#x2019;s ability to capture complex features, as shown in figure&#xa0;6 . in response to the morphological differences between early-stage small spots (1-3mm in diameter) and late-stage fused spots (over 20mm in diameter), the dynamic weight mechanism enables the model to adaptively allocate the contributions of 3&#xd7;3 kernels for capturing local textures and 11&#xd7;1 kernels for extracting strip-shaped spreading features. this addresses the limitation of fixed weights in traditional inception architectures in adapting to multi-scale morphologies. figure&#xa0;6 figure&#xa0;6 . architecture diagram of the adaptive multi-scale dilated depth wise inseparable convolution module (amsddicm). when the input feature tensor x with the number of channels c enters the amsddicm module, it first undergoes a feature grouping operation: the input features are evenly split into two groups along the channel dimension, with each group containing half of the original number of channels (c//2). this grouping strategy not only reduces computational complexity but also creates conditions for subsequent multi-scale feature extraction. each feature group is fed into an independent dmsconv2d module, which adopts different configurations such as 3&#xd7;3, 5&#xd7;5 square convolution kernels and 1&#xd7;11, 11&#xd7;1 strip-shaped convolution kernels &#x2014; the square kernels capture local textures, while the strip-shaped kernels extract direction-sensitive long-range dependencies. this dynamic weight allocation draws inspiration from adaptive strategies in agricultural iot systems. similar to how salinity-aware ets models prioritize soil conductivity features under salt stress ( zhang et&#xa0;al., 2023 ), our mechanism selectively amplifies lesion morphological features critical for severity grading. the dynamic weighting mechanism of dmsconv2d generates weights through global average pooling and 1&#xd7;1 convolution, with the core formulas as follows: weighted basic feature generation: x dkw = rearrange ( conv 1 &#xd7; 1 ( avgpoo l2 d ( x ) ) ) ( 5 ) equation 5 compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information (such as the overall distribution characteristics of lesions at different scales), and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization: x dkw = f . softmax ( x dkw ) ( 6 ) equation 6 compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information, such as the overall distribution characteristics of lesions at different scales, and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization: x = &#x2211; i = 0 2 ( dwcon v i ( x ) &#xd7; x dkw , i ) ( 7 ) equation 7 is based on dynamic weights x dkw , i ) for different convolution kernels &#xa0;dwconv i ( x ) weighted fusion is performed on the extracted features. for walnut brown spot lesions exhibiting size heterogeneity and morphological complexity&#x2014;ranging from early-stage circular micro-lesions to late-stage coalesced irregular lesions&#x2014;this mechanism adaptively modulates multi-scale feature contributions. it prioritizes retention of morphology-discriminative features, local textures in small lesions or directional distributions in large lesions, thereby comprehensively capturing pathological characteristics across developmental stages. the processed features undergo channel-wise concatenation to generate optimized outputs ( mircetich et&#xa0;al., 1980 ). 2.5 experimental process for severity grading of walnut leaf brown spot disease figure&#xa0;7 is the overall flow chart of severity grading of walnut leaf spot disease. first is the image acquisition stage, where raw images of walnut leaves are captured and collected to construct an image database containing pictures to be classified ( singh et&#xa0;al., 2019 ). next is the image preprocessing stage, which involves sequentially calculating and classifying disease severity levels, establishing image labels, and performing image preprocessing operations. subsequently, the processed data are used to train the constructed dataset. finally, in the model training and performance evaluation stage, the preprocessed images are input into the model for classifying walnut leaf brown spot disease, after which performance evaluation is conducted on the model&#x2019;s classification results to determine the model&#x2019;s effectiveness and accuracy. figure&#xa0;7 figure&#xa0;7 . overall flow chart for severity grading of walnut leaf brown spot disease. 2.6 experimental parameters and evaluation metrics 2.6.1 test environment and hyperparameter setting the experimental setup of this study is based on deep learning technology and leverages high-performance computing resources. all experiments were conducted on the windows 10 operating system. the hardware platform consists of an amd ryzen 7 3700x processor and an nvidia rtx 2080ti graphics card. the software environment was built using python 3.8, the pytorch 1.13.0 deep learning framework, and the cuda 11.3 parallel computing platform. additionally, pytorch 1.13.0&#x2014;a widely adopted open-source deep learning library renowned for its high flexibility&#x2014;was selected, making it well-suited for research and development. during model training, multiple adjustments were made to the hyperparameters to compare test results and select the optimal hyperparameter combination. the model accepts input images sized at 224&#xd7;224 pixels, with a configured batch size of 32 during training and a maximum of 100 training epochs. the optimizer employs an initial learning rate of 0.003. 2.6.2 evaluation metrics this paper aims to evaluate the performance of the model and verify the effectiveness of the improvement measures. we selected multiple evaluation metrics, and all subsequent experimental results adopt the method of calculating averages via 5-fold cross-validation, aiming to comprehensively and reliably evaluate the model performance. including precision (p), recall (r), etc. ( equations 8 &#x2013; 16 ) these metrics can be calculated using the following formulas. the arithmetic mean of the metric values across the five folds: m &#xaf; = 1 5 &#x2211; i = 1 5 m i ( 8 ) precision = tp / ( tp + fp ) ( 9 ) recall = tp / ( tp + fn ) ( 10 ) f 1 score = 2 tp 2 tp + fp + fn ( 11 ) macro&#xa0;precision= 1 c &#x2211; i = 1 c p i ( 12 ) macro&#xa0;recall= 1 c &#x2211; i = 1 c r i ( 13 ) weighted&#xa0;avg&#xa0;precision= &#x2211; ( n i &#xb7; precisio n i ) &#x2211; n i ( 14 ) weighted&#xa0;avg&#xa0;recall= &#x2211; ( n i &#xb7; precisio n i ) &#x2211; n i ( 15 ) weighted&#xa0;avg&#xa0;recall= &#x2211; i = 1 c t p i n ( 16 ) 3 experiments and results analysis 3.1 core module design and validity experimental verification 3.1.1 comparative test of necessity of hfsm module to validate the necessity of the hierarchical feature selection module (hfsm) within the cogfuse-mobilevit framework, this study conducted comparative tests against two mainstream lightweight attention modules: se (squeeze-and-excitation) and cbam (convolutional block attention module). as illustrated in table&#xa0;4 , the hfsm module achieves a significantly higher accuracy of 86.61%, outperforming the se module and cbam module by 7.38% and 4.46%, respectively. this demonstrates that its hierarchical feature selection mechanism more effectively captures discriminative features. in terms of computational efficiency, the parameters and flops of hfsm remain comparable with those of the se module and cbam module, indicating that its performance breakthrough stems from innovative structural design under equivalent lightweight constraints. comprehensive results confirm that replacing hfsm with se or cbam modules would incur a performance degradation exceeding 4%, thereby validating the indispensable value of hfsm in enabling efficient feature selection within the cogfuse-mobilevit framework. table&#xa0;4 table&#xa0;4 . performance comparison of attention modules within the cogfuse-mobilevit framework. 3.1.2 convolutional kernel selection in amsddicm rigorous validation via kernel combination ablation studies ( table&#xa0;5 ) demonstrates. the hybrid configuration (3&#xd7;3 + 5&#xd7;5 + 1&#xd7;11) significantly outperformed square-only kernels (3&#xd7;3 + 5&#xd7;5) accuracy 86.61% and 82.15% stage-specific recall gains. mid-stage lesions (level 2) raise 9.4 percentage points. late-stage lesions (level 3) raise 6.1 percentage points. this empirically validates the necessity of 1&#xd7;11 rectangular kernels for capturing linear pathological features, overcoming the limitation of isotropic kernels in detecting anisotropic structures. table&#xa0;5 table&#xa0;5 . comparison of different convolution kernels in amsddicm. 3.1.3 the impact of new modules on computational complexity figure&#xa0;8 shows that the hfsm module incurs a 169% flops increase to achieve a breakthrough improvement in early-stage lesion detection&#x2014;reducing level 0/1 misclassification by 24%, thereby establishing the pathological foundation for severity grading. the ecfm module contributes a mere 3% flops increment yet drives a 3.68 percentage point accuracy gain through enhanced edge feature representation. with only a 0.028g flops overhead 1.3%, the amsddicm module enables adaptive fusion of multiscale pathological deformations, culminating in a 7.80-pp accuracy leap. these modules form a cascaded optimization paradigm: hfsm&#x2019;s substantial cost resolves the core pathological bottleneck, while subsequent modules deliver superlinear returns&#x2014;harvesting 6.85-pp accuracy gain with just 14% additional flops&#x2014;collectively establishing the globally optimal computation-performance equilibrium. figure&#xa0;8 figure&#xa0;8 . comparison of the impact of each newly added module on computational complexity (a) accuracy (%); (b) flops (g); (c) params (m). measured on nvidia rtx 2080ti gpu (pytorch 1.13.0, cuda 11.3) with 224&#xd7;224 input. 3.1.4 comparison of the influence of different module fusion on model performance to validate the effect of module fusion, table&#xa0;6 compares model performances with different combinations. when only hfsm (hierarchical feature selection module) is introduced, precision increases from the baseline of 82.23% to 83.77%, but recall decreases by 3 percentage points to 72.31%, causing the f1 score to slightly decline to 78.04%. accuracy rises to 82.15%, indicating improved overall classification correctness of the model, but with potential risk of missed detections. table&#xa0;6 table&#xa0;6 . impact of fusion of different modules on model performance. when hfsm and ecfm (edge-context feature module) are synergistically introduced, precision increases to 86.34%, recall recovers to 74.69%, and both f1 score and accuracy are significantly optimized. hfsm fuses shallow and middle-layer features through a hierarchical attention mechanism, laying the foundation for subsequent edge feature processing; ecfm enhances lesion edge features. the combination of the two effectively improves the integrity of feature representation and the clarity of lesion boundaries. the combination of hfsm and amsddicm (adaptive multi-scale dilated depthwise inseparable convolution module) further pushes recall to 76.24%, accuracy to 83.94%, and f1 score to 80.79%, outperforming the hfsm+ecfm combination. amsddicm compensates for hfsm&#x2019;s deficiency in local feature refinement through attention-guided multi-scale detail fusion, which integrates multi-layer detail feature weights, especially suitable for scenarios with small targets or blurred features. when the three modules work synergistically, all indicators reach optimal levels: precision, recall, f1 score, and accuracy increase by 12.19%, 7.67%, 9.62%, and 10.00% respectively compared with the baseline. among them, hfsm lays the foundation for cross-layer feature fusion, ecfm effectively integrates edge-specific features with general features to enhance image edge information, and amsddicm adaptively fuses multi-scale and multi-type features through an attention mechanism, forming a progressive optimization chain from &#x201c;hierarchical feature extraction&#x201d; to &#x201c;edge semantic enhancement&#x201d; and then to &#x201c;multi-layer detail fusion&#x201d;. the experimental results show a balanced improvement in both precision and recall, indicating that the fusion of the three modules enables the model to focus more on learning the features of small brown spot lesions, thereby improving its classification performance. this synergistic task-specific design, combining hierarchical selection, explicit edge enhancement, and adaptive multi-scale fusion, fundamentally distinguishes cogfuse-mobilevit from prior mobilevit hybrids designed for general vision or localization tasks. 3.1.5 influence of different module combinations on f1-score of level (0-3) in order to verify the targeted improvement of each module for a specific disease level, we conducted the following comparative experiments as shown in the table&#xa0;6 , when hfsm is introduced alone, the f1-score of level 0 increases from 83.10% to 85.60%, effectively reducing feature confusion between healthy leaves and early-stage lesions. after adding ecfm, the f1-score of level 1 rises from 73.20% (with hfsm alone) to 79.80%, significantly mitigating the recognition bias caused by blurred edges of small lesions. amsddicm increases the f1-score of level 3 from 80.37% to 85.54%, enhancing the adaptability to the morphology of large-scale fused scorched areas. with the collaboration of the three modules, the f1-scores of level 0&#x2013;3 reach 93.99%, 82.57%, 84.28%, and 85.54% respectively, achieving balanced optimization of performance across all levels. 3.2 results comparison of different algorithms and statistical significance verification 3.2.1 comparison of grading results for different classification models to verify the effectiveness of the cogfuse-mobilevit model, we selected 9 commonly used classification models to compare with the optimized cogfuse-mobilevit model, and the results are the average of 5-fold cross-validation over 120 epochs. as shown in table&#xa0;7 , the proposed cogfuse-mobilevit model achieved the highest grading performance in terms of precision, recall, and f1-score for identifying walnut leaf brown spot disease at different severity levels among all compared models. the improved cogfuse-mobilevit model exhibits an accuracy of 86.61%, representing a 7.80-percentage-point improvement over the original model. overall, the experimental results highlight that the enhanced cogfuse-mobilevit model is more conducive to focusing on lesion edge details and accurately learning the features of different severity levels of walnut leaf brown spot disease, thereby improving the model&#x2019;s classification performance. table&#xa0;7 table&#xa0;7 . comparison of grading results for different classification models. 3.2.2 comparison of performance and reliability validation of different algorithms in model training, to quantify the reliability of results and avoid random biases from single experiments, this study employs 5-fold cross-validation to generate 95% confidence intervals, as shown in the figure&#xa0;9 . the results demonstrate that cogfuse-mobilevit takes a significant lead with an accuracy of 86.61% (95% ci: [85.24%, 87.89%]). its narrowest confidence interval range indicates the strongest generalization capability. among the comparative models, mobilevitv3 has a 95% ci of [77.20%, 80.42%]; its lower bound is significantly lower than that of cogfuse-mobilevit, confirming that the 7.8% performance improvement is not due to random fluctuations. furthermore, the confidence intervals constructed through 5-fold cross-validation further highlight the reliability of the experimental results. figure&#xa0;9 figure&#xa0;9 . confidence intervals of accuracy for different classification models in 5-fold cross-validation. 3.2.3 statistical significance verification of model improvement to statistically evaluate the performance improvement of cogfuse-mobilevit over the baseline mobilevitv3, an independent samples t-test was conducted ( table&#xa0;8 ). for each model architecture, five independent training runs. the null hypothesis stated that the mean accuracy of cogfuse-mobilevit was equal to that of mobilevitv3 h 0 : &#x3bc; c og = &#x3bc; m o b ), while the alternative hypothesis stated that cogfuse-mobilevit had a higher mean accuracy ( h 1 : &#x3bc; c og &gt; &#x3bc; m o b )cogfuse-mobilevit achieved a mean accuracy, significantly outperforming mobilevitv3. the independent samples t-test confirmed this improvement (t(8)=18.92,p=3.7&#xd7;10 &#x2212;8 ). table&#xa0;8 table&#xa0;8 . comparison of accuracy results between mobilevitv3 and cogfuse-mobilevit using independent samples t-test. under 5-fold cross-validation ( figure&#xa0;10 ), cogfuse-mobilevit achieves rapid convergence within the first 20 epochs, demonstrating more efficient feature learning capability. upon entering the steady-state phase, its validation loss is reduced by over 5-fold compared to the baseline mobilevitv3, directly confirming smaller prediction errors and superior generalization capability. meanwhile, the smooth and minimally fluctuating loss curve of cogfuse-mobilevit reflects strong robustness against data noise and distribution variations. combined with the previous statistical findings from accuracy confidence intervals and independent t-tests, these results collectively validate the statistical significance of the improved model. figure&#xa0;10 figure&#xa0;10 . comparison of loss curves between the cogfuse-mobilevit model and the original model in 5-fold cross-validation within 120 epochs. 3.3 model result analysis performance comparison of different models figure&#xa0;11a shows that in the walnut leaf brown spot disease severity grading task, the classification accuracy of all models gradually improved and stabilized with the increase of training epochs, indicating that the models continuously optimized during learning until convergence. cogfuse-mobilevit stood out prominently: it achieved rapid accuracy improvement, reached a high level in relatively early training epochs with minimal subsequent fluctuations, and stably maintained the highest accuracy, demonstrating fast convergence and strong generalization ability to efficiently extract features distinguishing different disease levels. densenet also maintained high accuracy in the late training stage, but its accuracy improvement was slower, with slightly inferior convergence speed and final stability compared to cogfuse-mobilevit. models like resnet and regnet showed limited accuracy gains with gentle upward trends, and their final stable accuracy values were significantly lower, reflecting insufficient feature extraction capabilities possibly due to network architecture or parameter optimization efficiency. figure&#xa0;11 figure&#xa0;11 . comparison of accuracy and loss across different models (a) accuracy line chart; (b) loss line chart. in figure&#xa0;11b , all models showed high initial loss values that dropped rapidly and then stabilized, following the typical learning process of iterative optimization. cogfuse-mobilevit achieved significant early loss decline and stabilized near the minimum, indicating efficient feature learning and strong fitting capability. while densenet also reached a relatively low loss level, it remained slightly higher than cogfuse-mobilevit. other models had higher late-stage loss values: some showed fast initial decline but converged to higher levels, indicating shortcomings in capturing key disease features. 3.4 analysis of detection results for different classification models the confusion matrix is a key indicator for evaluating classification models: the higher the diagonal values, the higher the classification accuracy for the corresponding category, and the lower the off-diagonal values, the fewer misjudgments. figure&#xa0;12 shows the classification results of different models for the four disease grades (0&#x2013;3) of walnut leaf brown spot disease. it is clearly evident that the overall classification performance, particularly the accuracy of cogfuse-mobilevit across all categories, is remarkably high with minimal misjudgments, demonstrating that the model has enhanced discrimination ability for disease grades and performs optimally in distinguishing the four disease levels. figure&#xa0;12 figure&#xa0;12 . confusion matrices of the improved cogfuse-mobilevit model and traditional classification models (a) densenet; (b) efficientnet; (c) efficientnetv2; (d) swin transformer; (e) mobilevitv3; (f) cogfuse-mobilevit. the edge-enhanced feature module (ecfm) effectively addressed level 0 and level 1 misclassification by capturing incipient lesion features. baseline analysis revealed substantial false negatives for early-stage lesions, with level 1 and level 0 misclassification reaching 32%. ecfm reduced this rate to 8%, demonstrating its capacity to resolve texture confusion through enhanced edge contour delineation. similarly, the adaptive multi-scale dilated convolution module (amsddicm) significantly mitigated level 2 and level 3 mutual misclassification. amsddicm drastically reduced both errors by enhancing local fine-grained features in mid-stage lesions (level 2) while strengthening global fusion features in late-stage coalesced lesions (level 3). this resolves grading ambiguity arising from the morphological continuum of lesion progression. to establish a comprehensive performance evaluation framework and quantify the model&#x2019;s overall discriminative capability, figure&#xa0;13 presents the receiver operating characteristic (roc) curves of cogfuse-mobilevit across four severity levels. these curves compare the true positive rate (tpr) against the false positive rate (fpr) by dynamically adjusting the classification threshold. the area under the curve (auc) serves as the primary performance metric, where a higher value indicates stronger classification ability. notably, the auc values for all categories significantly exceed the level of random guessing (auc=0.5), fully demonstrating that the model possesses robust discriminative capability across different severity levels. figure&#xa0;13 figure&#xa0;13 . roc curves of the four different severity levels for cogfuse-mobilevit. 3.5 tsne visualization of features extracted by different models the tsne visualization of features extracted from the models is shown in figures&#xa0;14a&#x2013;d . this study analyzed the features of the original model at the 20th, 50th, and 100th training epochs, as well as the improved model at the 100th epoch. after 20 epochs of training, the original model showed a highly discrete feature distribution. limited by the number of training epochs, the model failed to fully learn discriminative features, resulting in insufficient distinction between categories and significant overlap among different classes. this indicates that under this training intensity, the original model&#x2019;s ability to capture meaningful patterns was relatively limited. when the original model was trained to 50 epochs, compared with (a), the feature aggregation significantly improved. however, inter-class overlap still existed, suggesting that although prolonged training aided feature learning, the original model&#x2019;s architecture had inherent defects in achieving clear feature separation. at 100 epochs of training, the original model exhibited more prominent feature clustering. nevertheless, some regions still lacked clear separation between different categories, indicating that even after prolonged training, the original model faced challenges in maximizing inter-class distance and intra-class compactness. in figure (d), the improved model after 100 epochs of training demonstrated a more superior feature distribution: each category was tightly clustered, achieving high intra-class compactness, while distinct boundaries between different categories were established, resulting in significant inter-class distance. this suggests that the model improvements effectively enhanced its ability to extract discriminative features, greatly reduced feature ambiguity, and improved feature discriminative power. compared with the original model, it showed stronger feature discrimination and optimization potential. figure&#xa0;14 figure&#xa0;14 . tsne visualization of features extracted by different models. after the three modules work synergistically, the boundaries of feature clustering between level 0 (healthy) and level 1 (early-stage) are significantly clearer, which verifies the effectiveness of edge-texture collaborative learning (a) the original model was trained for 20 epochs; (b) the original model was trained for 50 epochs; (c) the original model was trained for 100 epochs; (d) the improved model was trained for 100 epochs. 3.6 radar chart for comparison of classification performance between original and improved models as shown in figure&#xa0;15 , a radar chart compares the performance of the original model and the improved cogfuse-mobilevit model across multiple evaluation metrics. in terms of accuracy, cogfuse-mobilevit exhibits significantly higher values than the original mobilevitv3 model, indicating that the improved model achieves overall higher classification accuracy. macro-average precision, which considers the average precision of each category, clearly shows that cogfuse-mobilevit also performs better, meaning it has superior precision across all categories. meanwhile, cogfuse-mobilevit also demonstrates better macro-average recall. when examining weighted-average precision and weighted-average recall&#x2014;metrics that account for class imbalance&#x2014;cogfuse-mobilevit outperforms mobilevitv3 in both, indicating that in practical applications, even with uneven class distribution, the cogfuse-mobilevit model can deliver better performance. these results further demonstrate the model&#x2019;s reliability and practicality. figure&#xa0;15 figure&#xa0;15 . multi-metric comparison between mobilevitv3 and improved cogfuse-mobilevit model. 3.7 public data set experiment to further validate the efficacy and generalizability of the proposed cogfuse-mobilevit model, experiments were conducted on the public dataset appleleaf9 ( yang et&#xa0;al., 2022 ). this dataset comprises healthy apple leaves and eight categories of apple leaf diseases captured in field environments without restrictions on imaging angles or noise interference. the dataset was partitioned into training and test sets at an 8:2 ratio. all images were resized to 224&#xd7;224 pixels to optimize deep learning model training efficiency. following the hyperparameter configurations specified in &#x201c;experimental parameters and evaluation metrics&#x201d;, both the baseline mobilevitv3 and cogfuse-mobilevit models were trained and evaluated. cross-species validation demonstrates that the cogfuse-mobilevit model delivers exceptional performance on the appleleaf9 dataset. as presented in table&#xa0;9 , the model comprehensively surpasses the baseline mobilevitv3 across all four core metrics: precision increases by 0.16 percentage points, recall achieves a breakthrough improvement of 3.45 percentage points, f1-score rises by 1.18 percentage points, and accuracy elevates by 1.14 percentage points. the synergistic enhancement in both precision and recall signifies that performance gains stem from strengthened feature discriminability rather than metric trade-off compromises. the remarkable percentage points recall gain substantially mitigates leaf disease omission rates, while the holistic advance in f1-score further attests to the model&#x2019;s robustness. these results collectively validate the generalizability of cogfuse-mobilevit&#x2019;s core innovations in agricultural fine-grained disease grading, establishing a transferable paradigm for small-target pathology identification across plant species. table&#xa0;9 table&#xa0;9 . experimental results of the original model and cogfuse-mobilevit model on appleleaf9 data set. 4 conclusion this study addresses the challenges in precise grading of walnut leaf brown spot disease. by adopting dynamic feature filtering, edge gradient reinforcement, and multi-scale morphological adaptation, it effectively resolves three key limitations of existing hybrid architectures and the baseline model mobilevitv3 in plant disease grading weak capability in capturing blurred edges, poor multi-scale adaptability, and interference from healthy tissues. experimental results show that the model achieves an 86.61% grading accuracy on a dataset encompassing diverse lighting conditions, cultivars, and lesion stages, representing a 7.80% improvement over the baseline mobilevitv3 and outperforming nine state-of-the-art models. at the same time, this study confirmed that the performance improvement of cogfuse-mobilevit was statistically significant and stable through strict statistical tests, providing a reliable method for accurate classification of walnut leaf spot disease. the constructed image dataset and proposed grading re-measurement method lay the foundation for accuracy. this approach provides a new paradigm for small-target disease grading, with core modules transferable to multi-crop disease recognition. beyond disease classification, context-aware ai frameworks demonstrate significant efficacy in agricultural management ( acharya et&#xa0;al., 2022 ). iot systems dynamically adjust fertilization recommendations by integrating real-time soil-crop data, while salinity-corrected evapotranspiration (ets) models optimize irrigation strategies for saline-alkali soils ( li et&#xa0;al., 2023 ; su et&#xa0;al., 2023 ). similarly, multimodal fusion techniques enhance reference evapotranspiration (eto) prediction accuracy, facilitating precision water allocation ( jo et&#xa0;al., 2021 ; rustia et&#xa0;al., 2023 ). these breakthroughs collectively validate the robust capability of adaptive feature processing in complex agricultural environments&#x2014;a core principle that resonates with our dynamic weighting strategy for disease feature extraction. future research should therefore integrate cross-modal and cross-domain capabilities to establish multidimensional assessment systems and regional dynamic monitoring frameworks, thereby delivering comprehensive technical solutions for small-target disease control. 5 discussion and future work compared with existing methods, this study overcomes the limitation of traditional deep learning models relying on fixed convolution kernels, enabling adaptive capture of lesion features with multi-shaped convolution kernels to accurately extract edge textures of circular micro-lesions and regional contours of irregular lesions. although the constructed multi-source dataset covers diverse lighting conditions, cultivars, and lesion development stages, the homogeneous internal features of severe diseases lead to limited recall rate. meanwhile, when lesions are severely overlapped or mixed with mechanical damage, insect damage, or other types of injuries, the discriminative accuracy of the model is affected. for ultra-small lesions, the local feature information is too weak and easily overlooked, and the computational efficiency still needs optimization on some hardware platforms. furthermore, the adaptability of the current model in extreme field scenarios, such as high-density occlusion and compound damage from pests and diseases, remains to be further verified. in these scenarios, the coupling of complex interference factors exacerbates feature confusion, affecting grading reliability. to address these issues, future work will focus on the following research directions. future research will focus on enhancing the model&#x2019;s performance in complex field environments through multiple synergistic strategies. this includes constructing multi-interference factor coupled datasets that incorporating insect holes, lesions, and soil adhesion to strengthen robustness against extreme field disturbances; introducing attention-based multi-damage feature decoupling modules and dedicated micro-lesion enhancement modules combining super-resolution and feature interpolation to improve discriminability in complex scenarios and for tiny lesions;&#xa0;optimizing dynamic weight calculations via approximate computation or hardware-friendly reconstruction, while developing lightweight real-time deployment frameworks integrated with uav near-ground sensing technology to boost efficiency and practical monitoring coverage; and establishing cross-crop pathological transfer learning mechanisms, extending the model to multi-crop disease recognition tasks with self-supervised pre-training to enhance algorithm universality and low-contrast lesion feature mining capabilities. through the above research, it is expected to further improve the applicability and practicality of the model in complex field environments, promoting the development of intelligent plant disease grading technology towards more accurate, efficient, and universal directions. data availability statement the raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. author contributions yw: writing &#x2013; original draft. dz: writing &#x2013; original draft. lz:&#xa0;writing &#x2013; review &amp; editing. funding the author(s) declare financial support was received for the research and/or publication of this article. this work was supported in part by the science and technology key project of the xinjiang production and construction corps (2023ab063) development and application demonstration of green technology for online monitoring of fresh fruits and cold chain logistics in xinjiang. conflict of interest the authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. generative ai statement the author(s) declare that no generative ai was used in the creation of this manuscript. any alternative text (alt text) provided alongside figures in this article has been generated by frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. if you identify any issues, please contact us. publisher&#x2019;s note all claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. references acharya, p., burgers, t., and nguyen, k.-d. (2022). ai-enabled droplet detection and tracking for agricultural spraying systems. computers and electronics in agriculture , 202, 107325. doi:&#xa0;10.1016/j.compag.2022.107325 crossref full text | google scholar adaskaveg, j., f&#xf6;rster, h., thompson, d., driever, g., connell, j., buchner, r., et al. (2009). epidemiology and management of walnut blight. walnut res. rep , 94, 225&#x2013;256. doi:&#xa0;10.1016/j.inpa.2016.10.005 crossref full text | google scholar arivazhagan, s., shebiah, r. n., ananthi, s., and varthini, s. v. (2013). detection of unhealthy region of plant leaves and classification of plant leaf diseases using texture features. agricultural engineering international: cigr journal , 15, 211&#x2013;217. google scholar chaudhary, p., chaudhari, a. k., cheeran, a., and godara, s. (2012). color transform based approach for disease spot detection on plant leaf. international journal of computer science and telecommunications , 3, 65&#x2013;70. google scholar chen, s., zhang, k., zhao, y., sun, y., ban, w., chen, y., et al. (2021). an approach for rice bacterial leaf streak disease segmentation and disease severity estimation. agriculture , 11, 420. doi:&#xa0;10.3390/agriculture11050420 crossref full text | google scholar chiang, k.-s., bock, c., el jarroudi, m., delfosse, p., lee, i., and liu, h. (2016). effects of rater bias and assessment method on disease severity estimation with regard to hypothesis testing. phytopathology , 65, 523&#x2013;535. doi:&#xa0;10.1111/ppa.12435 crossref full text | google scholar cooke, b. (2006). &#x201c;disease assessment and yield loss,&#x201d; in the epidemiology of plant diseases (dordrecht: springer), 43&#x2013;80. google scholar ghaiwat, s. n. and arora, p. (2014). detection and classification of plant leaf diseases using image processing techniques: a review. international journal of recent advances in engineering &amp; technology , 2, 1&#x2013;7. google scholar hu, g., wang, h., zhang, y., and wan, m. (2021). detection and severity analysis of tea leaf blight based on deep learning. computers &amp; electrical engineering , 90, 107023. doi:&#xa0;10.1016/j.compeleceng.2021.107023 crossref full text | google scholar jadhav, s. b. and patil, s. b. (2016). grading of soybean leaf disease based on segmented image using k-means clustering. iaes international journal of artificial intelligence , 5, 13&#x2013;13. doi:&#xa0;10.11591/ijai.v5.i1.pp13-21 crossref full text | google scholar jo, w. j., kim, d. s., sim, h. s., ahn, s. r., lee, h. j., moon, y. h., et al. (2021). estimation of evapotranspiration and water requirements of strawberry plants in greenhouses using environmental data. frontiers in sustainable food systems , 5, 684808. doi:&#xa0;10.3389/fsufs.2021.684808 crossref full text | google scholar khan, m. a., ali, m., shah, m., mahmood, t., ahmad, m., jhanjhi, n., et al. (2021). machine learning-based detection and classification of walnut fungi diseases. intelligent automation &amp; soft computing , 30, 771&#x2013;785. doi:&#xa0;10.32604/iasc.2021.018039 crossref full text | google scholar kim, s.-k. and ahn, j.-g. (2021). tomato crop diseases classification models using deep cnn-based architectures. j korea acad-ind coop soc , 22, 7&#x2013;14. doi:&#xa0;10.5762/kais.2021.22.5.7 crossref full text | google scholar lamichhane, j. r. (2014). xanthomonas arboricola diseases of stone fruit, almond, and walnut trees: progress toward understanding and management. plant disease , 98, 1600&#x2013;1610. doi:&#xa0;10.1094/pdis-08-14-0831-fe pubmed abstract | crossref full text | google scholar li, x., zha, t., liu, p., bourque, c. p.-a., jia, x., and tian, y. (2023). interannual variation in gross ecosystem production and evapotranspiration in a temperate semiarid grassland undergoing vegetation recovery. agricultural and forest meteorology , 341, 109672. doi:&#xa0;10.1016/j.agrformet.2023.109672 crossref full text | google scholar lin, k., gong, l., huang, y., liu, c., and pan, j. (2019). deep learning-based segmentation and quantification of cucumber powdery mildew using convolutional neural network. frontiers in plant science , 10, 155. doi:&#xa0;10.3389/fpls.2019.00155 pubmed abstract | crossref full text | google scholar ma, j., du, k., zhang, l., zheng, f., chu, j., and sun, z. (2017). a segmentation method for greenhouse vegetable foliar disease spots images using color information and region growing. computers and electronics in agriculture , 142, 110&#x2013;117. doi:&#xa0;10.1016/j.compag.2017.08.023 crossref full text | google scholar mao, r., wang, z., li, f., zhou, j., chen, y., and hu, x. (2023). gseyolox-s: an improved lightweight network for identifying the severity of wheat fusarium head blight. agronomy , 13, 242. doi:&#xa0;10.3390/agronomy13010242 crossref full text | google scholar mcgranahan, g. and leslie, c. (1991). walnuts (juglans). molecules , 907&#x2013;924. doi:&#xa0;10.17660/actahortic.1991.290.20 crossref full text | google scholar mircetich, s. m., sanborn, r., and ramos, d. (1980). natural spread, graft-transmission, and possible etiology of walnut blackline disease. phytopathology , 70, 962&#x2013;968. doi:&#xa0;10.1094/phyto-70-962 crossref full text | google scholar moragrega, c., matias, j., alet&#xe0;, n., montesinos, e., and rovira, m. (2011). apical necrosis and premature drop of persian (english) walnut fruit caused by xanthomonas arboricola pv. juglandis. plant disease , 95, 1565&#x2013;1570. doi:&#xa0;10.1094/pdis-03-11-0259 pubmed abstract | crossref full text | google scholar ngugi, l. c., abdelwahab, m., and abo-zahhad, m. (2020). tomato leaf segmentation algorithms for mobile phone applications using deep learning. computers and electronics in agriculture , 178, 105788. doi:&#xa0;10.1016/j.compag.2020.105788 crossref full text | google scholar ozturk, o., sarica, b., and seker, d. z. (2025). interpretable and robust ensemble deep learning framework for tea leaf disease classification. horticulturae , 11, 437. doi:&#xa0;10.3390/horticulturae11040437 crossref full text | google scholar parashar, n., johri, p., khan, a. a., gaur, n., and kadry, s. (2024). an integrated analysis of yield prediction models: a comprehensive review of advancements and challenges. computers, materials &amp; continua , 80 (1). doi:&#xa0;10.32604/cmc.2024.050240 crossref full text | google scholar picon, a., alvarez-gila, a., seitz, m., ortiz-barredo, a., echazarra, j., and johannes, a. (2019). deep convolutional neural networks for mobile capture device-based crop disease classification in the wild. computers and electronics in agriculture , 161, 280&#x2013;290. doi:&#xa0;10.1016/j.compag.2018.04.002 crossref full text | google scholar rastogi, a., arora, r., and sharma, s. (2015). &#x201c;leaf disease detection and grading using computer vision technology &amp; fuzzy logic,&#x201d; in paper presented at the 2015 2nd international conference on signal processing and integrated networks (spin). google scholar rustia, d. j. a., lee, w.-c., lu, c.-y., wu, y.-f., shih, p.-y., and chen, s.-k. (2023). edge-based wireless imaging system for continuous monitoring of insect pests in a remote outdoor mango orchard. computers and electronics in agriculture , 211, 108019. doi:&#xa0;10.1016/j.compag.2023.108019 crossref full text | google scholar shi, t., liu, y., zheng, x., hu, k., huang, h., liu, h., et al. (2023). recent advances in plant disease severity assessment using convolutional neural networks. scientific reports , 13, 2336. doi:&#xa0;10.1038/s41598-023-29230-7 pubmed abstract | crossref full text | google scholar singh, u. p., chouhan, s. s., jain, s., and jain, s. (2019). multilayer convolution neural network for the classification of mango leaves infected by anthracnose disease. ieee access , 7, 43721&#x2013;43729. doi:&#xa0;10.1109/access.2019.2907383 crossref full text | google scholar su, y., wang, j., li, j., wang, l., wang, k., li, a., et al. (2023). spatiotemporal changes and driving factors of reference evapotranspiration and crop evapotranspiration for cotton production in china from 1960 to 2019. frontiers in environmental science , 11, 1251789. doi:&#xa0;10.3389/fenvs.2023.1251789 crossref full text | google scholar tripathi, r. (2021). &#x201c;a deep learning approach for plant material disease identification,&#x201d; in paper presented at the iop conference series: materials science and engineering. google scholar vishnoi, v. k., kumar, k., kumar, b., mohan, s., and khan, a. a. (2022). detection of apple plant diseases using leaf images through convolutional neural network . ieee access , 11, 6594&#x2013;6609. doi:&#xa0;10.1109/access.2022.3232917 crossref full text | google scholar wang, f., dun, c., tang, t., duan, y., guo, x., and you, j. (2022). boeremia exigua causes leaf spot of walnut trees (juglans regia) in china. plant disease , 106, 1993. doi:&#xa0;10.1094/pdis-10-21-2304-pdn crossref full text | google scholar wang, q.-h., fan, k., li, d.-w., han, c.-m., qu, y.-y., qi, y.-k., et al. (2020). identification, virulence and fungicide sensitivity of colletotrichum gloeosporioides ss responsible for walnut anthracnose disease in china. plant disease , 104, 1358&#x2013;1368. doi:&#xa0;10.1094/pdis-12-19-2569-re pubmed abstract | crossref full text | google scholar weber, b. c. (1980). how to diagnose black walnut damage vol. 57 (north central forest experiment station, forest service, us department of agriculture). google scholar xu, w. (2024). hcf-net: hierarchicalcontextfusion network forinfrared small object detection. arxiv . arxiv, 2403.10778. doi:&#xa0;10.1109/icme57554.2024.10687431 crossref full text | google scholar yang, c., deng, y., wang, f., yang, h., xu, x., zeng, q., et al. (2021). brown leaf spot on juglans sigillata caused by ophiognomonia leptostyla in sichuan, china. plant disease , 105, 4160. doi:&#xa0;10.1094/pdis-02-21-0344-pdn crossref full text | google scholar yang, q., duan, s., and wang, l. (2022). efficient identification of apple leaf diseases in the wild using convolutional neural networks. agronomy , 12, 2784. doi:&#xa0;10.3390/agronomy12112784 crossref full text | google scholar zarei, s., taghavi, s. m., banihashemi, z., hamzehzarghani, h., and osdaghi, e. (2019). etiology of leaf spot and fruit canker symptoms on stone fruits and nut trees in iran. journal of plant pathology , 101, 1133&#x2013;1142. doi:&#xa0;10.1007/s42161-019-00283-w crossref full text | google scholar zhang, y., li, x., &#x160;im&#x16f;nek, j., shi, h., chen, n., and hu, q. (2023). quantifying water and salt movement in a soil-plant system of a corn field using hydrus (2d/3d) and the stable isotope method. agricultural water management , 288, 108492. doi:&#xa0;10.1016/j.agwat.2023.108492 crossref full text | google scholar keywords: walnut, brown spot disease ( ophiognomonia leptostyla ), hierarchical feature selection, edge features perception, adaptive multi-scale dilated convolution, disease grading citation: wei y, zeng d and zheng l (2025) a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution. front. plant sci. 16:1641677. doi: 10.3389/fpls.2025.1641677 received: 05 june 2025; accepted: 09 september 2025; published: 03 october 2025. edited by: ravinder kumar , indian agricultural research institute (icar), india reviewed by: geza bujdoso , hungarian university of agricultural and life sciences, hungary vasudha vedula , university of texas of the permian basin, united states copyright &#xa9; 2025 wei, zeng and zheng. this is an open-access article distributed under the terms of the creative commons attribution license (cc by) . the use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. no use, distribution or reproduction is permitted which does not comply with these terms. *correspondence: liangfang zheng, mtgxnjaynta0othamtyzlmnvbq== &#x2020; these authors have contributed equally to this work disclaimer: all claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. download article download pdf readcube epub xml share on export citation endnote reference manager simple text file bibtex 1,063 total views 103 downloads citation numbers are available from dimensions view article impact view altmetric score share on edited by r k ravinder kumar reviewed by g b geza bujdoso v v vasudha vedula table of contents abstract 1 introduction 2 materials and methods 3 experiments and results analysis 4 conclusion 5 discussion and future work data availability statement author contributions funding conflict of interest generative ai statement publisher&#x2019;s note references export citation endnote reference manager simple text file bibtex check for updates frontiers&#39; impact articles published with frontiers have received 12 million total citations your research is the real superpower - learn how we maximise its impact through our leading community journals explore our impact metrics supplementary material download article download download pdf readcube epub xml guidelines author guidelines services for authors policies and publication ethics editor guidelines fee policy explore articles research topics journals how we publish outreach frontiers forum frontiers policy labs frontiers for young minds frontiers planet prize connect help center emails and alerts contact us submit career opportunities follow us © 2025 frontiers media s.a. all rights reserved privacy policy | terms and conditions [["shallowreactive",1],{"data":2,"state":4,"once":10,"_errors":11,"serverrendered":13,"path":14,"pinia":15},["shallowreactive",3],{},["reactive",5],{"$ssite-config":6},{"env":7,"name":8,"url":9},"production","frontiers articles","https://article-pages-2024.frontiersin.org/",["set"],["shallowreactive",12],{},true,"/journals/plant-science/articles/10.3389/fpls.2025.1641677/full",["reactive",16],{"main":17,"user":534,"article":535,"articlehub":760,"mainheader":764},{"ibar":18,"footer":284,"newslettercomponent":-1,"snackbaritem":366,"toggleshowsnackbar":367,"contentfuljournal":368,"graphjournal":434,"settingsfeaturesswitchers":438,"templatetogglebanner":439,"tenantconfig":499},{"tenantlogo":19,"journallogo":19,"aboutus":20,"submiturl":113,"showsubmitbutton":13,"journal":114,"sectionterm":215,"aboutjournal":216,"mainlinks":265,"journallinks":272,"helpcenterlink":281},"",[21,38,47,71,87],{"title":22,"links":23},"who we are",[24,29,32,35],{"text":25,"url":26,"target":27,"arialabel":28},"mission and values","https://www.frontiersin.org/about/mission","_self",null,{"text":30,"url":31,"target":27,"arialabel":28},"history","https://www.frontiersin.org/about/history",{"text":33,"url":34,"target":27,"arialabel":28},"leadership","https://www.frontiersin.org/about/leadership",{"text":36,"url":37,"target":27,"arialabel":28},"awards","https://www.frontiersin.org/about/awards",{"title":39,"links":40},"impact and progress",[41,44],{"text":42,"url":43,"target":27,"arialabel":28},"frontiers' impact","https://www.frontiersin.org/about/impact",{"text":45,"url":46,"target":27,"arialabel":28},"our annual reports","https://www.frontiersin.org/about/annual-reports",{"title":48,"links":49},"publishing model",[50,53,56,59,62,65,68],{"text":51,"url":52,"target":27,"arialabel":28},"how we publish","https://www.frontiersin.org/about/how-we-publish",{"text":54,"url":55,"target":27,"arialabel":28},"open access","https://www.frontiersin.org/about/open-access",{"text":57,"url":58,"target":27,"arialabel":28},"peer review","https://www.frontiersin.org/about/peer-review",{"text":60,"url":61,"target":27,"arialabel":28},"research integrity","https://www.frontiersin.org/about/research-integrity",{"text":63,"url":64,"target":27,"arialabel":28},"research topics","https://www.frontiersin.org/about/research-topics",{"text":66,"url":67,"target":27,"arialabel":28},"fair² data management","https://www.frontiersin.org/about/fair-data-management",{"text":69,"url":70,"target":27,"arialabel":28},"fee policy","https://www.frontiersin.org/about/fee-policy",{"title":72,"links":73},"services",[74,78,81,84],{"text":75,"url":76,"target":77,"arialabel":28},"societies","https://publishingpartnerships.frontiersin.org/","_blank",{"text":79,"url":80,"target":27,"arialabel":28},"national consortia","https://www.frontiersin.org/open-access-agreements/consortia",{"text":82,"url":83,"target":27,"arialabel":28},"institutional partnerships","https://www.frontiersin.org/about/open-access-agreements",{"text":85,"url":86,"target":27,"arialabel":28},"collaborators","https://www.frontiersin.org/about/collaborators",{"title":88,"links":89},"more from frontiers",[90,94,97,101,105,109],{"text":91,"url":92,"target":77,"arialabel":93},"frontiers forum","https://forum.frontiersin.org/","this link will take you to the frontiers forum website",{"text":95,"url":96,"target":27,"arialabel":28},"frontiers planet prize","https://www.frontiersin.org/about/frontiers-planet-prize",{"text":98,"url":99,"target":77,"arialabel":100},"press office","https://pressoffice.frontiersin.org/","this link will take you to the frontiers press office website",{"text":102,"url":103,"target":27,"arialabel":104},"sustainability","https://www.frontiersin.org/about/sustainability","link to information about frontiers' sustainability",{"text":106,"url":107,"target":77,"arialabel":108},"career opportunities","https://careers.frontiersin.org/","this link will take you to the frontiers careers website",{"text":110,"url":111,"target":27,"arialabel":112},"contact us","https://www.frontiersin.org/about/contact","this link will take you to the help pages to contact our support team","https://www.frontiersin.org/submission/submit?domainid=1&fieldid=66&specialtyid=0&entitytype=2&entityid=373",{"id":115,"name":116,"slug":117,"sections":118},373,"frontiers in plant science","plant-science",[119,123,127,131,135,139,143,147,151,155,159,163,167,171,175,179,183,187,191,195,199,203,207,211],{"id":120,"name":121,"slug":122},1553,"aquatic photosynthetic organisms","aquatic-photosynthetic-organisms",{"id":124,"name":125,"slug":126},1356,"crop and product physiology","crop-and-product-physiology",{"id":128,"name":129,"slug":130},467,"functional plant ecology","functional-plant-ecology",{"id":132,"name":133,"slug":134},2844,"functional and applied plant genomics","functional-and-applied-plant-genomics",{"id":136,"name":137,"slug":138},2843,"photosynthesis and photobiology","photosynthesis-and-photobiology",{"id":140,"name":141,"slug":142},1312,"plant abiotic stress","plant-abiotic-stress",{"id":144,"name":145,"slug":146},2183,"plant bioinformatics","plant-bioinformatics",{"id":148,"name":149,"slug":150},589,"plant biophysics and modeling","plant-biophysics-and-modeling",{"id":152,"name":153,"slug":154},560,"plant biotechnology","plant-biotechnology",{"id":156,"name":157,"slug":158},468,"plant breeding","plant-breeding",{"id":160,"name":161,"slug":162},577,"plant cell biology","plant-cell-biology",{"id":164,"name":165,"slug":166},474,"plant development and evodevo","plant-development-and-evodevo",{"id":168,"name":169,"slug":170},2845,"plant genetics, epigenetics and chromosome biology","plant-genetics-epigenetics-and-chromosome-biology",{"id":172,"name":173,"slug":174},479,"plant membrane traffic and transport","plant-membrane-traffic-and-transport",{"id":176,"name":177,"slug":178},481,"plant metabolism and chemodiversity","plant-metabolism-and-chemodiversity",{"id":180,"name":181,"slug":182},486,"plant nutrition","plant-nutrition",{"id":184,"name":185,"slug":186},485,"plant pathogen interactions","plant-pathogen-interactions",{"id":188,"name":189,"slug":190},226,"plant physiology","plant-physiology",{"id":192,"name":193,"slug":194},580,"plant proteomics and protein structural biology","plant-proteomics-and-protein-structural-biology",{"id":196,"name":197,"slug":198},1570,"plant symbiotic interactions","plant-symbiotic-interactions",{"id":200,"name":201,"slug":202},1428,"plant systematics and evolution","plant-systematics-and-evolution",{"id":204,"name":205,"slug":206},483,"plant systems and synthetic biology","plant-systems-and-synthetic-biology",{"id":208,"name":209,"slug":210},2277,"sustainable and intelligent phytoprotection","sustainable-and-intelligent-phytoprotection",{"id":212,"name":213,"slug":214},484,"technical advances in plant science","technical-advances-in-plant-science","sections",[217,241],{"title":218,"links":219},"scope",[220,223,226,229,232,235,238],{"text":221,"url":222,"target":27,"arialabel":28},"field chief editors","https://www.frontiersin.org/journals/plant-science/about#about-editors",{"text":224,"url":225,"target":27,"arialabel":28},"mission & scope","https://www.frontiersin.org/journals/plant-science/about#about-scope",{"text":227,"url":228,"target":27,"arialabel":28},"facts","https://www.frontiersin.org/journals/plant-science/about#about-facts",{"text":230,"url":231,"target":27,"arialabel":28},"journal sections","https://www.frontiersin.org/journals/plant-science/about#about-submission",{"text":233,"url":234,"target":27,"arialabel":28},"open access statement","https://www.frontiersin.org/journals/plant-science/about#about-open",{"text":236,"url":237,"target":27,"arialabel":28},"copyright statement","https://www.frontiersin.org/journals/plant-science/about#copyright-statement",{"text":239,"url":240,"target":27,"arialabel":28},"quality","https://www.frontiersin.org/journals/plant-science/about#about-quality",{"title":242,"links":243},"for authors",[244,247,250,253,256,259,262],{"text":245,"url":246,"target":27,"arialabel":28},"why submit?","https://www.frontiersin.org/journals/plant-science/for-authors/why-submit",{"text":248,"url":249,"target":27,"arialabel":28},"article types","https://www.frontiersin.org/journals/plant-science/for-authors/article-types",{"text":251,"url":252,"target":27,"arialabel":28},"author guidelines","https://www.frontiersin.org/journals/plant-science/for-authors/author-guidelines",{"text":254,"url":255,"target":27,"arialabel":28},"editor guidelines","https://www.frontiersin.org/journals/plant-science/for-authors/editor-guidelines",{"text":257,"url":258,"target":27,"arialabel":28},"publishing fees","https://www.frontiersin.org/journals/plant-science/for-authors/publishing-fees",{"text":260,"url":261,"target":27,"arialabel":28},"submission checklist","https://www.frontiersin.org/journals/plant-science/for-authors/submission-checklist",{"text":263,"url":264,"target":27,"arialabel":28},"contact editorial office","https://www.frontiersin.org/journals/plant-science/for-authors/contact-editorial-office",[266,269],{"text":267,"url":268,"target":27,"arialabel":28},"all journals","https://www.frontiersin.org/journals",{"text":270,"url":271,"target":27,"arialabel":28},"all articles","https://www.frontiersin.org/articles",[273,276,278],{"text":274,"url":275,"target":27,"arialabel":28},"articles","articles",{"text":63,"url":277,"target":27,"arialabel":28},"research-topics",{"text":279,"url":280,"target":27,"arialabel":28},"editorial board","editors",{"text":282,"url":283,"target":77,"arialabel":282},"help center","https://helpcenter.frontiersin.org",{"blocks":285,"sociallinks":339,"copyright":363,"termsandconditionsurl":364,"privacypolicyurl":365},[286,300,310,324],{"title":287,"links":288},"guidelines",[289,291,294,297,299],{"text":251,"url":290,"target":27,"arialabel":28},"https://www.frontiersin.org/guidelines/author-guidelines",{"text":292,"url":293,"target":27,"arialabel":28},"services for authors","https://www.frontiersin.org/for-authors/author-services",{"text":295,"url":296,"target":27,"arialabel":28},"policies and publication ethics","https://www.frontiersin.org/guidelines/policies-and-publication-ethics",{"text":254,"url":298,"target":27,"arialabel":28},"https://www.frontiersin.org/guidelines/editor-guidelines",{"text":69,"url":70,"target":27,"arialabel":28},{"title":301,"links":302},"explore",[303,304,307,309],{"text":274,"url":271,"target":27,"arialabel":28},{"text":305,"url":306,"target":27,"arialabel":28},"research topics ","https://www.frontiersin.org/research-topics",{"text":308,"url":268,"target":27,"arialabel":28},"journals",{"text":51,"url":52,"target":27,"arialabel":28},{"title":311,"links":312},"outreach",[313,316,319,323],{"text":314,"url":92,"target":77,"arialabel":315},"frontiers forum ","frontiers forum website",{"text":317,"url":318,"target":77,"arialabel":28},"frontiers policy labs ","https://policylabs.frontiersin.org/",{"text":320,"url":321,"target":77,"arialabel":322},"frontiers for young minds","https://kids.frontiersin.org/","frontiers for young minds journal",{"text":95,"url":96,"target":27,"arialabel":28},{"title":325,"links":326},"connect",[327,328,332,335,338],{"text":282,"url":283,"target":77,"arialabel":282},{"text":329,"url":330,"target":77,"arialabel":331},"emails and alerts ","https://subscription-management.frontiersin.org/emails/preferences#block0","subscribe to frontiers emails",{"text":333,"url":111,"target":27,"arialabel":334},"contact us ","subscribe to newsletter",{"text":336,"url":337,"target":27,"arialabel":28},"submit","https://www.frontiersin.org/submission/submit",{"text":106,"url":107,"target":77,"arialabel":28},[340,348,353,358],{"link":341,"type":344,"color":345,"icon":346,"size":347,"hiddentext":13},{"text":342,"url":343,"target":77,"arialabel":342},"frontiers facebook","https://www.facebook.com/frontiersin","link","grey","facebook","medium",{"link":349,"type":344,"color":345,"icon":352,"size":347,"hiddentext":13},{"text":350,"url":351,"target":77,"arialabel":28},"frontiers twitter","https://twitter.com/frontiersin","twitter",{"link":354,"type":344,"color":345,"icon":357,"size":347,"hiddentext":13},{"text":355,"url":356,"target":77,"arialabel":28},"frontiers linkedin","https://www.linkedin.com/company/frontiers","linkedin",{"link":359,"type":344,"color":345,"icon":362,"size":347,"hiddentext":13},{"text":360,"url":361,"target":77,"arialabel":28},"frontiers instagram","https://www.instagram.com/frontiersin_","instagram","frontiers media s.a. all rights reserved","https://www.frontiersin.org/legal/terms-and-conditions","https://www.frontiersin.org/legal/privacy-policy",{},false,{"__typename":369,"identifier":115,"name":116,"slug":117,"banner":370,"description":427,"mission":428,"palette":429,"impactfactor":430,"citescore":431,"citations":432,"showtagline":28,"twitter":433},"journal",[371],{"id":372,"src":373,"name":374,"tags":375,"type":385,"width":386,"height":387,"idhash":388,"archive":389,"brandid":390,"limited":389,"filesize":391,"ispublic":392,"original":393,"copyright":394,"extension":395,"thumbnails":397,"datecreated":405,"description":406,"orientation":407,"usercreated":408,"watermarked":389,"datemodified":405,"datepublished":409,"ecsarchivefiles":410,"propertyoptions":411,"property_channel":416,"property_sub-type":418,"property_asset_type":420,"activeoriginalfocuspoint":422,"property_office_department":425},"450e9326-0272-405c-b8d614c72bed9f89","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/webimage-00a7f7fd-61fc-4329-b3bfcc0119b4b276.jpg","fpls_main visual_green_website",[376,377,378,379,380,381,382,383,384],"medical","vitality","decoration","five","cosmetic","r","cure","aloe vera","spike","image",4928,3264,"720507a162925515",0,"22c10171-81b3-4da6-99342f272a32e8bb",11359471,1,"https://brand.frontiersin.org/m/720507a162925515/original/fpls_main-visual_green_website.jpeg","copyright (c) 2017 sabine hortebusch/shutterstock. no use without permission.",[396],"jpeg",{"mini":398,"thul":399,"webimage":373,"guidelines":400,"websitejpg_xl":401,"websitewebp_l":402,"websitewebp_m":403,"websitewebp_xl":404},"https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/mini-006feee0-2d70-4630-b9abfb0a0fa410aa.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/thul-c817db1b-00e8-4b29-b71c4e5a6058947f.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/guidelines-fpls_main visual_green_website.png","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitejpg_xl-fpls_main visual_green_website.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_l-fpls_main visual_green_website.webp","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_m-fpls_main visual_green_website.webp","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_xl-fpls_main visual_green_website.webp","2022-06-27t10:00:48z","spiral aloe vera with water drops, closeup","landscape","caroline sutter","2022-06-27t09:27:09z",[],[412,413,414,415],"414fb2d4-2283-43fd-be14e534eca67928","6c18119b-14bd-4951-b437696f4357bd33","7c692885-db25-4858-b1fb4ff47b241e9b","d88c0047-ec30-4506-a7df28a4d765e1cf",[417],"frontiersin_org",[419],"main_visual",[421],"photography",{"x":423,"y":424},2464,1632,[426],"publishing","the most cited plant science journal, advancing our understanding of plant biology for sustainable food security, functional ecosystems and human health.","\u003cp>frontiers in plant science is a leading, multidisciplinary journal that seeks to advance our understanding of fundamental processes in plant biology.\u003c/p>\n\n\u003cp>led by field chief editor prof. chun-ming liu (institute of botany, chinese academy of sciences) and indexed in pubmed, pubmed central, and scopus, among others, the journal seeks original and significant contributions that cultivate plant biology and its applications. the journal has the long-term goal of supporting sustainable development, food security, functional ecosystems, biotechnology (including biofuels and biomaterials), and human health.\u003c/p>\n\u003cp>frontiers in plant science welcomes original research, review, opinion, and perspective articles, among other submission types, covering the journal’s specialty sections:\u003c/p>\n\n\u003cdiv> &bull; aquatic photosynthetic organisms\u003c/div>\n\u003cdiv> &bull; crop and product physiology\u003c/div>\n\u003cdiv> &bull; functional plant ecology\u003c/div>\n\u003cdiv> &bull; functional and applied plant genomics\u003c/div>\n\u003cdiv> &bull; photosynthesis and photobiology\u003c/div>\n\u003cdiv> &bull; plant abiotic stress\u003c/div>\n\u003cdiv> &bull; plant bioinformatics\u003c/div>\n\u003cdiv> &bull; plant biophysics and modeling\u003c/div>\n\u003cdiv> &bull; plant biotechnology\u003c/div>\n\u003cdiv> &bull; plant breeding\u003c/div>\n\u003cdiv> &bull; plant cell biology\u003c/div>\n\u003cdiv> &bull; plant development and evodevo\u003c/div>\n\u003cdiv> &bull; plant genetics, epigenetics and chromosome biology\u003c/div>\n\u003cdiv> &bull; plant membrane traffic and transport\u003c/div>\n\u003cdiv> &bull; plant metabolism and chemodiversity\u003c/div>\n\u003cdiv> &bull; plant nutrition\u003c/div>\n\u003cdiv> &bull; plant pathogen interactions\u003c/div>\n\u003cdiv> &bull; plant physiology\u003c/div>\n\u003cdiv> &bull; plant proteomics and protein structural biology\u003c/div>\n\u003cdiv> &bull; plant symbiotic interactions\u003c/div>\n\u003cdiv> &bull; plant systematics and evolution\u003c/div>\n\u003cdiv> &bull; plant systems and synthetic biology\u003c/div>\n\u003cdiv> &bull; sustainable and intelligent phytoprotection\u003c/div>\n\u003cdiv> &bull; technical advances in plant science.\u003c/div>\n\u003cbr>\n\u003cp>furthermore, the journal welcomes submissions that support and advance the un's sustainable development goals (sdgs), notably sdg 13: climate action and sdg 15: life on land.\u003c/p> \n\n\u003cp>frontiers in plant science is committed to advancing developments in the field of plant biology by allowing unrestricted access to articles and communicating scientific knowledge to researchers and the public alike, to enable the scientific breakthroughs of the future.\u003c/p> \n \n\u003cp>requirements\u003cp>\n\u003cp>manuscripts that focus on non-plant-related microbiology, human or animal genetics, and medical and pharmacological research are not suitable for publication in this journal. pure field agriculture studies such as those focusing on fertilizer application or yield optimization, without relevance to plant science, are also not within the scope of this journal.\u003c/p>\nstudies falling into the categories below will not be considered for review in this journal unless they are expanded and provide insight into the biological process being studied:\u003c/p>\n\n\u003cp>i) descriptive collections of transcripts, proteins, or metabolites, including comparative sets as a result of different conditions or treatments;\u003c/p>\n\u003cp>ii) descriptive studies that define gene families using pure phylogenetics and the assignment of cursory functional attributions (e.g. expression profiles, promoter analysis, and bioinformatic parameters).\u003c/p>\n\n\u003cp>quantitative analysis needs to be performed on a minimum of three biological replicates in order to enable an assessment of significance. this includes quantitative omics studies (transcriptomics, proteomics, metabolomics) as well as phenotypic measurements, quantitative assays, and qpcr expression analysis. studies that do not comply with these replication requirements will not be considered for review.\u003cp>\n\n\u003cp>studies using transgenic or mutant lines (plants and algae), for example, t-dna, transposon, rnai, crispr/cas9, chemically induced, overexpressors and reporter fusions (gus, gfps, luc), should be based on data from multiple alleles (minimum of two) displaying a common and stable phenotype. qualitative data can be presented from a single allele but should be indicative of observations from multiple alleles which should be explicitly stated in the text. quantitative data should be derived from multiple alleles (at least two) and should be displayed separately for each allele (with at least three independent replications for each allele). studies reporting single alleles may be considered acceptable when:\u003c/p>\n\n\u003cp>i) complementation via transformation is used for confirmation;\u003c/p> \n\u003cp>ii) the allele has been previously characterized and published, and is representative of multiple independent lines;\u003c/p>\n\u003cp>iii) in situations where genetic transformation is difficult or not yet possible, alternative evidence should be presented\u003c/p>\n\n","green","5.6","7.1","927148","@frontplantsci",{"id":115,"name":116,"slug":117,"abbreviation":435,"isonline":13,"isopenforsubmissions":13,"citescore":436,"impactfactor":437},"fpls",8.8,4.8,{"displaytitlepilllabels":13,"displayrelatedarticlesbox":13,"showeditors":13,"showreviewers":13,"showloopimpactlink":13,"enablefigshare":367,"usexmlimages":13},{"ispublic":13,"allowcompanyusers":367,"whitelistemails":440,"enablealljournals":13,"whitelistjournals":462},[441,442,443,444,445,446,447,448,449,446,450,451,452,453,454,455,456,457,458,459,460,461],"y2fybg9zlmxvcmnhqgzyb250awvyc2lulm9yzw==","zgfuawvslmx1ekbmcm9udgllcnnpbi5vcmc=","bgf1cmeudgvuyubmcm9udgllcnnpbi5vcmc=","cmfmywvslnjpyw5jag9aznjvbnrpzxjzaw4ub3jn","amftzxmuc21hbgxib25lqgzyb250awvyc2lulm9yzw==","znjhbi5tb3jlbm9aznjvbnrpzxjzaw4ub3jn","c2ftdwvslmzlcm5hbmrlekbmcm9udgllcnnpbi5vcmc=","ymfyymfyys5jyxjjyw5naxvaznjvbnrpzxjzaw4ub3jn","axzhbi5zyw50yw1hcmlhqgzyb250awvyc2lulm9yzw==","ahvllnryyw5aznjvbnrpzxjzaw4ub3jn","z29uy2fsby52yxjnyxnaznjvbnrpzxjzaw4ub3jn","bhvjawuuc2vubkbmcm9udgllcnnpbi5vcmc=","bg9ybi5mcmfzzxjaznjvbnrpzxjzaw4ub3jn","zwxzys5jyxjyb25aznjvbnrpzxjzaw4ub3jn","bwfhcnrlbi52yw5kawpja0bmcm9udgllcnnpbi5vcmc=","ymluzgh1lmtyaxnobmfuqgzyb250awvyc2lulm9yzw==","bwf0dghldy5hdhr3yxrlcnnaznjvbnrpzxjzaw4ub3jn","z2l1bglhlnzhbhnly2noaubmcm9udgllcnnpbi5vcmc=","zmfiawfulmrlbgxhbw9ydgvaznjvbnrpzxjzaw4ub3jn","dmlqyxlhbi5wcebwaxrzb2x1dglvbnmuy29t","z3vpbgxhdw1llnnldxjhdebmcm9udgllcnnpbi5vcmc=",[463,464,465,466,467,468,392,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498],2232,1729,2357,2456,2176,2333,1843,602,106,616,310,657,3123,1468,2137,628,1566,2726,2086,1490,451,141,1335,2655,1238,604,698,1547,176,1440,403,1239,755,2136,609,1534,{"spaceid":392,"name":392,"availablejournalpages":500,"announcement":504},[275,280,277,501,502,503],"volumes","about","community-reviewers",{"__typename":505,"sys":506,"preheader":42,"title":508,"description":509,"image":510,"link":532},"announcement",{"id":507},"5ittnttqgazppgh6rx0alm","articles published with frontiers have received 12 million total citations","your research is the real superpower - learn how we maximise its impact through our leading community journals",[511],{"archive":389,"brandid":390,"copyright":28,"datecreated":512,"datemodified":513,"datepublished":514,"description":28,"extension":515,"filesize":517,"height":518,"id":519,"ispublic":389,"limited":389,"name":520,"orientation":407,"original":28,"thumbnails":521,"type":385,"watermarked":389,"width":528,"videopreviewurls":529,"tags":530,"textmetaproperties":531,"src":522},"2025-06-18t12:58:01z","2025-06-18t14:15:34z","2025-06-18t12:57:32z",[516],"png",1365026,920,"7c5269c4-3c83-4591-951a74d15b95daea","impact metrics banner for article pages",{"webimage":522,"thul":523,"mini":524,"websitewebp_l":525,"websitewebp_m":526,"guidelines":527},"https://brand.frontiersin.org/m/59ee6275cb9a849c/webimage-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/m/59ee6275cb9a849c/thul-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/m/59ee6275cb9a849c/mini-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/websitewebp_l/impact-metrics-banner-for-article-pages.webp","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/websitewebp_m/impact-metrics-banner-for-article-pages.webp","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/guidelines/impact-metrics-banner-for-article-pages.png",1600,[],[],[],{"text":533,"url":43,"target":27,"arialabel":28},"explore our impact metrics",{"loggeduserinfo":-1},{"currentarticle":536,"ispreviewpage":367,"hassupplementaldata":367,"showcrossmarkwidget":13,"articletemplate":648,"currentarticlepagemetainfo":649,"xmlparsedarticlecontent":-1,"xmlparsingerror":-1},{"id":537,"doi":538,"title":539,"acceptancedate":540,"receptiondate":541,"publicationdate":542,"lastmodifieddate":543,"ispublished":13,"abstract":544,"researchtopic":545,"articletype":551,"stage":554,"keywords":556,"authors":563,"editors":587,"reviewers":595,"journal":610,"section":617,"impactmetrics":619,"volume":622,"articlevolume":623,"relatedarticles":624,"ispublishedv2":13,"contents":625,"files":628},1641677,"10.3389/fpls.2025.1641677","a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution","2025-09-09t14:48:37.000z","2025-06-05t09:39:42.000z","2025-10-03t00:00:00.000z","2025-10-21t02:25:15.907z","walnut leaf brown spot disease, caused by ophiognomonia leptostyla, is among the most destructive fungal diseases in walnut cultivation. in the development of smart agriculture, precision grading of plant diseases remains a core technical challenge; specifically, this disease is plagued by blurred lesion edges and inefficient extraction of complex features, which directly limits the accurate grading of the disease. to address these issues, this study proposes a disease grading method integrating hierarchical feature selection and adaptive multi-scale dilated convolution, and develops the cogfuse-mobilevit model. this model overcomes the limitations of the standard mobilevitv3 model in capturing blurred edges of tiny lesions via three innovative modules: specifically, the hierarchical feature screening module (hfsm) enables hierarchical screening of disease-related features; the edge feature focus module (ecfm) works in synergy with the hfsm to enhance the focus on lesion edge features; and the adaptive multi-scale dilated convolution fusion module (amsdidcm) achieves dynamic multi-scale fusion of lesion textures and global structures. experimental results demonstrate that the proposed model achieves an accuracy of 86.61% on the test set, representing an improvement of 7.8 percentage points compared with the original mobilevitv3 model and significantly outperforming other mainstream disease grading models. this study confirms that the cogfuse-mobilevit model can effectively resolve the issues of blurred edges and inefficient feature extraction in this disease, provides a reliable technical solution for its precision grading, and holds practical application value for the intelligent diagnosis of plant diseases in smart agriculture.",{"id":546,"title":547,"articlescount":548,"ismagazinepage":367,"slug":549,"isopenforsubmission":13,"views":550},66487,"innovative field diagnostics for real-time plant pathogen detection and management",9,"innovative-field-diagnostics-for-real-time-plant-pathogen-detection-and-management",14940,{"id":552,"name":553},24,"original research",{"id":555,"name":19},18,[557,558,559,560,561,562],"walnut","brown spot disease (ophiognomonia leptostyla)","hierarchical feature selection","edge features perception","adaptive multi-scale dilated convolution","disease grading",[564,575,581],{"id":565,"firstname":566,"middlename":19,"lastname":567,"givennames":568,"iscorresponding":367,"isprofilepublic":13,"userid":565,"email":-1,"affiliations":569},3089871,"yuting","wei","yuting ",[570,573],{"organizationname":571,"countryname":572,"cityname":19,"statename":19,"zipcode":19},"college of information engineering, tarim university","china",{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},"key laboratory of tarim oasis agriculture, ministry of education, tarim university",{"id":389,"firstname":576,"middlename":19,"lastname":577,"givennames":578,"iscorresponding":367,"isprofilepublic":367,"userid":389,"email":-1,"affiliations":579},"debin","zeng","debin ",[580],{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},{"id":389,"firstname":582,"middlename":19,"lastname":583,"givennames":584,"iscorresponding":13,"isprofilepublic":367,"userid":389,"email":19,"affiliations":585},"liangfang","zheng","liangfang ",[586],{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},[588],{"id":589,"firstname":590,"middlename":19,"lastname":591,"givennames":592,"iscorresponding":367,"isprofilepublic":13,"userid":589,"email":-1,"affiliations":593},1037951,"ravinder","kumar","ravinder ",[594],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},[596,603],{"id":597,"firstname":598,"middlename":19,"lastname":599,"givennames":600,"iscorresponding":367,"isprofilepublic":13,"userid":597,"email":-1,"affiliations":601},2082015,"geza","bujdoso","geza ",[602],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},{"id":604,"firstname":605,"middlename":19,"lastname":606,"givennames":607,"iscorresponding":367,"isprofilepublic":13,"userid":604,"email":-1,"affiliations":608},3078480,"vasudha","vedula","vasudha ",[609],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},{"id":115,"slug":117,"name":116,"shortname":611,"electronicissn":612,"field":613,"specialtyid":28,"journalsectionpaths":615},"front. plant sci.","1664-462x",{"id":614,"domainid":392},66,[616],{"section":617},{"id":208,"name":209,"slug":210,"specialtyid":618},2685,{"views":620,"downloads":621,"citations":389},1063,103,16,"volume 16 - 2025",[],{"titlehtml":539,"fulltexthtml":626,"menuhtml":627},"\u003cdiv class=\"journalabstract\">\u003ca id=\"h1\" name=\"h1\">\u003c/a>\u003cdiv class=\"authors\">\u003cspan class=\"author-wrapper notranslate\">\u003ca href=\"https://loop.frontiersin.org/people/3089871/overview\" class=\"user-id-3089871/overview\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/images/profile/3089871/overview/74\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\" alt=\"yuting wei,&#x;\">yuting wei\u003c/a>\u003csup>1,2&#x2020;\u003c/sup>\u003c/span>\u003cspan class=\"author-wrapper notranslate\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/cdn/images/profile/default_32.jpg\" alt=\"debin zeng&#x;\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\">debin zeng\u003csup>2&#x2020;\u003c/sup>\u003c/span>\u003cspan class=\"author-wrapper notranslate\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/cdn/images/profile/default_32.jpg\" alt=\"liangfang zheng*\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\">liangfang zheng\u003csup>2*\u003c/sup>\u003c/span>\u003c/div>\u003cul class=\"notes\">\u003cli>\u003cspan>\u003csup>1\u003c/sup>\u003c/span>college of information engineering, tarim university, alaer, china\u003c/li>\u003cli>\u003cspan>\u003csup>2\u003c/sup>\u003c/span>key laboratory of tarim oasis agriculture, ministry of education, tarim university, alaer, china\u003c/li>\u003c/ul>\u003cp>walnut leaf brown spot disease, caused by \u003ci>ophiognomonia leptostyla\u003c/i>, is among the most destructive fungal diseases in walnut cultivation. in the development of smart agriculture, precision grading of plant diseases remains a core technical challenge; specifically, this disease is plagued by blurred lesion edges and inefficient extraction of complex features, which directly limits the accurate grading of the disease. to address these issues, this study proposes a disease grading method integrating hierarchical feature selection and adaptive multi-scale dilated convolution, and develops the cogfuse-mobilevit model. this model overcomes the limitations of the standard mobilevitv3 model in capturing blurred edges of tiny lesions via three innovative modules: specifically, the hierarchical feature screening module (hfsm) enables hierarchical screening of disease-related features; the edge feature focus module (ecfm) works in synergy with the hfsm to enhance the focus on lesion edge features; and the adaptive multi-scale dilated convolution fusion module (amsdidcm) achieves dynamic multi-scale fusion of lesion textures and global structures. experimental results demonstrate that the proposed model achieves an accuracy of 86.61% on the test set, representing an improvement of 7.8 percentage points compared with the original mobilevitv3 model and significantly outperforming other mainstream disease grading models. this study confirms that the cogfuse-mobilevit model can effectively resolve the issues of blurred edges and inefficient feature extraction in this disease, provides a reliable technical solution for its precision grading, and holds practical application value for the intelligent diagnosis of plant diseases in smart agriculture.\u003c/p>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"journalfulltext\">\u003ca id=\"h2\" name=\"h2\">\u003c/a>\u003ch2>1 introduction\u003c/h2>\u003cp class=\"mb15\">walnut diseases pose a serious threat to walnut production in xinjiang. the spread of crop diseases significantly exacerbates the security risks of the walnut industry if timely prevention and control measures are not taken (\u003ca href=\"#b7\">cooke, 2006\u003c/a>; \u003ca href=\"#b12\">khan et&#xa0;al., 2021\u003c/a>). as a core component of the disease prevention and control system, early precise grading plays a critical role in agricultural production management (\u003ca href=\"#b2\">adaskaveg et&#xa0;al., 2009\u003c/a>; \u003ca href=\"#b6\">chiang et&#xa0;al., 2016\u003c/a>; \u003ca href=\"#b14\">lamichhane, 2014\u003c/a>). walnut brown spot, caused primarily by the fungus \u003ci>ophiognomonia leptostyla\u003c/i>, typically forms specific symptoms on organs such as leaves, flowers, and fruits. among them, leaves, as the primary carrier of plant diseases, exhibit characteristics such as lesion morphology and color changes on their surfaces, which often serve as important bases for disease grading (\u003ca href=\"#b8\">ghaiwat and arora, 2014\u003c/a>; \u003ca href=\"#b35\">weber, 1980\u003c/a>; \u003ca href=\"#b39\">zarei et&#xa0;al., 2019\u003c/a>). traditional disease identification relies on manual experience (\u003ca href=\"#b21\">moragrega et&#xa0;al., 2011\u003c/a>; \u003ca href=\"#b34\">wang et&#xa0;al., 2020\u003c/a>), a method that is not only time- color system to reduce interference from light and leaf veins, and lesion edge detection using the sobel operator, ultimately achieving fast and accurate grading based on the ratio of lesion area to leaf area. \u003ca href=\"#b10\">jadhav and patil (2016)\u003c/a> developed a leaf image partitioning technique based on k-means clustering and squared euclidean distance, automatically quantifying damaged leaf area through the pixel ratio of lesions to leaves for disease grading. \u003ca href=\"#b3\">arivazhagan et al. (2013)\u003c/a> proposed a four-step processing system involving color conversion, green pixel removal, segmentation, and texture feature classification for leaf disease grading. however, traditional methods for acquiring disease information suffer from insufficient segmentation accuracy for plant leaf disease images with complex textures and unclear lesion boundaries, thereby resulting in poor disease severity grading performance.\u003c/p>\u003cp class=\"mb15\">with the continuous advancement of computational power, deep learning has increasingly become a key technology for addressing complex lesion segmentation, primarily due to its superior capability in automatic feature extraction (\u003ca href=\"#b18\">mao et&#xa0;al., 2023\u003c/a>). \u003ca href=\"#b5\">chen et al. (2021)\u003c/a> proposed the blsnet method based on the unet semantic segmentation network, enhancing lesion segmentation accuracy through the introduction of attention mechanisms and multi-scale feature fusion. experiments showed that its segmentation and classification accuracy outperformed benchmark models such as deeplabv3+ and unet, preliminarily verifying the reliability of this method in automatic assessment of bls disease severity. \u003ca href=\"#b22\">ngugi et al. (2020)\u003c/a> developed the kijaninet segmentation network based on fully convolutional neural networks, demonstrating excellent performance in tomato leaf segmentation under complex backgrounds; \u003ca href=\"#b16\">lin et al. (2019)\u003c/a> developed a cnn semantic segmentation model that achieved high-precision pixel-level segmentation of cucumber powdery mildew on leaves. additionally, some studies have fused traditional features with deep learning: \u003ca href=\"#b31\">tripathi (2021)\u003c/a> proposed a convolutional neural network method based on alexnet, achieving disease grading by fusing features extracted by the model with external leaf segmentation features; \u003ca href=\"#b17\">ma et al. (2017)\u003c/a> introduced comprehensive color features (ccf) combining hyper-red index, hsv/h and lab/b components, and achieved lesion segmentation via interactive region growing, with experiments confirming that this method enables accurate grading of disease images such as cucumber downy mildew under actual field conditions. however, although such methods have improved grading accuracy to some extent, they still suffer from insufficient feature capture of blurred disease edges, making it difficult to achieve fine-grained differentiation of subtle differences between disease grades (\u003ca href=\"#b26\">rastogi et&#xa0;al., 2015\u003c/a>).\u003c/p>\u003cp class=\"mb15\">in recent years, the application of deep learning in plant disease classification has continued to expand. \u003ca href=\"#b24\">parashar et al. (2024)\u003c/a> systematically validated the capability of complex feature modeling for crop yield prediction, further substantiating the critical role of adaptive feature extraction in agricultural intelligent decision-making. concurrently, \u003ca href=\"#b32\">vishnoi et al. (2022)\u003c/a> enhanced small-target detection precision in apple disease identification through a spatial attention mechanism-based cnn architecture for leaf disease diagnosis. \u003ca href=\"#b23\">ozturk et al. (2025)\u003c/a> constructed an ensemble learning classification model based on resnet50, mobile net, efficientnetb0, and densenet121, enhancing generalization performance through statistical cross-validation and improving decision interpretability via grad-cam visualization. experimental results showed that the ensemble model achieved stable high-precision classification performance across multiple validation rounds. these studies provide robust and interpretable solutions for intelligent plant disease recognition through technical integration and architectural innovation. \u003ca href=\"#b9\">hu et al. (2021)\u003c/a> integrated retinex enhancement, faster r-cnn detection, and vgg16 classification to improve the grading and detection accuracy of blurred diseased leaves. \u003ca href=\"#b25\">picon et al. (2019)\u003c/a> proposed an adaptive deep residual neural network algorithm for the classification of three european wheat diseases (septoria leaf blotch, brown spot, and rust) in real-world scenarios, which effectively improved the classification accuracy of wheat diseases. \u003ca href=\"#b13\">kim and ahn (2021)\u003c/a> employed deep cnn architectures such as resnet, xception, and densenet, combined with transfer learning and fine-tuning, to classify 9 categories of pests, diseases, and healthy states in tomatoes. although densenet combined with the rmsprop algorithm achieved an accuracy of 98.63%, single cnn architectures have clear bottlenecks in handling complex plant lesions and overlapping multiple diseases, including insufficient specificity in feature extraction and limited ability to distinguish subtle differences between lesions. \u003ca href=\"#b28\">shi et al. (2023)\u003c/a> analyzed the application of cnns in evaluating the severity of plant diseases, pointing out that hybrid architectures such as classical cnns, improved cnns, and segmentation networks have limitations in handling complex plant lesions: classification errors caused by concurrent multiple diseases, as well as constraints on model generalization ability due to unbalanced datasets and insufficient annotation accuracy. these issues significantly restrict their precise application in practical agricultural scenarios.\u003c/p>\u003cp class=\"mb15\">although significant advancements have been made in plant lesion segmentation and disease classification using deep learning, the grading task for walnut leaf brown spot disease&#x2014;characterized by blurred edge representation and complex small lesions&#x2014;still faces notable challenges. current mobilevit-based hybrid models fall into two categories: general architectures (mobilevitv1/v2/v3) focus on optimizing the cnn-transformer balance for natural images; domain-improved models (mobile-former/edgevit) are oriented toward medical/industrial tasks, with their task focus on localization rather than grading, which mismatches the pain points and fails to address the blurriness of agricultural lesions, tissue interference, and morphological variations. thus, the present study proposes a novel cogfuse-mobilevit model specifically designed for disease grading, with its specific contributions as follows:\u003c/p>\u003cp style=\"margin-top:1em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">1. a diverse field-collected walnut leaf dataset was constructed, with images divided into four distinct severity levels based on qualitative assessment of disease severity, ensuring the accuracy of disease severity grading.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">2. the hierarchical feature selection module (hfsm) enhances the fusion of local details (such as lesion texture and color) with global context through local and global attention mechanisms and task-driven feature selection, while suppressing interference from healthy regions.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">3. the edge convolution fusion module (ecfm) strengthens edge details through sobel convolution and residual connections, achieving effective integration of edge-specific features with general features, further enhancing edge details and enabling more precise capture of lesion contour details.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:1em;margin-left:1em;text-indent:-1em;text-align:left\">4. the adaptive multi-scale dilated dense inception convolution module (amsddicm) extracts differentiated features using multi-shaped convolution kernels and adaptively fuses multi-scale information via a dynamic weight mechanism, capturing lesion morphologies at different pathogenesis stages.\u003c/p>\u003ca id=\"h3\" name=\"h3\">\u003c/a>\u003ch2>2 materials and methods\u003c/h2>\u003ch3>2.1 characteristics of walnut leaf brown spot and classification of disease severity levels\u003c/h3>\u003cp class=\"mb0\">in the local standard technical regulations for prevention and control of walnut brown spot (\u003ca href=\"#b19\">mcgranahan and leslie, 1991\u003c/a>), walnut brown spot is divided into four severity levels. the severity of plant leaf diseases is generally determined using the spot coverage method. in this study, walnut leaves with different disease severities were processed to separate disease-infected spots from healthy leaf areas, and the percentage of lesion area to total leaf area was calculated. with reference to the local standard, the disease grades specified in the standard were re-determined through calculations in this study, as shown in \u003ca href=\"#t1\">table&#xa0;1\u003c/a>. classification of different severity levels of walnut leaf brown spot was conducted in accordance with technical regulations for prevention and control of walnut brown spot (\u003ca href=\"#b33\">wang et&#xa0;al., 2022\u003c/a>; \u003ca href=\"#b37\">yang et&#xa0;al., 2021\u003c/a>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;1\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" name=\"table&#xa0;1\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" alt=\"www.frontiersin.org\" id=\"t1\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;1\u003c/b>. walnut leaf brown spot disease severity grading criteria.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">walnut leaf brown spot is caused by infection with the fungus \u003ci>ophiognomonia leptostyla\u003c/i> (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>). in the early infection stage, near-circular or irregular small spots appear on the leaves, with a gray-brown center and dark yellow-green to purplish-brown edges, and diseased leaves tend to fall off prematurely. in the middle stage, elongated elliptical or irregular slightly sunken dark brown lesions form, with larger spot size and light brown edges; longitudinal cracks are often present in the center of the lesions. in the late stage, lesions often coalesce to form large scorched necrotic areas, surrounded by yellow to golden-yellow zones, and small black granules (conidiomata and conidia of the pathogen) are scattered on the surface of the diseased tissue (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>). according to the degree of color and texture feature changes in infected leaves, walnut leaf disease is divided into four stages: healthy, early, middle, and late stages, corresponding to disease severity levels: healthy (level 0), mild (level 1), moderate (level 2), and severe (level 3). the different severity levels of walnut leaf brown spot are shown in \u003ca href=\"#f1\">figure&#xa0;1\u003c/a>.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;1\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" alt=\"four leaves are displayed, labeled a to d. a shows a healthy leaf with no spots. b features a mildly infected leaf with a few small spots. c presents a moderately infected leaf with numerous spots. d shows a severely infected leaf covered with dark spots.\" id=\"f1\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;1\u003c/b>. walnut leaves infected with brown spot disease at different severity levels. \u003cb>(a)\u003c/b> healthy leaves; \u003cb>(b)\u003c/b> mildly infected leaves; \u003cb>(c)\u003c/b> moderately infected leaves; \u003cb>(d)\u003c/b> severely infected leaves.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">\u003ca href=\"#t1\">table&#xa0;1\u003c/a> shows the ratio of walnut leaf brown spot lesion area to total leaf area. level 0 (healthy): healthy leaves without symptoms. level 1 (mild): near-circular or irregular small white spots appear on leaves, accounting for less than 5% of the leaf area. level 2 (moderate): lesions expand into irregular dark brown spots, covering 5% to 30% of the leaf area. level 3 (severe): abundant lesion coalesce to form large scorched necrotic areas, exceeding 30% of the leaf area.\u003c/p>\u003ch3>2.2 calculation algorithm for walnut leaf brown spot disease severity levels\u003c/h3>\u003cp class=\"mb0\">after capturing walnut leaf brown spot samples using a camera, to minimize subjective bias, this study strictly adhered to the objective quantitative criteria defined in the technical regulations for the control of walnut brown leaf spot (\u003ca href=\"#t1\">table&#xa0;1\u003c/a>), utilizing the lesion area percentage (k) as the primary grading indicator. we re-determined the severity levels through computational analysis, with the specific determination process shown in \u003ca href=\"#f2\">figure&#xa0;2\u003c/a>.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;2\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" alt=\"flowchart illustrating a process for identifying disease areas on leaves. images of leaves are converted through morphological color space to grayscale. the grayscale is used to highlight disease area s1 and leaf area s2. the ratio s1/s2 is applied, producing a final analysis image showing leaf disease marked in red.\" id=\"f2\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;2\u003c/b>. computational method for different severity levels of brown spot infection in walnut leaves.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">first, a python-based color segmentation algorithm was employed to convert images from the bgr color space to the hsv color space for processing. the hsv color space offers inherent advantages in color segmentation tasks. by analyzing the hue, saturation, and value characteristics of diseased regions, the color threshold range in the hsv space was determined (\u003ca href=\"#b4\">chaudhary et&#xa0;al., 2012\u003c/a>). after generating an initial disease region mask based on this threshold range, morphological operations such as closing and opening were applied to optimize mask quality, effectively eliminating noise while preserving the integrity of lesion contours. subsequently, the original image was converted to grayscale mode, and the leaf region was extracted using an adaptive threshold binarization algorithm. leaf boundaries were localized via contour detection technology, and a complete leaf mask was generated through contour filling. pixel statistical analysis was performed on both the leaf mask and disease mask to calculate the total leaf area and diseased region area, respectively. when a valid leaf area (&gt;0) was detected, the disease severity index was calculated using the following formula: disease severity index \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im1\">\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmo mathsize=\"10.5pt\">%\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">s\u003c/mi>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003c/mrow>\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">s\u003c/mi>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmn mathsize=\"10.5pt\">100\u003c/mn>\u003cmo mathsize=\"10.5pt\">%\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;&#xa0;&#xa0;s\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>: total area of diseased regions \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im2\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>: total area of the complete leaf.\u003c/p>\u003ch3>2.3 dataset construction\u003c/h3>\u003cp class=\"mb0\">data were collected from the walnut plantation base of tarim university (alar, xinjiang) between may and october 2024. leaves from three locally dominant early-fruiting cultivars (&#x2018;wen 185&#x2019;, &#x2018;xinxin 2&#x2019;, &#x2018;zha 343&#x2019;)widely cultivated in southern xinjiang were vertically photographed using an iphone 13. this genotypic diversity ensures model generalizability by encompassing varied disease phenotypes. the orchard follows standardized cultivation with 4m plant spacing, 5m row spacing (&#x2248;50 plants/mu), and conventional management practices. sampled trees were in full fruiting stage. to capture disease traits across microenvironments, samples were collected from safely accessible crown layers. the dataset includes healthy and brown spot-infected leaves three severity levels, spanning varied time periods, light conditions, and angles. after removing duplicates and invalid images,5,120 high-quality jpgs were retained for analysis.\u003c/p>\u003ch4>2.3.1 development of the walnut leaf brown spot dataset\u003c/h4>\u003cp class=\"mb0\">disease severity grading was established through quantitative lesion area analysis (\u003ca href=\"#f2\">figure&#xa0;2\u003c/a>). a representative subset of 512 images (10% of the full 5,120-image dataset) was selected via stratified random sampling, accurately preserving the original severity distribution. to ensure labeling rigor, two plant protection specialists (&gt;5 years&#x2019; expertise in walnut pathology, certified in local protocols) executed standardized annotation: pre-annotation training unified diagnostic criteria for ambiguous cases, with formal annotation commencing only after achieving pre-calibration inter-rater agreement (kappa coefficient &#x2265;0.75). as validated in \u003ca href=\"#t2\">table&#xa0;2\u003c/a>, dual statistical metrics confirmed gold-standard reliability&#x2014;inter-rater cohen&#x2019;s kappa reached 0.71, meeting landis &amp; koch&#x2019;s &#x201c;substantial agreement&#x201d; threshold; algorithm-expert consensus attained a weighted fleiss&#x2019; kappa of 0.77, substantially outperforming conventional cohen&#x2019;s kappa in multi-annotator scenarios. a three-stage standardization pipeline eliminated preprocessing discrepancies. the final dataset, partitioned into training/testing sets (8:2 ratio), strictly adheres to plant disease survey protocols and provides benchmarked data for deep learning model development (\u003ca href=\"#t3\">table&#xa0;3\u003c/a>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;2\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" name=\"table&#xa0;2\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" alt=\"www.frontiersin.org\" id=\"t2\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;2\u003c/b>. algorithm vs. expert consensus agreement evaluation.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;3\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" name=\"table&#xa0;3\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" alt=\"www.frontiersin.org\" id=\"t3\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;3\u003c/b>. walnut leaf brown spot disease image acquisition data.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>2.4 construction of walnut leaf brown spot disease severity grading model\u003c/h3>\u003ch4>2.4.1 the optimized mobilevitv3 network: cogfuse-mobilevit\u003c/h4>\u003cp class=\"mb15\">to address the challenge of difficult feature capture for small lesions in walnut leaf brown spot severity grading, this study proposes an innovative model, cogfuse-mobilevit. mobilevitv3 was selected as the backbone network due to its suitability for walnut brown spot grading. its hybrid mobilenet-vit architecture integrates cnn-based local feature extraction essential for lesion detail capture with transformer-enabled global contextual modeling critical for analyzing lesion spatial distribution. this design aligns with pathological requirements for severity grading, necessitating concurrent attention to local lesion characteristics and global infection patterns. the lightweight architecture further supports real-time processing on embedded devices, enabling future field deployment.\u003c/p>\u003cp class=\"mb0\">the model embodies a &#x201c;grading task-driven&#x201d; design principle. conceptually, it establishes a disease-specific framework of &#x201c;hierarchical screening to edge enhancement to dynamic fusion.&#x201d; this framework elevates general feature extraction to targeted solutions addressing three major agricultural challenges: suppressing interference from healthy tissues via hierarchical screening resolving feature confusion; strengthening blurred lesion contours through edge enhancement overcoming the bottleneck of edge blurriness; adaptively handling multi-stage morphological variations using dynamic fusion addressing morphological variation challenges. structurally, as illustrated in \u003ca href=\"#f3\">figure&#xa0;3\u003c/a>, the network implements a progressive feature extraction strategy. the hierarchical feature selection module (hfsm) first fuses shallow and middle-layer features. it employs a hierarchical attention mechanism to enhance semantic consistency while preserving spatial details. the edge convolution fusion module (ecfm) then processes these features. it utilizes learnable sobel operators to extract edge information, which is fused with conventional convolutional features via residual connections to augment edge perception capability. finally, the adaptive multi-scale dilated inception convolution module (amsddicm) enables adaptive processing of multi-scale edge features. this module is capable of both capturing fine edge changes in minute lesions and grasping the overall contour structure of large lesions, thereby comprehensively covering edge features across different developmental stages. based on these enhanced edge features, the network accurately outputs the final disease severity grade.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;3\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" alt=\"diagram illustrating a deep learning process for leaf classification. input images of leaves undergo convolution and mobilevit blocks, reducing dimensions and processing through hfsm, ecfm, and amsddicm modules. outputs are pooled and linearly transformed to logits.\" id=\"f3\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;3\u003c/b>. cogfuse-mobilevit architecture diagram (hfsm suppresses healthy regions through dynamic masks, ecfm explicitly extracts edges via sobel convolution, and amsddicm fuses multi-scale features through dynamic weights).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>2.4.2 the hierarchical feature selection module\u003c/h4>\u003cp class=\"mb0\">the hfsm (hierarchical feature selection module) is an innovative architecture proposed in this study. unlike mobilevit&#x2019;s direct feature concatenation, hfsm utilizes learnable prompt vectors (\u003ca href=\"#eq1\">equation 1\u003c/a>) to generate spatial masks, dynamically suppressing non-lesion regions. such task-driven selection is crucial for tiny objects. in the grading task of walnut leaf brown spot disease, this module utilizes a local attention mechanism to focus on micro-regions of walnut leaves, fusing shallow and middle-layer features. meanwhile, the hierarchical feature selection mechanism enables precise capture of local detail features such as lesion texture and color, effectively suppressing interference from healthy leaf regions and enhancing disease features. this provides high-discriminative feature representations for brown spot disease grading while preparing for subsequent lesion edge processing. as depicted in \u003ca href=\"#f4\">figure&#xa0;4\u003c/a>, the module processes two hierarchical feature maps. initial 1&#xd7;1 convolutions reduce both maps&#x2019; channels to half the output dimension curtailing computational load. one reduced map undergoes bilinear upsampling for spatial alignment with the other. these aligned features are summed and processed by a 3&#xd7;3 convolution to generate base-path features. simultaneously, the original reduced map and upsampled map are fed into dual local-global attention modules for parallel processing (\u003ca href=\"#b36\">xu, 2024\u003c/a>). each group contains local and global branches. during branch processing, the feature map is partitioned into p&#xd7;p non-overlapping patches \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im3\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">p\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmi mathsize=\"10.5pt\">j\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math> via the unfold operation. after calculating the mean of pixel features within each patch, the result is processed using the following core formula: attention distribution generation.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;4\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" alt=\"diagram illustrating a neural network architecture with multiple components. the upper section shows two convolution layers, lga modules, concatenation, and rep blocks. the lower section features processes like unfold, mean calculation, softmax, and feature selection, highlighting token and channel selection. various operations and components like multiplication, addition, and dropout are labeled, with symbols explaining their functions.\" id=\"f4\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;4\u003c/b>. architecture diagram of the hierarchical feature selection module (hfsm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">z\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ml\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">layernor\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ml\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">mean\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>1\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">a\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">softmax\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">z\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>2\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq1\">equations 1\u003c/a> and \u003ca href=\"#eq2\">2\u003c/a> convert the mean value of patch features into a high-dimensional feature vector through multi-layer perceptrons (mlp) and layer normalization (layernorm) (wherein(\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im4\">\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">mlp\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;and&#xa0;&#xa0;mlp\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>) \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im5\">\u003cmrow>\u003cmsup>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">achieve&#xa0;dimensional&#xa0;transformation&#xa0;p\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msup>\u003cmo mathsize=\"10.5pt\">&#x2192;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ouc\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmo mathsize=\"10.5pt\">&#x2192;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ouc\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/mrow>\u003c/math>)), and then generates the attention distribution via the softmax function \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im6\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">a\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmi mathsize=\"10.5pt\">j\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>. this mechanism enables the model to focus on the key features of lesion regions, weaken the interference from healthy leaf areas, enhance the specificity of feature selection, and provide high-quality features for subsequent precise grading. after the local and global features output by the two modules are spliced along the channel dimension, they are combined with the basic path features to generate the output features of the final processing module.\u003c/p>\u003cp class=\"mb0\">for leaf images with over 80% healthy tissue, dynamic masks are generated via learnable prompt vectors to suppress green texture features while enhancing the gray-brown features of lesions (\u003ca href=\"#b19\">mcgranahan and leslie, 1991\u003c/a>).\u003c/p>\u003ch4>2.4.3 ecfm edge convolutional fusion module\u003c/h4>\u003cp class=\"mb0\">in the walnut leaf brown spot grading task, lesion edge features are crucial for accurately determining disease severity. the standard mobilevit relies on cnn-transformer blocks to implicitly learn edges, whereas the ecfm (edge convolution fusion module) incorporates sobel convolutions, conventional convolutions, and residual connections, effectively fusing edge features with general features and thereby enhancing edge details. as shown in \u003ca href=\"#f5\">figure&#xa0;5\u003c/a>, the sobel branch employs sobel convolution to extract image edge information, accurately capturing the contour details of brown spot lesions; the convolutional branch captures general features such as leaf color and texture through standard convolution. the two realize feature fusion via the following core formula (\u003ca href=\"#eq3\">equation 3\u003c/a>):\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;5\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" alt=\"diagram showing a network flow. input x is split into two paths: one goes through a sobel filter and the other through a convolution (conv) layer. outputs are concatenated (concat) and followed by another conv layer. the result is summed with a bypass connection and passed through a final conv layer.\" id=\"f5\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;5\u003c/b>. architecture diagram of the edge convolutional fusion module (ecfm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">sobelconv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>3\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">processing the input feature map \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im7\">\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmi mathsize=\"10.5pt\">x\u003c/mi>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> through the sobel convolution operator (sobelconv) specifically extracts edge information of lesions (such as the boundary contours between lesions and healthy tissues, and the edge textures of small lesions). for the commonly seen blurred edges in brown spot disease, sobel convolution can enhance edge gradient changes, making the originally blurred lesion contours clearer, thus providing key edge feature support for subsequent grading. conventional feature extraction and fusion (\u003ca href=\"#eq4\">equation 4\u003c/a>):\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>4\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im8\">\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;c\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> extracting the overall features of leaves (such as the color distribution of lesions and the overall texture of leaves) through standard convolution forms a complement to edge features, preventing the model from focusing only on local edges while ignoring global lesion information.\u003c/p>\u003cp class=\"mb15\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im9\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> concatenating the edge feature s and the conventional feature c along the channel dimension achieves the initial fusion of edge details and global features. this fusion enables the model to both identify the fine contours of lesions and judge the disease condition by combining the overall color and texture changes of lesions, thereby improving the accuracy of grading. the module also introduces feature addition and subsequent convolution operations: the fused features are first processed by the first convolution layer \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im10\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math>, the corresponding operation results are added to the original input, and the final output is generated through the second convolution layer \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im11\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">final\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math>. in this process, the residual connection retains the original feature information, further strengthens the integration of edge features and conventional features, and ensures the effective transmission and enhancement of multi-level features.\u003c/p>\u003cp class=\"mb0\">the edge gradient information extracted by sobel convolution (such as grayscale differences between diseased spots and healthy tissues) and the texture features from conventional convolution (such as the roughness of necrotic areas) are fused via residual connections. this not only preserves the blurred edges of early-stage lesions but also prevents edge features from being disconnected from the overall texture (\u003ca href=\"#b8\">ghaiwat and arora, 2014\u003c/a>).\u003c/p>\u003ch4>2.4.4 amsddicm adaptive multi-scale dilated depthwise inseparable convolution module\u003c/h4>\u003cp class=\"mb0\">this module differs from mobilevit&#x2019;s single-scale convolution, leverages multi-shaped convolution kernels to accurately extract lesion features of circular, irregular, and other shapes from multiple dimensions, enabling the identification of subtle differences in lesion edges and internal colors to enrich feature dimensions. meanwhile, depth wise separable convolutions are employed to accommodate lesion scales at different stages of disease development. specifically, the input features are first split into two groups, each entering the amsddicm module to extract features via multi-scale depth wise separable convolutions. a dynamic weighting mechanism (global pooling + convolution + softmax) is used to generate weights for fusing multi-scale features. the processed features from the two groups are concatenated, and finally, cross-channel fusion is completed via 1&#xd7;1 convolution to output the final optimized features. the entire process integrates multi-scale convolution, dynamic weight allocation, and feature fusion to enhance the model&#x2019;s ability to capture complex features, as shown in \u003ca href=\"#f6\">figure&#xa0;6\u003c/a>. in response to the morphological differences between early-stage small spots (1-3mm in diameter) and late-stage fused spots (over 20mm in diameter), the dynamic weight mechanism enables the model to adaptively allocate the contributions of 3&#xd7;3 kernels for capturing local textures and 11&#xd7;1 kernels for extracting strip-shaped spreading features. this addresses the limitation of fixed weights in traditional inception architectures in adapting to multi-scale morphologies.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;6\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" alt=\"diagram showing a neural network process. the top section involves splitting input channels, applying a dms 2d convolution, concatenation, and another convolution. the bottom section includes a pooling layer, convolution, rearrangement, softmax, summation, batch normalization, and activation. the process results in output \\(x^{''}\\).\" id=\"f6\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;6\u003c/b>. architecture diagram of the adaptive multi-scale dilated depth wise inseparable convolution module (amsddicm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">when the input feature tensor x with the number of channels c enters the amsddicm module, it first undergoes a feature grouping operation: the input features are evenly split into two groups along the channel dimension, with each group containing half of the original number of channels (c//2). this grouping strategy not only reduces computational complexity but also creates conditions for subsequent multi-scale feature extraction. each feature group is fed into an independent dmsconv2d module, which adopts different configurations such as 3&#xd7;3, 5&#xd7;5 square convolution kernels and 1&#xd7;11, 11&#xd7;1 strip-shaped convolution kernels &#x2014; the square kernels capture local textures, while the strip-shaped kernels extract direction-sensitive long-range dependencies. this dynamic weight allocation draws inspiration from adaptive strategies in agricultural iot systems. similar to how salinity-aware ets models prioritize soil conductivity features under salt stress (\u003ca href=\"#b40\">zhang et&#xa0;al., 2023\u003c/a>), our mechanism selectively amplifies lesion morphological features critical for severity grading. the dynamic weighting mechanism of dmsconv2d generates weights through global average pooling and 1&#xd7;1 convolution, with the core formulas as follows:\u003c/p>\u003cp class=\"mb15\">weighted basic feature generation:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">rearrange\u003c/mtext>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003c/msub>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">avgpoo\u003c/mtext>\u003cmn mathsize=\"10.5pt\">l2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">d\u003c/mtext>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>5\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq5\">equation 5\u003c/a> compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information (such as the overall distribution characteristics of lesions at different scales), and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmo mathsize=\"10.5pt\">.\u003c/mo>\u003cmtext mathsize=\"10.5pt\">softmax\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>6\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq6\">equation 6\u003c/a> compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information, such as the overall distribution characteristics of lesions at different scales, and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">0\u003c/mn>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msubsup>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">dwcon\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">v\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>7\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb0\">\u003ca href=\"#eq7\">equation 7\u003c/a> is based on dynamic weights \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im12\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>) for different convolution kernels \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im13\">\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;dwconv\u003c/mtext>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> weighted fusion is performed on the extracted features. for walnut brown spot lesions exhibiting size heterogeneity and morphological complexity&#x2014;ranging from early-stage circular micro-lesions to late-stage coalesced irregular lesions&#x2014;this mechanism adaptively modulates multi-scale feature contributions. it prioritizes retention of morphology-discriminative features, local textures in small lesions or directional distributions in large lesions, thereby comprehensively capturing pathological characteristics across developmental stages. the processed features undergo channel-wise concatenation to generate optimized outputs (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>).\u003c/p>\u003ch3>2.5 experimental process for severity grading of walnut leaf brown spot disease\u003c/h3>\u003cp class=\"mb0\">\u003ca href=\"#f7\">figure&#xa0;7\u003c/a> is the overall flow chart of severity grading of walnut leaf spot disease. first is the image acquisition stage, where raw images of walnut leaves are captured and collected to construct an image database containing pictures to be classified (\u003ca href=\"#b29\">singh et&#xa0;al., 2019\u003c/a>). next is the image preprocessing stage, which involves sequentially calculating and classifying disease severity levels, establishing image labels, and performing image preprocessing operations. subsequently, the processed data are used to train the constructed dataset. finally, in the model training and performance evaluation stage, the preprocessed images are input into the model for classifying walnut leaf brown spot disease, after which performance evaluation is conducted on the model&#x2019;s classification results to determine the model&#x2019;s effectiveness and accuracy.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;7\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" alt=\"flowchart illustrating a process for walnut leaf brown spot disease classification. it consists of three main phases: image acquisition, preprocessing, and model training and evaluation. the image acquisition phase involves collecting walnut leaf images and storing them in a database. the preprocessing phase includes classification of disease grades, establishing image labels, and training the dataset. finally, the model classifies the disease, resulting in output images and a performance evaluation matrix for accuracy verification. the chart uses labeled boxes, arrows, and image samples for illustration.\" id=\"f7\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;7\u003c/b>. overall flow chart for severity grading of walnut leaf brown spot disease.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>2.6 experimental parameters and evaluation metrics\u003c/h3>\u003ch4>2.6.1 test environment and hyperparameter setting\u003c/h4>\u003cp class=\"mb15\">the experimental setup of this study is based on deep learning technology and leverages high-performance computing resources. all experiments were conducted on the windows 10 operating system. the hardware platform consists of an amd ryzen 7 3700x processor and an nvidia rtx 2080ti graphics card. the software environment was built using python 3.8, the pytorch 1.13.0 deep learning framework, and the cuda 11.3 parallel computing platform. additionally, pytorch 1.13.0&#x2014;a widely adopted open-source deep learning library renowned for its high flexibility&#x2014;was selected, making it well-suited for research and development.\u003c/p>\u003cp class=\"mb0\">during model training, multiple adjustments were made to the hyperparameters to compare test results and select the optimal hyperparameter combination. the model accepts input images sized at 224&#xd7;224 pixels, with a configured batch size of 32 during training and a maximum of 100 training epochs. the optimizer employs an initial learning rate of 0.003.\u003c/p>\u003ch4>2.6.2 evaluation metrics\u003c/h4>\u003cp class=\"mb15\">this paper aims to evaluate the performance of the model and verify the effectiveness of the improvement measures. we selected multiple evaluation metrics, and all subsequent experimental results adopt the method of calculating averages via 5-fold cross-validation, aiming to comprehensively and reliably evaluate the model performance. including precision (p), recall (r), etc. (\u003ca href=\"#eq8\">equations 8\u003c/a>&#x2013;\u003ca href=\"#eq16\">16\u003c/a>) these metrics can be calculated using the following formulas.\u003c/p>\u003cp class=\"mb15\">the arithmetic mean of the metric values across the five folds:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmover accent=\"true\">\u003cmtext mathsize=\"10.5pt\">m\u003c/mtext>\u003cmo mathsize=\"10.5pt\">&#xaf;\u003c/mo>\u003c/mover>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmn mathsize=\"10.5pt\">5\u003c/mn>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">5\u003c/mn>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">m\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>8\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">precision\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>9\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">recall\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fn\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>10\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">score\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003c/mrow>\u003cmrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fn\u003c/mtext>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>11\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">macro&#xa0;precision=\u003c/mtext>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>12\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">macro&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">r\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>13\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;precision=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&#xb7;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">precisio\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>14\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&#xb7;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">precisio\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>15\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmsubsup>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003cmtext mathsize=\"10.5pt\">t\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>16\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003ca id=\"h4\" name=\"h4\">\u003c/a>\u003ch2>3 experiments and results analysis\u003c/h2>\u003ch3>3.1 core module design and validity experimental verification\u003c/h3>\u003ch4>3.1.1 comparative test of necessity of hfsm module\u003c/h4>\u003cp class=\"mb0\">to validate the necessity of the hierarchical feature selection module (hfsm) within the cogfuse-mobilevit framework, this study conducted comparative tests against two mainstream lightweight attention modules: se (squeeze-and-excitation) and cbam (convolutional block attention module). as illustrated in \u003ca href=\"#t4\">table&#xa0;4\u003c/a>, the hfsm module achieves a significantly higher accuracy of 86.61%, outperforming the se module and cbam module by 7.38% and 4.46%, respectively. this demonstrates that its hierarchical feature selection mechanism more effectively captures discriminative features. in terms of computational efficiency, the parameters and flops of hfsm remain comparable with those of the se module and cbam module, indicating that its performance breakthrough stems from innovative structural design under equivalent lightweight constraints. comprehensive results confirm that replacing hfsm with se or cbam modules would incur a performance degradation exceeding 4%, thereby validating the indispensable value of hfsm in enabling efficient feature selection within the cogfuse-mobilevit framework.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;4\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" name=\"table&#xa0;4\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" alt=\"www.frontiersin.org\" id=\"t4\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;4\u003c/b>. performance comparison of attention modules within the cogfuse-mobilevit framework.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.2 convolutional kernel selection in amsddicm\u003c/h4>\u003cp class=\"mb0\">rigorous validation via kernel combination ablation studies (\u003ca href=\"#t5\">table&#xa0;5\u003c/a>) demonstrates. the hybrid configuration (3&#xd7;3 \u003cb>+\u003c/b> 5&#xd7;5 \u003cb>+\u003c/b> 1&#xd7;11) significantly outperformed square-only kernels (3&#xd7;3 \u003cb>+\u003c/b> 5&#xd7;5) accuracy 86.61% and 82.15% stage-specific recall gains. mid-stage lesions (level 2) raise 9.4 percentage points. late-stage lesions (level 3) raise 6.1 percentage points. this empirically validates the necessity of 1&#xd7;11 rectangular kernels for capturing linear pathological features, overcoming the limitation of isotropic kernels in detecting anisotropic structures.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;5\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" name=\"table&#xa0;5\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" alt=\"www.frontiersin.org\" id=\"t5\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;5\u003c/b>. comparison of different convolution kernels in amsddicm.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.3 the impact of new modules on computational complexity\u003c/h4>\u003cp class=\"mb0\">\u003ca href=\"#f8\">figure&#xa0;8\u003c/a> shows that the hfsm module incurs a 169% flops increase to achieve a breakthrough improvement in early-stage lesion detection&#x2014;reducing level 0/1 misclassification by 24%, thereby establishing the pathological foundation for severity grading. the ecfm module contributes a mere 3% flops increment yet drives a 3.68 percentage point accuracy gain through enhanced edge feature representation. with only a 0.028g flops overhead 1.3%, the amsddicm module enables adaptive fusion of multiscale pathological deformations, culminating in a 7.80-pp accuracy leap. these modules form a cascaded optimization paradigm: hfsm&#x2019;s substantial cost resolves the core pathological bottleneck, while subsequent modules deliver superlinear returns&#x2014;harvesting 6.85-pp accuracy gain with just 14% additional flops&#x2014;collectively establishing the globally optimal computation-performance equilibrium.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;8\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" alt=\"bar graphs comparing different configurations of mobilevit models across three metrics. panel a shows accuracy percentages, with values from 78.81% to 86.61%. panel b displays flops, ranging from 0.74g to 2.1g. panel c illustrates parameters in millions, with values between 1.94m and 2.02m. each graph compares mobilevit, mobilevit plus hfsm, mobilevit plus hfsm plus ecfm, and cogfuse-mobilevit.\" id=\"f8\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;8\u003c/b>. comparison of the impact of each newly added module on computational complexity \u003cb>(a)\u003c/b> accuracy (%); \u003cb>(b)\u003c/b> flops (g); \u003cb>(c)\u003c/b> params (m). measured on nvidia rtx 2080ti gpu (pytorch 1.13.0, cuda 11.3) with 224&#xd7;224 input.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.4 comparison of the influence of different module fusion on model performance\u003c/h4>\u003cp class=\"mb0\">to validate the effect of module fusion, \u003ca href=\"#t6\">table&#xa0;6\u003c/a> compares model performances with different combinations. when only hfsm (hierarchical feature selection module) is introduced, precision increases from the baseline of 82.23% to 83.77%, but recall decreases by 3 percentage points to 72.31%, causing the f1 score to slightly decline to 78.04%. accuracy rises to 82.15%, indicating improved overall classification correctness of the model, but with potential risk of missed detections.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;6\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" name=\"table&#xa0;6\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" alt=\"www.frontiersin.org\" id=\"t6\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;6\u003c/b>. impact of fusion of different modules on model performance.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">when hfsm and ecfm (edge-context feature module) are synergistically introduced, precision increases to 86.34%, recall recovers to 74.69%, and both f1 score and accuracy are significantly optimized. hfsm fuses shallow and middle-layer features through a hierarchical attention mechanism, laying the foundation for subsequent edge feature processing; ecfm enhances lesion edge features. the combination of the two effectively improves the integrity of feature representation and the clarity of lesion boundaries.\u003c/p>\u003cp class=\"mb15\">the combination of hfsm and amsddicm (adaptive multi-scale dilated depthwise inseparable convolution module) further pushes recall to 76.24%, accuracy to 83.94%, and f1 score to 80.79%, outperforming the hfsm+ecfm combination. amsddicm compensates for hfsm&#x2019;s deficiency in local feature refinement through attention-guided multi-scale detail fusion, which integrates multi-layer detail feature weights, especially suitable for scenarios with small targets or blurred features.\u003c/p>\u003cp class=\"mb0\">when the three modules work synergistically, all indicators reach optimal levels: precision, recall, f1 score, and accuracy increase by 12.19%, 7.67%, 9.62%, and 10.00% respectively compared with the baseline. among them, hfsm lays the foundation for cross-layer feature fusion, ecfm effectively integrates edge-specific features with general features to enhance image edge information, and amsddicm adaptively fuses multi-scale and multi-type features through an attention mechanism, forming a progressive optimization chain from &#x201c;hierarchical feature extraction&#x201d; to &#x201c;edge semantic enhancement&#x201d; and then to &#x201c;multi-layer detail fusion&#x201d;. the experimental results show a balanced improvement in both precision and recall, indicating that the fusion of the three modules enables the model to focus more on learning the features of small brown spot lesions, thereby improving its classification performance. this synergistic task-specific design, combining hierarchical selection, explicit edge enhancement, and adaptive multi-scale fusion, fundamentally distinguishes cogfuse-mobilevit from prior mobilevit hybrids designed for general vision or localization tasks.\u003c/p>\u003ch4>3.1.5 influence of different module combinations on f1-score of level (0-3)\u003c/h4>\u003cp class=\"mb0\">in order to verify the targeted improvement of each module for a specific disease level, we conducted the following comparative experiments as shown in the \u003ca href=\"#t6\">table&#xa0;6\u003c/a>, when hfsm is introduced alone, the f1-score of level 0 increases from 83.10% to 85.60%, effectively reducing feature confusion between healthy leaves and early-stage lesions. after adding ecfm, the f1-score of level 1 rises from 73.20% (with hfsm alone) to 79.80%, significantly mitigating the recognition bias caused by blurred edges of small lesions. amsddicm increases the f1-score of level 3 from 80.37% to 85.54%, enhancing the adaptability to the morphology of large-scale fused scorched areas. with the collaboration of the three modules, the f1-scores of level 0&#x2013;3 reach 93.99%, 82.57%, 84.28%, and 85.54% respectively, achieving balanced optimization of performance across all levels.\u003c/p>\u003ch3>3.2 results comparison of different algorithms and statistical significance verification\u003c/h3>\u003ch4>3.2.1 comparison of grading results for different classification models\u003c/h4>\u003cp class=\"mb0\">to verify the effectiveness of the cogfuse-mobilevit model, we selected 9 commonly used classification models to compare with the optimized cogfuse-mobilevit model, and the results are the average of 5-fold cross-validation over 120 epochs. as shown in \u003ca href=\"#t7\">table&#xa0;7\u003c/a>, the proposed cogfuse-mobilevit model achieved the highest grading performance in terms of precision, recall, and f1-score for identifying walnut leaf brown spot disease at different severity levels among all compared models. the improved cogfuse-mobilevit model exhibits an accuracy of 86.61%, representing a 7.80-percentage-point improvement over the original model. overall, the experimental results highlight that the enhanced cogfuse-mobilevit model is more conducive to focusing on lesion edge details and accurately learning the features of different severity levels of walnut leaf brown spot disease, thereby improving the model&#x2019;s classification performance.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;7\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" name=\"table&#xa0;7\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" alt=\"www.frontiersin.org\" id=\"t7\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;7\u003c/b>. comparison of grading results for different classification models.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.2.2 comparison of performance and reliability validation of different algorithms\u003c/h4>\u003cp class=\"mb0\">in model training, to quantify the reliability of results and avoid random biases from single experiments, this study employs 5-fold cross-validation to generate 95% confidence intervals, as shown in the \u003ca href=\"#f9\">figure&#xa0;9\u003c/a>. the results demonstrate that cogfuse-mobilevit takes a significant lead with an accuracy of 86.61% (95% ci: [85.24%, 87.89%]). its narrowest confidence interval range indicates the strongest generalization capability. among the comparative models, mobilevitv3 has a 95% ci of [77.20%, 80.42%]; its lower bound is significantly lower than that of cogfuse-mobilevit, confirming that the 7.8% performance improvement is not due to random fluctuations. furthermore, the confidence intervals constructed through 5-fold cross-validation further highlight the reliability of the experimental results.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;9\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" alt=\"bar chart showing confidence intervals for accuracy rates of different classification models. models with higher accuracy include cogfuse-mobilevit at 86.61 and mobilevitv3 at 78.81. other models range from 73.63 to 68.72. each model is color-coded, with a corresponding legend on the right.\" id=\"f9\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;9\u003c/b>. confidence intervals of accuracy for different classification models in 5-fold cross-validation.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.2.3 statistical significance verification of model improvement\u003c/h4>\u003cp class=\"mb0\">to statistically evaluate the performance improvement of cogfuse-mobilevit over the baseline mobilevitv3, an independent samples t-test was conducted (\u003ca href=\"#t8\">table&#xa0;8\u003c/a>). for each model architecture, five independent training runs. the null hypothesis stated that the mean accuracy of cogfuse-mobilevit was equal to that of mobilevitv3 \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im14\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">h\u003c/mi>\u003cmn mathsize=\"10.5pt\">0\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">:\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">c\u003c/mi>\u003cmtext mathsize=\"10.5pt\">og\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">m\u003c/mi>\u003cmi mathsize=\"10.5pt\">o\u003c/mi>\u003cmi mathsize=\"10.5pt\">b\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>), while the alternative hypothesis stated that cogfuse-mobilevit had a higher mean accuracy (\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im15\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">h\u003c/mi>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">:\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">c\u003c/mi>\u003cmtext mathsize=\"10.5pt\">og\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&gt;\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">m\u003c/mi>\u003cmi mathsize=\"10.5pt\">o\u003c/mi>\u003cmi mathsize=\"10.5pt\">b\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>)cogfuse-mobilevit achieved a mean accuracy, significantly outperforming mobilevitv3. the independent samples t-test confirmed this improvement (t(8)=18.92,p=3.7&#xd7;10 \u003csup>&#x2212;8\u003c/sup>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;8\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" name=\"table&#xa0;8\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" alt=\"www.frontiersin.org\" id=\"t8\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;8\u003c/b>. comparison of accuracy results between mobilevitv3 and cogfuse-mobilevit using independent samples t-test.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">under 5-fold cross-validation (\u003ca href=\"#f10\">figure&#xa0;10\u003c/a>), cogfuse-mobilevit achieves rapid convergence within the first 20 epochs, demonstrating more efficient feature learning capability. upon entering the steady-state phase, its validation loss is reduced by over 5-fold compared to the baseline mobilevitv3, directly confirming smaller prediction errors and superior generalization capability. meanwhile, the smooth and minimally fluctuating loss curve of cogfuse-mobilevit reflects strong robustness against data noise and distribution variations. combined with the previous statistical findings from accuracy confidence intervals and independent t-tests, these results collectively validate the statistical significance of the improved model.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;10\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" alt=\"line graph showing validation loss over epochs for two models: cogfuse-mobilevit (blue) and mobilevitv3 (orange). loss decreases sharply initially, then stabilizes, with cogfuse-mobilevit exhibiting a consistently lower loss.\" id=\"f10\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;10\u003c/b>. comparison of loss curves between the cogfuse-mobilevit model and the original model in 5-fold cross-validation within 120 epochs.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.3 model result analysis performance comparison of different models\u003c/h3>\u003cp class=\"mb0\">\u003ca href=\"#f11\">figure&#xa0;11a\u003c/a> shows that in the walnut leaf brown spot disease severity grading task, the classification accuracy of all models gradually improved and stabilized with the increase of training epochs, indicating that the models continuously optimized during learning until convergence. cogfuse-mobilevit stood out prominently: it achieved rapid accuracy improvement, reached a high level in relatively early training epochs with minimal subsequent fluctuations, and stably maintained the highest accuracy, demonstrating fast convergence and strong generalization ability to efficiently extract features distinguishing different disease levels. densenet also maintained high accuracy in the late training stage, but its accuracy improvement was slower, with slightly inferior convergence speed and final stability compared to cogfuse-mobilevit. models like resnet and regnet showed limited accuracy gains with gentle upward trends, and their final stable accuracy values were significantly lower, reflecting insufficient feature extraction capabilities possibly due to network architecture or parameter optimization efficiency.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;11\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" alt=\"panel a shows a line graph comparing the accuracy of different neural networks over 100 epochs. cogfuse-mobilevit reaches the highest accuracy, achieving over 90 percent. panel b displays a line graph comparing loss over epochs. cogfuse-mobilevit has the lowest loss, declining swiftly to under 0.2. the legend lists networks like densenet, efficientnet, and mobilevitv3.\" id=\"f11\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;11\u003c/b>. comparison of accuracy and loss across different models \u003cb>(a)\u003c/b> accuracy line chart; \u003cb>(b)\u003c/b> loss line chart.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">in \u003ca href=\"#f11\">figure&#xa0;11b\u003c/a>, all models showed high initial loss values that dropped rapidly and then stabilized, following the typical learning process of iterative optimization. cogfuse-mobilevit achieved significant early loss decline and stabilized near the minimum, indicating efficient feature learning and strong fitting capability. while densenet also reached a relatively low loss level, it remained slightly higher than cogfuse-mobilevit. other models had higher late-stage loss values: some showed fast initial decline but converged to higher levels, indicating shortcomings in capturing key disease features.\u003c/p>\u003ch3>3.4 analysis of detection results for different classification models\u003c/h3>\u003cp class=\"mb0\">the confusion matrix is a key indicator for evaluating classification models: the higher the diagonal values, the higher the classification accuracy for the corresponding category, and the lower the off-diagonal values, the fewer misjudgments. \u003ca href=\"#f12\">figure&#xa0;12\u003c/a> shows the classification results of different models for the four disease grades (0&#x2013;3) of walnut leaf brown spot disease. it is clearly evident that the overall classification performance, particularly the accuracy of cogfuse-mobilevit across all categories, is remarkably high with minimal misjudgments, demonstrating that the model has enhanced discrimination ability for disease grades and performs optimally in distinguishing the four disease levels.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;12\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" alt=\"six normalized confusion matrices labeled a to f show predicted versus true values ranging from zero to three. each matrix illustrates classification performance with varying accuracy per class, indicated by different shades of red. brighter reds denote higher accuracy, while lighter shades indicate lower accuracy or misclassification.\" id=\"f12\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;12\u003c/b>. confusion matrices of the improved cogfuse-mobilevit model and traditional classification models \u003cb>(a)\u003c/b> densenet; \u003cb>(b)\u003c/b> efficientnet; \u003cb>(c)\u003c/b> efficientnetv2; \u003cb>(d)\u003c/b> swin transformer; \u003cb>(e)\u003c/b> mobilevitv3; \u003cb>(f)\u003c/b> cogfuse-mobilevit.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">the edge-enhanced feature module (ecfm) effectively addressed level 0 and level 1 misclassification by capturing incipient lesion features. baseline analysis revealed substantial false negatives for early-stage lesions, with level 1 and level 0 misclassification reaching 32%. ecfm reduced this rate to 8%, demonstrating its capacity to resolve texture confusion through enhanced edge contour delineation. similarly, the adaptive multi-scale dilated convolution module (amsddicm) significantly mitigated level 2 and level 3 mutual misclassification. amsddicm drastically reduced both errors by enhancing local fine-grained features in mid-stage lesions (level 2) while strengthening global fusion features in late-stage coalesced lesions (level 3). this resolves grading ambiguity arising from the morphological continuum of lesion progression.\u003c/p>\u003cp class=\"mb0\">to establish a comprehensive performance evaluation framework and quantify the model&#x2019;s overall discriminative capability, \u003ca href=\"#f13\">figure&#xa0;13\u003c/a> presents the receiver operating characteristic (roc) curves of cogfuse-mobilevit across four severity levels. these curves compare the true positive rate (tpr) against the false positive rate (fpr) by dynamically adjusting the classification threshold. the area under the curve (auc) serves as the primary performance metric, where a higher value indicates stronger classification ability. notably, the auc values for all categories significantly exceed the level of random guessing (auc=0.5), fully demonstrating that the model possesses robust discriminative capability across different severity levels.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;13\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" alt=\"receiver operating characteristic (roc) curve chart showing true positive rate versus false positive rate for four classes: class 0 (blue), class 1 (orange), class 2 (green), and class 3 (red), along with an average roc curve (dashed purple). the curves are above the diagonal, indicating good model performance.\" id=\"f13\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;13\u003c/b>. roc curves of the four different severity levels for cogfuse-mobilevit.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.5 tsne visualization of features extracted by different models\u003c/h3>\u003cp class=\"mb0\">the tsne visualization of features extracted from the models is shown in \u003ca href=\"#f14\">figures&#xa0;14a&#x2013;d\u003c/a>. this study analyzed the features of the original model at the 20th, 50th, and 100th training epochs, as well as the improved model at the 100th epoch. after 20 epochs of training, the original model showed a highly discrete feature distribution. limited by the number of training epochs, the model failed to fully learn discriminative features, resulting in insufficient distinction between categories and significant overlap among different classes. this indicates that under this training intensity, the original model&#x2019;s ability to capture meaningful patterns was relatively limited. when the original model was trained to 50 epochs, compared with (a), the feature aggregation significantly improved. however, inter-class overlap still existed, suggesting that although prolonged training aided feature learning, the original model&#x2019;s architecture had inherent defects in achieving clear feature separation. at 100 epochs of training, the original model exhibited more prominent feature clustering. nevertheless, some regions still lacked clear separation between different categories, indicating that even after prolonged training, the original model faced challenges in maximizing inter-class distance and intra-class compactness. in figure (d), the improved model after 100 epochs of training demonstrated a more superior feature distribution: each category was tightly clustered, achieving high intra-class compactness, while distinct boundaries between different categories were established, resulting in significant inter-class distance. this suggests that the model improvements effectively enhanced its ability to extract discriminative features, greatly reduced feature ambiguity, and improved feature discriminative power. compared with the original model, it showed stronger feature discrimination and optimization potential.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;14\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" alt=\"four scatter plots labeled a, b, c, and d illustrate data points grouped by colors representing levels zero to three. each plot displays varying cluster formations. plot a shows two distinct clusters. plot b has dispersed clusters, with one elongated group. plot c features overlapping clusters, and plot d displays four well-separated clusters.\" id=\"f14\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;14\u003c/b>. tsne visualization of features extracted by different models. after the three modules work synergistically, the boundaries of feature clustering between level 0 (healthy) and level 1 (early-stage) are significantly clearer, which verifies the effectiveness of edge-texture collaborative learning \u003cb>(a)\u003c/b> the original model was trained for 20 epochs; \u003cb>(b)\u003c/b> the original model was trained for 50 epochs; \u003cb>(c)\u003c/b> the original model was trained for 100 epochs; \u003cb>(d)\u003c/b> the improved model was trained for 100 epochs.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.6 radar chart for comparison of classification performance between original and improved models\u003c/h3>\u003cp class=\"mb0\">as shown in \u003ca href=\"#f15\">figure&#xa0;15\u003c/a>, a radar chart compares the performance of the original model and the improved cogfuse-mobilevit model across multiple evaluation metrics. in terms of accuracy, cogfuse-mobilevit exhibits significantly higher values than the original mobilevitv3 model, indicating that the improved model achieves overall higher classification accuracy. macro-average precision, which considers the average precision of each category, clearly shows that cogfuse-mobilevit also performs better, meaning it has superior precision across all categories. meanwhile, cogfuse-mobilevit also demonstrates better macro-average recall. when examining weighted-average precision and weighted-average recall&#x2014;metrics that account for class imbalance&#x2014;cogfuse-mobilevit outperforms mobilevitv3 in both, indicating that in practical applications, even with uneven class distribution, the cogfuse-mobilevit model can deliver better performance. these results further demonstrate the model&#x2019;s reliability and practicality.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;15\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" alt=\"radar chart comparing two models: mobilevitv3 (red) and cogfuse-mobilevit (blue). metrics include accuracy, macro average precision, macro average recall, weighted average precision, and weighted average recall. each axis ranges from fifty to one hundred.\" id=\"f15\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;15\u003c/b>. multi-metric comparison between mobilevitv3 and improved cogfuse-mobilevit model.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.7 public data set experiment\u003c/h3>\u003cp class=\"mb15\">to further validate the efficacy and generalizability of the proposed cogfuse-mobilevit model, experiments were conducted on the public dataset appleleaf9 (\u003ca href=\"#b38\">yang et&#xa0;al., 2022\u003c/a>). this dataset comprises healthy apple leaves and eight categories of apple leaf diseases captured in field environments without restrictions on imaging angles or noise interference. the dataset was partitioned into training and test sets at an 8:2 ratio. all images were resized to 224&#xd7;224 pixels to optimize deep learning model training efficiency. following the hyperparameter configurations specified in &#x201c;experimental parameters and evaluation metrics&#x201d;, both the baseline mobilevitv3 and cogfuse-mobilevit models were trained and evaluated.\u003c/p>\u003cp class=\"mb0\">cross-species validation demonstrates that the cogfuse-mobilevit model delivers exceptional performance on the appleleaf9 dataset. as presented in \u003ca href=\"#t9\">table&#xa0;9\u003c/a>, the model comprehensively surpasses the baseline mobilevitv3 across all four core metrics: precision increases by 0.16 percentage points, recall achieves a breakthrough improvement of 3.45 percentage points, f1-score rises by 1.18 percentage points, and accuracy elevates by 1.14 percentage points. the synergistic enhancement in both precision and recall signifies that performance gains stem from strengthened feature discriminability rather than metric trade-off compromises. the remarkable percentage points recall gain substantially mitigates leaf disease omission rates, while the holistic advance in f1-score further attests to the model&#x2019;s robustness. these results collectively validate the generalizability of cogfuse-mobilevit&#x2019;s core innovations in agricultural fine-grained disease grading, establishing a transferable paradigm for small-target pathology identification across plant species.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;9\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" name=\"table&#xa0;9\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" alt=\"www.frontiersin.org\" id=\"t9\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;9\u003c/b>. experimental results of the original model and cogfuse-mobilevit model on appleleaf9 data set.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ca id=\"h5\" name=\"h5\">\u003c/a>\u003ch2>4 conclusion\u003c/h2>\u003cp class=\"mb0\">this study addresses the challenges in precise grading of walnut leaf brown spot disease. by adopting dynamic feature filtering, edge gradient reinforcement, and multi-scale morphological adaptation, it effectively resolves three key limitations of existing hybrid architectures and the baseline model mobilevitv3 in plant disease grading weak capability in capturing blurred edges, poor multi-scale adaptability, and interference from healthy tissues. experimental results show that the model achieves an 86.61% grading accuracy on a dataset encompassing diverse lighting conditions, cultivars, and lesion stages, representing a 7.80% improvement over the baseline mobilevitv3 and outperforming nine state-of-the-art models. at the same time, this study confirmed that the performance improvement of cogfuse-mobilevit was statistically significant and stable through strict statistical tests, providing a reliable method for accurate classification of walnut leaf spot disease. the constructed image dataset and proposed grading re-measurement method lay the foundation for accuracy. this approach provides a new paradigm for small-target disease grading, with core modules transferable to multi-crop disease recognition. beyond disease classification, context-aware ai frameworks demonstrate significant efficacy in agricultural management (\u003ca href=\"#b1\">acharya et&#xa0;al., 2022\u003c/a>). iot systems dynamically adjust fertilization recommendations by integrating real-time soil-crop data, while salinity-corrected evapotranspiration (ets) models optimize irrigation strategies for saline-alkali soils (\u003ca href=\"#b15\">li et&#xa0;al., 2023\u003c/a>; \u003ca href=\"#b30\">su et&#xa0;al., 2023\u003c/a>). similarly, multimodal fusion techniques enhance reference evapotranspiration (eto) prediction accuracy, facilitating precision water allocation (\u003ca href=\"#b11\">jo et&#xa0;al., 2021\u003c/a>; \u003ca href=\"#b27\">rustia et&#xa0;al., 2023\u003c/a>). these breakthroughs collectively validate the robust capability of adaptive feature processing in complex agricultural environments&#x2014;a core principle that resonates with our dynamic weighting strategy for disease feature extraction. future research should therefore integrate cross-modal and cross-domain capabilities to establish multidimensional assessment systems and regional dynamic monitoring frameworks, thereby delivering comprehensive technical solutions for small-target disease control.\u003c/p>\u003ca id=\"h6\" name=\"h6\">\u003c/a>\u003ch2>5 discussion and future work\u003c/h2>\u003cp class=\"mb15\">compared with existing methods, this study overcomes the limitation of traditional deep learning models relying on fixed convolution kernels, enabling adaptive capture of lesion features with multi-shaped convolution kernels to accurately extract edge textures of circular micro-lesions and regional contours of irregular lesions. although the constructed multi-source dataset covers diverse lighting conditions, cultivars, and lesion development stages, the homogeneous internal features of severe diseases lead to limited recall rate. meanwhile, when lesions are severely overlapped or mixed with mechanical damage, insect damage, or other types of injuries, the discriminative accuracy of the model is affected. for ultra-small lesions, the local feature information is too weak and easily overlooked, and the computational efficiency still needs optimization on some hardware platforms. furthermore, the adaptability of the current model in extreme field scenarios, such as high-density occlusion and compound damage from pests and diseases, remains to be further verified. in these scenarios, the coupling of complex interference factors exacerbates feature confusion, affecting grading reliability.\u003c/p>\u003cp class=\"mb15\">to address these issues, future work will focus on the following research directions. future research will focus on enhancing the model&#x2019;s performance in complex field environments through multiple synergistic strategies. this includes constructing multi-interference factor coupled datasets that incorporating insect holes, lesions, and soil adhesion to strengthen robustness against extreme field disturbances; introducing attention-based multi-damage feature decoupling modules and dedicated micro-lesion enhancement modules combining super-resolution and feature interpolation to improve discriminability in complex scenarios and for tiny lesions;&#xa0;optimizing dynamic weight calculations via approximate computation or hardware-friendly reconstruction, while developing lightweight real-time deployment frameworks integrated with uav near-ground sensing technology to boost efficiency and practical monitoring coverage; and establishing cross-crop pathological transfer learning mechanisms, extending the model to multi-crop disease recognition tasks with self-supervised pre-training to enhance algorithm universality and low-contrast lesion feature mining capabilities. through the above research, it is expected to further improve the applicability and practicality of the model in complex field environments, promoting the development of intelligent plant disease grading technology towards more accurate, efficient, and universal directions.\u003c/p>\u003ca id=\"h7\" name=\"h7\">\u003c/a>\u003ch2>data availability statement\u003c/h2>\u003cp class=\"mb0\">the raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.\u003c/p>\u003ca id=\"h8\" name=\"h8\">\u003c/a>\u003ch2>author contributions\u003c/h2>\u003cp class=\"mb0\">yw: writing &#x2013; original draft. dz: writing &#x2013; original draft. lz:&#xa0;writing &#x2013; review &amp; editing.\u003c/p>\u003ca id=\"h9\" name=\"h9\">\u003c/a>\u003ch2>funding\u003c/h2>\u003cp class=\"mb0\">the author(s) declare financial support was received for the research and/or publication of this article. this work was supported in part by the science and technology key project of the xinjiang production and construction corps (2023ab063) development and application demonstration of green technology for online monitoring of fresh fruits and cold chain logistics in xinjiang.\u003c/p>\u003ca id=\"h10\" name=\"h10\">\u003c/a>\u003ch2>conflict of interest\u003c/h2>\u003cp class=\"mb0\">the authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\u003c/p>\u003ca id=\"h11\" name=\"h11\">\u003c/a>\u003ch2>generative ai statement\u003c/h2>\u003cp class=\"mb15\">the author(s) declare that no generative ai was used in the creation of this manuscript.\u003c/p>\u003cp class=\"mb0\">any alternative text (alt text) provided alongside figures in this article has been generated by frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. if you identify any issues, please contact us.\u003c/p>\u003ca id=\"h12\" name=\"h12\">\u003c/a>\u003ch2>publisher&#x2019;s note\u003c/h2>\u003cp class=\"mb15\">all claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.\u003c/p>\u003ca id=\"h13\" name=\"h13\">\u003c/a>\u003ch2>references\u003c/h2>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b1\" id=\"b1\">\u003c/a> acharya, p., burgers, t., and nguyen, k.-d. (2022). ai-enabled droplet detection and tracking for agricultural spraying systems. \u003ci>computers and electronics in agriculture\u003c/i>, 202, 107325. doi:&#xa0;10.1016/j.compag.2022.107325\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2022.107325\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=p.+acharya&amp;author=t.+burgers&amp;author=k.-d.+nguyen&amp;publication_year=2022&amp;title=ai-enabled%20droplet%20detection%20and%20tracking%20for%20agricultural%20spraying%20systems&amp;journal=computers+and+electronics+in+agriculture&amp;volume=202&amp;pages=107325\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b2\" id=\"b2\">\u003c/a> adaskaveg, j., f&#xf6;rster, h., thompson, d., driever, g., connell, j., buchner, r., et al. (2009). epidemiology and management of walnut blight. \u003ci>walnut res. rep\u003c/i>, 94, 225&#x2013;256. doi:&#xa0;10.1016/j.inpa.2016.10.005\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.inpa.2016.10.005\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.+adaskaveg&amp;author=h.+f%c3%b6rster&amp;author=d.+thompson&amp;author=g.+driever&amp;author=j.+connell&amp;author=r.+buchner&amp;publication_year=2009&amp;title=epidemiology%20and%20management%20of%20walnut%20blight&amp;journal=walnut+res.+rep&amp;volume=94&amp;pages=225\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b3\" id=\"b3\">\u003c/a> arivazhagan, s., shebiah, r. n., ananthi, s., and varthini, s. v. (2013). detection of unhealthy region of plant leaves and classification of plant leaf diseases using texture features. \u003ci>agricultural engineering international: cigr journal\u003c/i>, 15, 211&#x2013;217.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+arivazhagan&amp;author=r.%20n.+shebiah&amp;author=s.+ananthi&amp;author=s.%20v.+varthini&amp;publication_year=2013&amp;title=detection%20of%20unhealthy%20region%20of%20plant%20leaves%20and%20classification%20of%20plant%20leaf%20diseases%20using%20texture%20features&amp;journal=agricultural+engineering+international:+cigr+journal&amp;volume=15&amp;pages=211\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b4\" id=\"b4\">\u003c/a> chaudhary, p., chaudhari, a. k., cheeran, a., and godara, s. (2012). color transform based approach for disease spot detection on plant leaf. \u003ci>international journal of computer science and telecommunications\u003c/i>, 3, 65&#x2013;70.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=p.+chaudhary&amp;author=a.%20k.+chaudhari&amp;author=a.+cheeran&amp;author=s.+godara&amp;publication_year=2012&amp;title=color%20transform%20based%20approach%20for%20disease%20spot%20detection%20on%20plant%20leaf&amp;journal=international+journal+of+computer+science+and+telecommunications&amp;volume=3&amp;pages=65\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b5\" id=\"b5\">\u003c/a> chen, s., zhang, k., zhao, y., sun, y., ban, w., chen, y., et al. (2021). an approach for rice bacterial leaf streak disease segmentation and disease severity estimation. \u003ci>agriculture\u003c/i>, 11, 420. doi:&#xa0;10.3390/agriculture11050420\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agriculture11050420\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+chen&amp;author=k.+zhang&amp;author=y.+zhao&amp;author=y.+sun&amp;author=w.+ban&amp;author=y.+chen&amp;publication_year=2021&amp;title=an%20approach%20for%20rice%20bacterial%20leaf%20streak%20disease%20segmentation%20and%20disease%20severity%20estimation&amp;journal=agriculture&amp;volume=11&amp;pages=420\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b6\" id=\"b6\">\u003c/a> chiang, k.-s., bock, c., el jarroudi, m., delfosse, p., lee, i., and liu, h. (2016). effects of rater bias and assessment method on disease severity estimation with regard to hypothesis testing. \u003ci>phytopathology\u003c/i>, 65, 523&#x2013;535. doi:&#xa0;10.1111/ppa.12435\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1111/ppa.12435\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=k.-s.+chiang&amp;author=c.+bock&amp;author=m.+el%20jarroudi&amp;author=p.+delfosse&amp;author=i.+lee&amp;author=h.+liu&amp;publication_year=2016&amp;title=effects%20of%20rater%20bias%20and%20assessment%20method%20on%20disease%20severity%20estimation%20with%20regard%20to%20hypothesis%20testing&amp;journal=phytopathology&amp;volume=65&amp;pages=523\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b7\" id=\"b7\">\u003c/a> cooke, b. (2006). &#x201c;disease assessment and yield loss,&#x201d; in \u003ci>the epidemiology of plant diseases\u003c/i> (dordrecht: springer), 43&#x2013;80.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=b.+cooke&amp;publication_year=2006&amp;title=disease%20assessment%20and%20yield%20loss&amp;book=the+epidemiology+of+plant+diseases&amp;pages=43\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b8\" id=\"b8\">\u003c/a> ghaiwat, s. n. and arora, p. (2014). detection and classification of plant leaf diseases using image processing techniques: a review. \u003ci>international journal of recent advances in engineering &amp; technology\u003c/i>, 2, 1&#x2013;7.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20n.+ghaiwat&amp;author=p.+arora&amp;publication_year=2014&amp;title=detection%20and%20classification%20of%20plant%20leaf%20diseases%20using%20image%20processing%20techniques%3a%20a%20review&amp;journal=international+journal+of+recent+advances+in+engineering+&amp;+technology&amp;volume=2&amp;pages=1\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b9\" id=\"b9\">\u003c/a> hu, g., wang, h., zhang, y., and wan, m. (2021). detection and severity analysis of tea leaf blight based on deep learning. \u003ci>computers &amp; electrical engineering\u003c/i>, 90, 107023. doi:&#xa0;10.1016/j.compeleceng.2021.107023\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compeleceng.2021.107023\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=g.+hu&amp;author=h.+wang&amp;author=y.+zhang&amp;author=m.+wan&amp;publication_year=2021&amp;title=detection%20and%20severity%20analysis%20of%20tea%20leaf%20blight%20based%20on%20deep%20learning&amp;journal=computers+&amp;+electrical+engineering&amp;volume=90&amp;pages=107023\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b10\" id=\"b10\">\u003c/a> jadhav, s. b. and patil, s. b. (2016). grading of soybean leaf disease based on segmented image using k-means clustering. \u003ci>iaes international journal of artificial intelligence\u003c/i>, 5, 13&#x2013;13. doi:&#xa0;10.11591/ijai.v5.i1.pp13-21\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.11591/ijai.v5.i1.pp13-21\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20b.+jadhav&amp;author=s.%20b.+patil&amp;publication_year=2016&amp;title=grading%20of%20soybean%20leaf%20disease%20based%20on%20segmented%20image%20using%20k-means%20clustering&amp;journal=iaes+international+journal+of+artificial+intelligence&amp;volume=5&amp;pages=13\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b11\" id=\"b11\">\u003c/a> jo, w. j., kim, d. s., sim, h. s., ahn, s. r., lee, h. j., moon, y. h., et al. (2021). estimation of evapotranspiration and water requirements of strawberry plants in greenhouses using environmental data. \u003ci>frontiers in sustainable food systems\u003c/i>, 5, 684808. doi:&#xa0;10.3389/fsufs.2021.684808\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3389/fsufs.2021.684808\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=w.%20j.+jo&amp;author=d.%20s.+kim&amp;author=h.%20s.+sim&amp;author=s.%20r.+ahn&amp;author=h.%20j.+lee&amp;author=y.%20h.+moon&amp;publication_year=2021&amp;title=estimation%20of%20evapotranspiration%20and%20water%20requirements%20of%20strawberry%20plants%20in%20greenhouses%20using%20environmental%20data&amp;journal=frontiers+in+sustainable+food+systems&amp;volume=5&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b12\" id=\"b12\">\u003c/a> khan, m. a., ali, m., shah, m., mahmood, t., ahmad, m., jhanjhi, n., et al. (2021). machine learning-based detection and classification of walnut fungi diseases. \u003ci>intelligent automation &amp; soft computing\u003c/i>, 30, 771&#x2013;785. doi:&#xa0;10.32604/iasc.2021.018039\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.32604/iasc.2021.018039\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=m.%20a.+khan&amp;author=m.+ali&amp;author=m.+shah&amp;author=t.+mahmood&amp;author=m.+ahmad&amp;author=n.+jhanjhi&amp;publication_year=2021&amp;title=machine%20learning-based%20detection%20and%20classification%20of%20walnut%20fungi%20diseases&amp;journal=intelligent+automation+&amp;+soft+computing&amp;volume=30&amp;pages=771\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b13\" id=\"b13\">\u003c/a> kim, s.-k. and ahn, j.-g. (2021). tomato crop diseases classification models using deep cnn-based architectures. \u003ci>j korea acad-ind coop soc\u003c/i>, 22, 7&#x2013;14. doi:&#xa0;10.5762/kais.2021.22.5.7\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.5762/kais.2021.22.5.7\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.-k.+kim&amp;author=j.-g.+ahn&amp;publication_year=2021&amp;title=tomato%20crop%20diseases%20classification%20models%20using%20deep%20cnn-based%20architectures&amp;journal=j+korea+acad-ind+coop+soc&amp;volume=22&amp;pages=7\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b14\" id=\"b14\">\u003c/a> lamichhane, j. r. (2014). xanthomonas arboricola diseases of stone fruit, almond, and walnut trees: progress toward understanding and management. \u003ci>plant disease\u003c/i>, 98, 1600&#x2013;1610. doi:&#xa0;10.1094/pdis-08-14-0831-fe\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30703892/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-08-14-0831-fe\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.%20r.+lamichhane&amp;publication_year=2014&amp;title=xanthomonas%20arboricola%20diseases%20of%20stone%20fruit%2c%20almond%2c%20and%20walnut%20trees%3a%20progress%20toward%20understanding%20and%20management&amp;journal=plant+disease&amp;volume=98&amp;pages=1600\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b15\" id=\"b15\">\u003c/a> li, x., zha, t., liu, p., bourque, c. p.-a., jia, x., and tian, y. (2023). interannual variation in gross ecosystem production and evapotranspiration in a temperate semiarid grassland undergoing vegetation recovery. \u003ci>agricultural and forest meteorology\u003c/i>, 341, 109672. doi:&#xa0;10.1016/j.agrformet.2023.109672\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.agrformet.2023.109672\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=x.+li&amp;author=t.+zha&amp;author=p.+liu&amp;author=c.%20p.-a.+bourque&amp;author=x.+jia&amp;author=y.+tian&amp;publication_year=2023&amp;title=interannual%20variation%20in%20gross%20ecosystem%20production%20and%20evapotranspiration%20in%20a%20temperate%20semiarid%20grassland%20undergoing%20vegetation%20recovery&amp;journal=agricultural+and+forest+meteorology&amp;volume=341&amp;pages=109672\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b16\" id=\"b16\">\u003c/a> lin, k., gong, l., huang, y., liu, c., and pan, j. (2019). deep learning-based segmentation and quantification of cucumber powdery mildew using convolutional neural network. \u003ci>frontiers in plant science\u003c/i>, 10, 155. doi:&#xa0;10.3389/fpls.2019.00155\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30891048/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.3389/fpls.2019.00155\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=k.+lin&amp;author=l.+gong&amp;author=y.+huang&amp;author=c.+liu&amp;author=j.+pan&amp;publication_year=2019&amp;title=deep%20learning-based%20segmentation%20and%20quantification%20of%20cucumber%20powdery%20mildew%20using%20convolutional%20neural%20network&amp;journal=frontiers+in+plant+science&amp;volume=10&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b17\" id=\"b17\">\u003c/a> ma, j., du, k., zhang, l., zheng, f., chu, j., and sun, z. (2017). a segmentation method for greenhouse vegetable foliar disease spots images using color information and region growing. \u003ci>computers and electronics in agriculture\u003c/i>, 142, 110&#x2013;117. doi:&#xa0;10.1016/j.compag.2017.08.023\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2017.08.023\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.+ma&amp;author=k.+du&amp;author=l.+zhang&amp;author=f.+zheng&amp;author=j.+chu&amp;author=z.+sun&amp;publication_year=2017&amp;title=a%20segmentation%20method%20for%20greenhouse%20vegetable%20foliar%20disease%20spots%20images%20using%20color%20information%20and%20region%20growing&amp;journal=computers+and+electronics+in+agriculture&amp;volume=142&amp;pages=110\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b18\" id=\"b18\">\u003c/a> mao, r., wang, z., li, f., zhou, j., chen, y., and hu, x. (2023). gseyolox-s: an improved lightweight network for identifying the severity of wheat fusarium head blight. \u003ci>agronomy\u003c/i>, 13, 242. doi:&#xa0;10.3390/agronomy13010242\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agronomy13010242\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=r.+mao&amp;author=z.+wang&amp;author=f.+li&amp;author=j.+zhou&amp;author=y.+chen&amp;author=x.+hu&amp;publication_year=2023&amp;title=gseyolox-s%3a%20an%20improved%20lightweight%20network%20for%20identifying%20the%20severity%20of%20wheat%20fusarium%20head%20blight&amp;journal=agronomy&amp;volume=13&amp;pages=242\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b19\" id=\"b19\">\u003c/a> mcgranahan, g. and leslie, c. (1991). walnuts (juglans). \u003ci>molecules\u003c/i>, 907&#x2013;924. doi:&#xa0;10.17660/actahortic.1991.290.20\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.17660/actahortic.1991.290.20\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=g.+mcgranahan&amp;author=c.+leslie&amp;publication_year=1991&amp;title=walnuts%20%28juglans%29&amp;journal=molecules&amp;pages=907\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b20\" id=\"b20\">\u003c/a> mircetich, s. m., sanborn, r., and ramos, d. (1980). natural spread, graft-transmission, and possible etiology of walnut blackline disease. \u003ci>phytopathology\u003c/i>, 70, 962&#x2013;968. doi:&#xa0;10.1094/phyto-70-962\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/phyto-70-962\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20m.+mircetich&amp;author=r.+sanborn&amp;author=d.+ramos&amp;publication_year=1980&amp;title=natural%20spread%2c%20graft-transmission%2c%20and%20possible%20etiology%20of%20walnut%20blackline%20disease&amp;journal=phytopathology&amp;volume=70&amp;pages=962\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b21\" id=\"b21\">\u003c/a> moragrega, c., matias, j., alet&#xe0;, n., montesinos, e., and rovira, m. (2011). apical necrosis and premature drop of persian (english) walnut fruit caused by xanthomonas arboricola pv. juglandis. \u003ci>plant disease\u003c/i>, 95, 1565&#x2013;1570. doi:&#xa0;10.1094/pdis-03-11-0259\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30732020/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-03-11-0259\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=c.+moragrega&amp;author=j.+matias&amp;author=n.+alet%c3%a0&amp;author=e.+montesinos&amp;author=m.+rovira&amp;publication_year=2011&amp;title=apical%20necrosis%20and%20premature%20drop%20of%20persian%20%28english%29%20walnut%20fruit%20caused%20by%20xanthomonas%20arboricola%20pv.%20juglandis&amp;journal=plant+disease&amp;volume=95&amp;pages=1565\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b22\" id=\"b22\">\u003c/a> ngugi, l. c., abdelwahab, m., and abo-zahhad, m. (2020). tomato leaf segmentation algorithms for mobile phone applications using deep learning. \u003ci>computers and electronics in agriculture\u003c/i>, 178, 105788. doi:&#xa0;10.1016/j.compag.2020.105788\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2020.105788\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=l.%20c.+ngugi&amp;author=m.+abdelwahab&amp;author=m.+abo-zahhad&amp;publication_year=2020&amp;title=tomato%20leaf%20segmentation%20algorithms%20for%20mobile%20phone%20applications%20using%20deep%20learning&amp;journal=computers+and+electronics+in+agriculture&amp;volume=178&amp;pages=105788\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b23\" id=\"b23\">\u003c/a> ozturk, o., sarica, b., and seker, d. z. (2025). interpretable and robust ensemble deep learning framework for tea leaf disease classification. \u003ci>horticulturae\u003c/i>, 11, 437. doi:&#xa0;10.3390/horticulturae11040437\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/horticulturae11040437\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=o.+ozturk&amp;author=b.+sarica&amp;author=d.%20z.+seker&amp;publication_year=2025&amp;title=interpretable%20and%20robust%20ensemble%20deep%20learning%20framework%20for%20tea%20leaf%20disease%20classification&amp;journal=horticulturae&amp;volume=11&amp;pages=437\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b24\" id=\"b24\">\u003c/a> parashar, n., johri, p., khan, a. a., gaur, n., and kadry, s. (2024). an integrated analysis of yield prediction models: a comprehensive review of advancements and challenges. \u003ci>computers, materials &amp; continua\u003c/i>, 80 (1). doi:&#xa0;10.32604/cmc.2024.050240\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.32604/cmc.2024.050240\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=n.+parashar&amp;author=p.+johri&amp;author=a.%20a.+khan&amp;author=n.+gaur&amp;author=s.+kadry&amp;publication_year=2024&amp;title=an%20integrated%20analysis%20of%20yield%20prediction%20models%3a%20a%20comprehensive%20review%20of%20advancements%20and%20challenges&amp;journal=computers,+materials+&amp;+continua&amp;volume=80&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b25\" id=\"b25\">\u003c/a> picon, a., alvarez-gila, a., seitz, m., ortiz-barredo, a., echazarra, j., and johannes, a. (2019). deep convolutional neural networks for mobile capture device-based crop disease classification in the wild. \u003ci>computers and electronics in agriculture\u003c/i>, 161, 280&#x2013;290. doi:&#xa0;10.1016/j.compag.2018.04.002\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2018.04.002\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=a.+picon&amp;author=a.+alvarez-gila&amp;author=m.+seitz&amp;author=a.+ortiz-barredo&amp;author=j.+echazarra&amp;author=a.+johannes&amp;publication_year=2019&amp;title=deep%20convolutional%20neural%20networks%20for%20mobile%20capture%20device-based%20crop%20disease%20classification%20in%20the%20wild&amp;journal=computers+and+electronics+in+agriculture&amp;volume=161&amp;pages=280\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b26\" id=\"b26\">\u003c/a> rastogi, a., arora, r., and sharma, s. (2015). &#x201c;leaf disease detection and grading using computer vision technology &amp; fuzzy logic,&#x201d; in paper presented at the 2015 2nd international conference on signal processing and integrated networks (spin).\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=a.+rastogi&amp;author=r.+arora&amp;author=s.+sharma&amp;publication_year=2015&amp;title=leaf%20disease%20detection%20and%20grading%20using%20computer%20vision%20technology%20%26%20fuzzy%20logic&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b27\" id=\"b27\">\u003c/a> rustia, d. j. a., lee, w.-c., lu, c.-y., wu, y.-f., shih, p.-y., and chen, s.-k. (2023). edge-based wireless imaging system for continuous monitoring of insect pests in a remote outdoor mango orchard. \u003ci>computers and electronics in agriculture\u003c/i>, 211, 108019. doi:&#xa0;10.1016/j.compag.2023.108019\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2023.108019\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=d.%20j.%20a.+rustia&amp;author=w.-c.+lee&amp;author=c.-y.+lu&amp;author=y.-f.+wu&amp;author=p.-y.+shih&amp;author=s.-k.+chen&amp;publication_year=2023&amp;title=edge-based%20wireless%20imaging%20system%20for%20continuous%20monitoring%20of%20insect%20pests%20in%20a%20remote%20outdoor%20mango%20orchard&amp;journal=computers+and+electronics+in+agriculture&amp;volume=211&amp;pages=108019\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b28\" id=\"b28\">\u003c/a> shi, t., liu, y., zheng, x., hu, k., huang, h., liu, h., et al. (2023). recent advances in plant disease severity assessment using convolutional neural networks. \u003ci>scientific reports\u003c/i>, 13, 2336. doi:&#xa0;10.1038/s41598-023-29230-7\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/36759626/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1038/s41598-023-29230-7\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=t.+shi&amp;author=y.+liu&amp;author=x.+zheng&amp;author=k.+hu&amp;author=h.+huang&amp;author=h.+liu&amp;publication_year=2023&amp;title=recent%20advances%20in%20plant%20disease%20severity%20assessment%20using%20convolutional%20neural%20networks&amp;journal=scientific+reports&amp;volume=13&amp;pages=2336\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b29\" id=\"b29\">\u003c/a> singh, u. p., chouhan, s. s., jain, s., and jain, s. (2019). multilayer convolution neural network for the classification of mango leaves infected by anthracnose disease. \u003ci>ieee access\u003c/i>, 7, 43721&#x2013;43729. doi:&#xa0;10.1109/access.2019.2907383\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/access.2019.2907383\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=u.%20p.+singh&amp;author=s.%20s.+chouhan&amp;author=s.+jain&amp;author=s.+jain&amp;publication_year=2019&amp;title=multilayer%20convolution%20neural%20network%20for%20the%20classification%20of%20mango%20leaves%20infected%20by%20anthracnose%20disease&amp;journal=ieee+access&amp;volume=7&amp;pages=43721\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b30\" id=\"b30\">\u003c/a> su, y., wang, j., li, j., wang, l., wang, k., li, a., et al. (2023). spatiotemporal changes and driving factors of reference evapotranspiration and crop evapotranspiration for cotton production in china from 1960 to 2019. \u003ci>frontiers in environmental science\u003c/i>, 11, 1251789. doi:&#xa0;10.3389/fenvs.2023.1251789\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3389/fenvs.2023.1251789\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=y.+su&amp;author=j.+wang&amp;author=j.+li&amp;author=l.+wang&amp;author=k.+wang&amp;author=a.+li&amp;publication_year=2023&amp;title=spatiotemporal%20changes%20and%20driving%20factors%20of%20reference%20evapotranspiration%20and%20crop%20evapotranspiration%20for%20cotton%20production%20in%20china%20from%201960%20to%202019&amp;journal=frontiers+in+environmental+science&amp;volume=11&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b31\" id=\"b31\">\u003c/a> tripathi, r. (2021). &#x201c;a deep learning approach for plant material disease identification,&#x201d; in paper presented at the iop conference series: materials science and engineering.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=r.+tripathi&amp;publication_year=2021&amp;title=a%20deep%20learning%20approach%20for%20plant%20material%20disease%20identification&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b32\" id=\"b32\">\u003c/a> vishnoi, v. k., kumar, k., kumar, b., mohan, s., and khan, a. a. (2022). detection of apple plant diseases using leaf images through convolutional neural network . \u003ci>ieee access\u003c/i>, 11, 6594&#x2013;6609. doi:&#xa0;10.1109/access.2022.3232917\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/access.2022.3232917\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=v.%20k.+vishnoi&amp;author=k.+kumar&amp;author=b.+kumar&amp;author=s.+mohan&amp;author=a.%20a.+khan&amp;publication_year=2022&amp;title=detection%20of%20apple%20plant%20diseases%20using%20leaf%20images%20through%20convolutional%20neural%20network&amp;journal=ieee+access&amp;volume=11&amp;pages=6594\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b33\" id=\"b33\">\u003c/a> wang, f., dun, c., tang, t., duan, y., guo, x., and you, j. (2022). boeremia exigua causes leaf spot of walnut trees (juglans regia) in china. \u003ci>plant disease\u003c/i>, 106, 1993. doi:&#xa0;10.1094/pdis-10-21-2304-pdn\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/pdis-10-21-2304-pdn\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=f.+wang&amp;author=c.+dun&amp;author=t.+tang&amp;author=y.+duan&amp;author=x.+guo&amp;author=j.+you&amp;publication_year=2022&amp;title=boeremia%20exigua%20causes%20leaf%20spot%20of%20walnut%20trees%20%28juglans%20regia%29%20in%20china&amp;journal=plant+disease&amp;volume=106&amp;pages=1993\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b34\" id=\"b34\">\u003c/a> wang, q.-h., fan, k., li, d.-w., han, c.-m., qu, y.-y., qi, y.-k., et al. (2020). identification, virulence and fungicide sensitivity of colletotrichum gloeosporioides ss responsible for walnut anthracnose disease in china. \u003ci>plant disease\u003c/i>, 104, 1358&#x2013;1368. doi:&#xa0;10.1094/pdis-12-19-2569-re\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/32196416/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-12-19-2569-re\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=q.-h.+wang&amp;author=k.+fan&amp;author=d.-w.+li&amp;author=c.-m.+han&amp;author=y.-y.+qu&amp;author=y.-k.+qi&amp;publication_year=2020&amp;title=identification%2c%20virulence%20and%20fungicide%20sensitivity%20of%20colletotrichum%20gloeosporioides%20ss%20responsible%20for%20walnut%20anthracnose%20disease%20in%20china&amp;journal=plant+disease&amp;volume=104&amp;pages=1358\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b35\" id=\"b35\">\u003c/a> weber, b. c. (1980). \u003ci>how to diagnose black walnut damage\u003c/i> vol. 57 (north central forest experiment station, forest service, us department of agriculture).\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=b.%20c.+weber&amp;publication_year=1980&amp;book=how+to+diagnose+black+walnut+damage&amp;volume=57&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b36\" id=\"b36\">\u003c/a> xu, w. (2024). hcf-net: hierarchicalcontextfusion network forinfrared small object detection. \u003ci>arxiv\u003c/i>. arxiv, 2403.10778. doi:&#xa0;10.1109/icme57554.2024.10687431\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/icme57554.2024.10687431\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=w.+xu&amp;publication_year=2024&amp;title=hcf-net%3a%20hierarchicalcontextfusion%20network%20forinfrared%20small%20object%20detection&amp;journal=arxiv&amp;volume=arxiv&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b37\" id=\"b37\">\u003c/a> yang, c., deng, y., wang, f., yang, h., xu, x., zeng, q., et al. (2021). brown leaf spot on \u003ci>juglans sigillata\u003c/i> caused by \u003ci>ophiognomonia leptostyla\u003c/i> in sichuan, china. \u003ci>plant disease\u003c/i>, 105, 4160. doi:&#xa0;10.1094/pdis-02-21-0344-pdn\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/pdis-02-21-0344-pdn\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=c.+yang&amp;author=y.+deng&amp;author=f.+wang&amp;author=h.+yang&amp;author=x.+xu&amp;author=q.+zeng&amp;publication_year=2021&amp;title=brown%20leaf%20spot%20on%20juglans%20sigillata%20caused%20by%20ophiognomonia%20leptostyla%20in%20sichuan%2c%20china&amp;journal=plant+disease&amp;volume=105&amp;pages=4160\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b38\" id=\"b38\">\u003c/a> yang, q., duan, s., and wang, l. (2022). efficient identification of apple leaf diseases in the wild using convolutional neural networks. \u003ci>agronomy\u003c/i>, 12, 2784. doi:&#xa0;10.3390/agronomy12112784\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agronomy12112784\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=q.+yang&amp;author=s.+duan&amp;author=l.+wang&amp;publication_year=2022&amp;title=efficient%20identification%20of%20apple%20leaf%20diseases%20in%20the%20wild%20using%20convolutional%20neural%20networks&amp;journal=agronomy&amp;volume=12&amp;pages=2784\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b39\" id=\"b39\">\u003c/a> zarei, s., taghavi, s. m., banihashemi, z., hamzehzarghani, h., and osdaghi, e. (2019). etiology of leaf spot and fruit canker symptoms on stone fruits and nut trees in iran. \u003ci>journal of plant pathology\u003c/i>, 101, 1133&#x2013;1142. doi:&#xa0;10.1007/s42161-019-00283-w\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1007/s42161-019-00283-w\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+zarei&amp;author=s.%20m.+taghavi&amp;author=z.+banihashemi&amp;author=h.+hamzehzarghani&amp;author=e.+osdaghi&amp;publication_year=2019&amp;title=etiology%20of%20leaf%20spot%20and%20fruit%20canker%20symptoms%20on%20stone%20fruits%20and%20nut%20trees%20in%20iran&amp;journal=journal+of+plant+pathology&amp;volume=101&amp;pages=1133\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b40\" id=\"b40\">\u003c/a> zhang, y., li, x., &#x160;im&#x16f;nek, j., shi, h., chen, n., and hu, q. (2023). quantifying water and salt movement in a soil-plant system of a corn field using hydrus (2d/3d) and the stable isotope method. \u003ci>agricultural water management\u003c/i>, 288, 108492. doi:&#xa0;10.1016/j.agwat.2023.108492\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.agwat.2023.108492\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=y.+zhang&amp;author=x.+li&amp;author=j.+%c5%a0im%c5%afnek&amp;author=h.+shi&amp;author=n.+chen&amp;author=q.+hu&amp;publication_year=2023&amp;title=quantifying%20water%20and%20salt%20movement%20in%20a%20soil-plant%20system%20of%20a%20corn%20field%20using%20hydrus%20%282d%2f3d%29%20and%20the%20stable%20isotope%20method&amp;journal=agricultural+water+management&amp;volume=288&amp;pages=108492\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003c/div>\u003cdiv class=\"thinlinem20\">\u003c/div>\u003cdiv class=\"abstractsummary\">\u003cp>\u003cspan>keywords:\u003c/span> walnut, brown spot disease (\u003ci>ophiognomonia leptostyla\u003c/i>), hierarchical feature selection, edge features perception, adaptive multi-scale dilated convolution, disease grading\u003c/p>\u003cp>\u003cspan>citation:\u003c/span> wei y, zeng d and zheng l (2025) a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution. \u003ci>front. plant sci.\u003c/i> 16:1641677. doi: 10.3389/fpls.2025.1641677\u003c/p>\u003cp id=\"timestamps\">\u003cspan>received:\u003c/span> 05 june 2025; \u003cspan>accepted:\u003c/span> 09 september 2025;\u003cbr>\u003cspan>published:\u003c/span> 03 october 2025.\u003c/p>\u003cdiv>\u003cp>edited by:\u003c/p>\u003ca href=\"https://loop.frontiersin.org/people/1037951\">ravinder kumar\u003c/a>, indian agricultural research institute (icar), india\u003c/div>\u003cdiv>\u003cp>reviewed by:\u003c/p>\u003ca href=\"https://loop.frontiersin.org/people/2082015\">geza bujdoso\u003c/a>, hungarian university of agricultural and life sciences, hungary\u003cbr>\r\n\u003ca href=\"https://loop.frontiersin.org/people/3078480\">vasudha vedula\u003c/a>, university of texas of the permian basin, united states\u003c/div>\u003cp>\u003cspan>copyright\u003c/span> &#xa9; 2025 wei, zeng and zheng. this is an open-access article distributed under the terms of the \u003ca rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\" target=\"_blank\">creative commons attribution license (cc by)\u003c/a>. the use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. no use, distribution or reproduction is permitted which does not comply with these terms.\u003c/p>\u003cp>\u003cspan>*correspondence:\u003c/span> liangfang zheng, \u003ca id=\"encmail\">mtgxnjaynta0othamtyzlmnvbq==\u003c/a>\u003c/p>\u003cp>\u003cspan>\u003csup>&#x2020;\u003c/sup>\u003c/span>these authors have contributed equally to this work\u003c/p>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>","\u003cul class=\"flyoutjournal\">\u003cli>\u003ca href=\"#h1\">abstract\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h2\">1 introduction\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h3\">2 materials and methods\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h4\">3 experiments and results analysis\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h5\">4 conclusion\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h6\">5 discussion and future work\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h7\">data availability statement\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h8\">author contributions\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h9\">funding\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h10\">conflict of interest\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h11\">generative ai statement\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h12\">publisher&#x2019;s note\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h13\">references\u003c/a>\u003c/li>\u003c/ul>",[629,636,642],{"name":630,"fileserverpackageentryid":19,"fileserverid":631,"fileserverversionnumber":632,"type":633},"epub.epub","1641677/epub",2,{"code":634,"name":635},"epub","epub",{"name":637,"fileserverpackageentryid":637,"fileserverid":638,"fileserverversionnumber":632,"type":639},"fpls-16-1641677.xml","1641677/xml",{"code":640,"name":641},"nlm_xml","xml",{"name":643,"fileserverpackageentryid":19,"fileserverid":644,"fileserverversionnumber":632,"type":645},"publishers-proof.pdf","1641677/publishers-proof",{"code":646,"name":647},"pdf","pdf","v3",{"title":650,"link":651,"meta":655,"script":748},"frontiers | a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution",[652],{"rel":653,"href":654},"canonical","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/full",[656,659,662,664,667,671,673,677,680,683,686,688,690,692,694,696,699,702,704,707,709,711,714,717,720,723,726,730,734,737,740,743,745],{"hid":657,"property":657,"name":657,"content":658},"description","walnut leaf brown spot disease, caused by ophiognomonia leptostyla, is among the most destructive fungal diseases in walnut cultivation. in the development o...",{"hid":660,"property":660,"name":661,"content":650},"og:title","title",{"hid":663,"property":663,"name":657,"content":658},"og:description",{"hid":665,"name":665,"content":666},"keywords","walnut,brown spot disease (ophiognomonia leptostyla),hierarchical feature selection,edge features perception,adaptive multi-scale dilated convolution,disease grading",{"hid":668,"property":668,"name":669,"content":670},"og:site_name","site_name","frontiers",{"hid":672,"property":672,"name":385,"content":404},"og:image",{"hid":674,"property":674,"name":675,"content":676},"og:type","type","article",{"hid":678,"property":678,"name":679,"content":654},"og:url","url",{"hid":681,"name":681,"content":682},"twitter:card","summary_large_image",{"hid":684,"name":684,"content":685},"citation_volume","16",{"hid":687,"name":687,"content":116},"citation_journal_title",{"hid":689,"name":689,"content":670},"citation_publisher",{"hid":691,"name":691,"content":611},"citation_journal_abbrev",{"hid":693,"name":693,"content":612},"citation_issn",{"hid":695,"name":695,"content":538},"citation_doi",{"hid":697,"name":697,"content":698},"citation_firstpage","1641677",{"hid":700,"name":700,"content":701},"citation_language","english",{"hid":703,"name":703,"content":539},"citation_title",{"hid":705,"name":705,"content":706},"citation_keywords","walnut; brown spot disease (ophiognomonia leptostyla); hierarchical feature selection; edge features perception; adaptive multi-scale dilated convolution; disease grading",{"hid":708,"name":708,"content":544},"citation_abstract",{"hid":710,"name":710,"content":553},"citation_article_type",{"hid":712,"name":712,"content":713},"citation_pdf_url","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/pdf",{"hid":715,"name":715,"content":716},"citation_xml_url","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/xml",{"hid":718,"name":718,"content":719},"citation_fulltext_world_readable","yes",{"hid":721,"name":721,"content":722},"citation_online_date","2025/09/09",{"hid":724,"name":724,"content":725},"citation_publication_date","2025/10/03",{"hid":727,"name":728,"content":729},"citation_author_0","citation_author","wei, yuting ",{"hid":731,"name":732,"content":733},"citation_author_institution_0","citation_author_institution","college of information engineering, tarim university, china",{"hid":735,"name":728,"content":736},"citation_author_1","zeng, debin ",{"hid":738,"name":732,"content":739},"citation_author_institution_1","key laboratory of tarim oasis agriculture, ministry of education, tarim university, china",{"hid":741,"name":728,"content":742},"citation_author_2","zheng, liangfang ",{"hid":744,"name":732,"content":739},"citation_author_institution_2",{"hid":746,"name":746,"content":747},"dc.identifier","doi:10.3389/fpls.2025.1641677",[749,752,754,756,758],{"src":750,"body":13,"type":751,"async":13},"https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6","text/javascript",{"src":753,"body":13,"type":751,"async":13},"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/mathjax.js?config=tex-mml-am_chtml",{"src":755,"body":13,"type":751,"async":13},"https://d1bxh8uas1mnw7.cloudfront.net/assets/altmetric_badges-f0bc9b243ff5677d05460c1eb71834ca998946d764eb3bc244ab4b18ba50d21e.js",{"src":757,"body":13,"type":751,"async":13},"https://api.altmetric.com/v1/doi/10.3389/fpls.2025.1641677?callback=_altmetric.embed_callback&domain=www.frontiersin.org&key=3c130976ca2b8f2e88f8377633751ba1&cache_until=14-15",{"src":759,"body":13,"type":751,"async":13},"https://crossmark-cdn.crossref.org/widget/v2.0/widget.js",{"articlehubslug":19,"articlehubpage":761,"articlehubarticleslist":762,"canjournalhasarticlehub":367,"articledoilist":763},{},[],[],{"title":19,"image":-1,"breadcrumbs":765,"linkscollection":766,"metricscollection":768},[],{"total":389,"items":767},[],{"total":389,"items":769},[]] window.__nuxt__={};window.__nuxt__.config={public:{isdevmode:false,appname:"article-pages-2024",baseurl:"https://www.frontiersin.org",domain:"frontiersin.org",loopurl:"https://loop.frontiersin.org",ssmaindomain:"frontiersin.org",contentfulurl:"https://graphql.contentful.com/content/v1/spaces",spaceid:1,spacename:"frontiers",livespaceid:1,tenantlogourl:"",frontiersgraphurl:"https://apollo-federation-gateway.frontiersin.org",registrationapiurl:"https://onboarding-ui.frontiersin.org",emaildigestapiurl:"https://api-email-digest.frontiersin.org",journalfilesapiurl:"https://files-journal-api.frontiersin.org",searchapiurl:"https://search-api.frontiersin.org",productionforumapiurl:"https://production-forum-api-v2.frontiersin.org",gtmserverurl:"https://tag-manager.frontiersin.org",gtmid:"gtm-m322fv2",gtmauth:"owvbwxfajr21yqv1fe1caq",gtmpreview:"env-1",figsharepluginurl:"https://widgets.figshare.com/static/figshare.js",figshareapiurl:"https://api.figshare.com/v2/collections/search",figsharetimeout:3000,crossmarkpublisheddate:"2015/08/24",googlerecaptchasitekey:"6ldg3i0uaaaaaoc4quh35ubhgjotehp_stxhgr_v",googlerecaptchakeyname:"frontiersrecaptchav2",faviconsize16:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_16-tenantfavicon-frontiers.png",faviconsize32:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_32-tenantfavicon-frontiers.png",faviconsize180:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_180-tenantfavicon-frontiers.png",faviconsize192:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_192-tenantfavicon-frontiers.png",faviconsize512:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_512-tenantfavicon-frontiers.png",socialmediaimg:"https://brand.frontiersin.org/m/1c8bcb536c789e11/guidelines-frontiers_logo_1200x628_1-91to1.png",linkedarticlecopytext:"'{\"articletypecopytext\":[{\"articletypeid\":0,\"originalarticlecopytext\":\"part of this article's content has been mentioned in:\",\"linkedarticlecopytext\":\"this article mentions parts of:\"},{\"articletypeid\":122,\"originalarticlecopytext\":\"parts of this article's content have been modified or rectified in:\",\"linkedarticlecopytext\":\"this article is an erratum on:\"},{\"articletypeid\":129,\"originalarticlecopytext\":\"parts of this article's content have been modified or rectified in:\",\"linkedarticlecopytext\":\"this article is an addendum to:\"},{\"articletypeid\":128,\"originalarticlecopytext\":\"a correction has been applied to this article in:\",\"linkedarticlecopytext\":\"this article is a correction to:\"},{\"articletypeid\":134,\"originalarticlecopytext\":\"a retraction of this article was approved in:\",\"linkedarticlecopytext\":\"this article is a retraction of:\"},{\"articletypeid\":29,\"originalarticlecopytext\":\"a commentary has been posted on this article:\",\"linkedarticlecopytext\":\"this article is a commentary on:\"},{\"articletypeid\":30,\"originalarticlecopytext\":\"a commentary has been posted on this article:\",\"linkedarticlecopytext\":\"this article is a commentary on:\"}],\"articleidcopytext\":[]}'\n",acceptedarticleexcludedjournalids:"'[2766,979]'\n",articletypeconfigurablelabel:"\u003c\u003carticle-type:uppercase>> article",settingsfeaturesswitchers:"'{\"displaytitlepilllabels\":true,\"displayrelatedarticlesbox\":true,\"showeditors\":true,\"showreviewers\":true,\"showloopimpactlink\":true,\"enablefigshare\":false,\"usexmlimages\":true}'\n",journalswitharticlehub:"'{\"strings\":[\"science\"]}'\n",terminologysettings:"'{\"terms\":[{\"sequencenumber\":1,\"key\":\"frontiers\",\"tenantterm\":\"frontiers\",\"frontiersdefaultterm\":\"frontiers\",\"category\":\"customer\"},{\"sequencenumber\":2,\"key\":\"submission_system\",\"tenantterm\":\"submission system\",\"frontiersdefaultterm\":\"submission system\",\"category\":\"product\"},{\"sequencenumber\":3,\"key\":\"public_pages\",\"tenantterm\":\"public pages\",\"frontiersdefaultterm\":\"public pages\",\"category\":\"product\"},{\"sequencenumber\":4,\"key\":\"my_frontiers\",\"tenantterm\":\"my frontiers\",\"frontiersdefaultterm\":\"my frontiers\",\"category\":\"product\"},{\"sequencenumber\":5,\"key\":\"digital_editorial_office\",\"tenantterm\":\"digital editorial office\",\"frontiersdefaultterm\":\"digital editorial office\",\"category\":\"product\"},{\"sequencenumber\":6,\"key\":\"deo\",\"tenantterm\":\"deo\",\"frontiersdefaultterm\":\"deo\",\"category\":\"product\"},{\"sequencenumber\":7,\"key\":\"digital_editorial_office_for_chiefs\",\"tenantterm\":\"digital editorial office for chiefs\",\"frontiersdefaultterm\":\"digital editorial office for chiefs\",\"category\":\"product\"},{\"sequencenumber\":8,\"key\":\"digital_editorial_office_for_eof\",\"tenantterm\":\"digital editorial office for eof\",\"frontiersdefaultterm\":\"digital editorial office for eof\",\"category\":\"product\"},{\"sequencenumber\":9,\"key\":\"editorial_office\",\"tenantterm\":\"editorial office\",\"frontiersdefaultterm\":\"editorial office\",\"category\":\"product\"},{\"sequencenumber\":10,\"key\":\"eof\",\"tenantterm\":\"eof\",\"frontiersdefaultterm\":\"eof\",\"category\":\"product\"},{\"sequencenumber\":11,\"key\":\"research_topic_management\",\"tenantterm\":\"research topic management\",\"frontiersdefaultterm\":\"research topic management\",\"category\":\"product\"},{\"sequencenumber\":12,\"key\":\"review_forum\",\"tenantterm\":\"review forum\",\"frontiersdefaultterm\":\"review forum\",\"category\":\"product\"},{\"sequencenumber\":13,\"key\":\"accounting_office\",\"tenantterm\":\"accounting office\",\"frontiersdefaultterm\":\"accounting office\",\"category\":\"product\"},{\"sequencenumber\":14,\"key\":\"aof\",\"tenantterm\":\"aof\",\"frontiersdefaultterm\":\"aof\",\"category\":\"product\"},{\"sequencenumber\":15,\"key\":\"publishing_office\",\"tenantterm\":\"publishing office\",\"frontiersdefaultterm\":\"publishing office\",\"category\":\"product\"},{\"sequencenumber\":16,\"key\":\"production_office\",\"tenantterm\":\"production office forum\",\"frontiersdefaultterm\":\"production office forum\",\"category\":\"product\"},{\"sequencenumber\":17,\"key\":\"pof\",\"tenantterm\":\"pof\",\"frontiersdefaultterm\":\"pof\",\"category\":\"product\"},{\"sequencenumber\":18,\"key\":\"book_office_forum\",\"tenantterm\":\"book office forum\",\"frontiersdefaultterm\":\"book office forum\",\"category\":\"product\"},{\"sequencenumber\":19,\"key\":\"bof\",\"tenantterm\":\"bof\",\"frontiersdefaultterm\":\"bof\",\"category\":\"product\"},{\"sequencenumber\":20,\"key\":\"aira\",\"tenantterm\":\"aira\",\"frontiersdefaultterm\":\"aira\",\"category\":\"product\"},{\"sequencenumber\":21,\"key\":\"editorial_board_management\",\"tenantterm\":\"editorial board management\",\"frontiersdefaultterm\":\"editorial board management\",\"category\":\"product\"},{\"sequencenumber\":22,\"key\":\"ebm\",\"tenantterm\":\"ebm\",\"frontiersdefaultterm\":\"ebm\",\"category\":\"product\"},{\"sequencenumber\":23,\"key\":\"domain\",\"tenantterm\":\"domain\",\"frontiersdefaultterm\":\"domain\",\"category\":\"taxonomy\"},{\"sequencenumber\":24,\"key\":\"journal\",\"tenantterm\":\"journal\",\"frontiersdefaultterm\":\"journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":25,\"key\":\"section\",\"tenantterm\":\"section\",\"frontiersdefaultterm\":\"section\",\"category\":\"taxonomy\"},{\"sequencenumber\":26,\"key\":\"domains\",\"tenantterm\":\"domains\",\"frontiersdefaultterm\":\"domains\",\"category\":\"taxonomy\"},{\"sequencenumber\":27,\"key\":\"specialty_section\",\"tenantterm\":\"specialty section\",\"frontiersdefaultterm\":\"specialty section\",\"category\":\"taxonomy\"},{\"sequencenumber\":28,\"key\":\"specialty_journal\",\"tenantterm\":\"specialty journal\",\"frontiersdefaultterm\":\"specialty journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":29,\"key\":\"journals\",\"tenantterm\":\"journals\",\"frontiersdefaultterm\":\"journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":30,\"key\":\"sections\",\"tenantterm\":\"sections\",\"frontiersdefaultterm\":\"sections\",\"category\":\"taxonomy\"},{\"sequencenumber\":31,\"key\":\"specialty_sections\",\"tenantterm\":\"specialty sections\",\"frontiersdefaultterm\":\"specialty sections\",\"category\":\"taxonomy\"},{\"sequencenumber\":32,\"key\":\"specialty_journals\",\"tenantterm\":\"specialty journals\",\"frontiersdefaultterm\":\"specialty journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":33,\"key\":\"manuscript\",\"tenantterm\":\"manuscript\",\"frontiersdefaultterm\":\"manuscript\",\"category\":\"core\"},{\"sequencenumber\":34,\"key\":\"manuscripts\",\"tenantterm\":\"manuscripts\",\"frontiersdefaultterm\":\"manuscripts\",\"category\":\"core\"},{\"sequencenumber\":35,\"key\":\"article\",\"tenantterm\":\"article\",\"frontiersdefaultterm\":\"article\",\"category\":\"core\"},{\"sequencenumber\":36,\"key\":\"articles\",\"tenantterm\":\"articles\",\"frontiersdefaultterm\":\"articles\",\"category\":\"core\"},{\"sequencenumber\":37,\"key\":\"article_type\",\"tenantterm\":\"article type\",\"frontiersdefaultterm\":\"article type\",\"category\":\"core\"},{\"sequencenumber\":38,\"key\":\"article_types\",\"tenantterm\":\"article types\",\"frontiersdefaultterm\":\"article types\",\"category\":\"core\"},{\"sequencenumber\":39,\"key\":\"author\",\"tenantterm\":\"author\",\"frontiersdefaultterm\":\"author\",\"category\":\"label (role)\"},{\"sequencenumber\":40,\"key\":\"authors\",\"tenantterm\":\"authors\",\"frontiersdefaultterm\":\"authors\",\"category\":\"label (role)\"},{\"sequencenumber\":41,\"key\":\"authoring\",\"tenantterm\":\"authoring\",\"frontiersdefaultterm\":\"authoring\",\"category\":\"core\"},{\"sequencenumber\":42,\"key\":\"authored\",\"tenantterm\":\"authored\",\"frontiersdefaultterm\":\"authored\",\"category\":\"core\"},{\"sequencenumber\":43,\"key\":\"accept\",\"tenantterm\":\"accept\",\"frontiersdefaultterm\":\"accept\",\"category\":\"process\"},{\"sequencenumber\":44,\"key\":\"accepted\",\"tenantterm\":\"accepted\",\"frontiersdefaultterm\":\"accepted\",\"category\":\"process\"},{\"sequencenumber\":45,\"key\":\"assistant_field_chief_editor\",\"tenantterm\":\"assistant field chief editor\",\"frontiersdefaultterm\":\"assistant field chief editor\",\"description\":\"an editorial role on a field journal that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":46,\"key\":\"assistant_specialty_chief_editor\",\"tenantterm\":\"assistant specialty chief editor\",\"frontiersdefaultterm\":\"assistant specialty chief editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":47,\"key\":\"assistant_specialty_chief_editors\",\"tenantterm\":\"assistant specialty chief editors\",\"frontiersdefaultterm\":\"assistant specialty chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":48,\"key\":\"associate_editor\",\"tenantterm\":\"associate editor\",\"frontiersdefaultterm\":\"associate editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":49,\"key\":\"specialty_chief_editor\",\"tenantterm\":\"specialty chief editor\",\"frontiersdefaultterm\":\"specialty chief editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":50,\"key\":\"specialty_chief_editors\",\"tenantterm\":\"specialty chief editors\",\"frontiersdefaultterm\":\"specialty chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":51,\"key\":\"chief_editor\",\"tenantterm\":\"chief editor\",\"frontiersdefaultterm\":\"chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":52,\"key\":\"chief_editors\",\"tenantterm\":\"chief editors\",\"frontiersdefaultterm\":\"chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":53,\"key\":\"call_for_participation\",\"tenantterm\":\"call for participation\",\"frontiersdefaultterm\":\"call for participation\",\"category\":\"process\"},{\"sequencenumber\":54,\"key\":\"citation\",\"tenantterm\":\"citation\",\"frontiersdefaultterm\":\"citation\",\"category\":\"misc.\"},{\"sequencenumber\":55,\"key\":\"citations\",\"tenantterm\":\"citations\",\"frontiersdefaultterm\":\"citations\",\"category\":\"misc.\"},{\"sequencenumber\":56,\"key\":\"contributor\",\"tenantterm\":\"contributor\",\"frontiersdefaultterm\":\"contributor\",\"category\":\"label (role)\"},{\"sequencenumber\":57,\"key\":\"contributors\",\"tenantterm\":\"contributors\",\"frontiersdefaultterm\":\"contributors\",\"category\":\"label (role)\"},{\"sequencenumber\":58,\"key\":\"corresponding_author\",\"tenantterm\":\"corresponding author\",\"frontiersdefaultterm\":\"corresponding author\",\"category\":\"label (role)\"},{\"sequencenumber\":59,\"key\":\"corresponding_authors\",\"tenantterm\":\"corresponding authors\",\"frontiersdefaultterm\":\"corresponding authors\",\"category\":\"label (role)\"},{\"sequencenumber\":60,\"key\":\"decline\",\"tenantterm\":\"decline\",\"frontiersdefaultterm\":\"decline\",\"category\":\"process\"},{\"sequencenumber\":61,\"key\":\"declined\",\"tenantterm\":\"declined\",\"frontiersdefaultterm\":\"declined\",\"category\":\"process\"},{\"sequencenumber\":62,\"key\":\"reject\",\"tenantterm\":\"reject\",\"frontiersdefaultterm\":\"reject\",\"category\":\"process\"},{\"sequencenumber\":63,\"key\":\"rejected\",\"tenantterm\":\"rejected\",\"frontiersdefaultterm\":\"rejected\",\"category\":\"process\"},{\"sequencenumber\":64,\"key\":\"publish\",\"tenantterm\":\"publish\",\"frontiersdefaultterm\":\"publish\",\"category\":\"core\"},{\"sequencenumber\":65,\"key\":\"published\",\"tenantterm\":\"published\",\"frontiersdefaultterm\":\"published\",\"category\":\"core\"},{\"sequencenumber\":66,\"key\":\"publication\",\"tenantterm\":\"publication\",\"frontiersdefaultterm\":\"publication\",\"category\":\"core\"},{\"sequencenumber\":67,\"key\":\"peer_review\",\"tenantterm\":\"peer review\",\"frontiersdefaultterm\":\"peer review\",\"category\":\"peer review process\"},{\"sequencenumber\":68,\"key\":\"peer_reviewed\",\"tenantterm\":\"peer reviewed\",\"frontiersdefaultterm\":\"peer reviewed\",\"category\":\"peer review process\"},{\"sequencenumber\":69,\"key\":\"initial_validation\",\"tenantterm\":\"initial validation\",\"frontiersdefaultterm\":\"initial validation\",\"category\":\"peer review process\"},{\"sequencenumber\":70,\"key\":\"editorial_assignment\",\"tenantterm\":\"editorial assignment\",\"frontiersdefaultterm\":\"editorial assignment\",\"category\":\"peer review process\"},{\"sequencenumber\":71,\"key\":\"independent_review\",\"tenantterm\":\"independent review\",\"frontiersdefaultterm\":\"independent review\",\"category\":\"peer review process\"},{\"sequencenumber\":72,\"key\":\"interactive_review\",\"tenantterm\":\"interactive review\",\"frontiersdefaultterm\":\"interactive review\",\"category\":\"peer review process\"},{\"sequencenumber\":73,\"key\":\"review\",\"tenantterm\":\"review\",\"frontiersdefaultterm\":\"review\",\"category\":\"peer review process\"},{\"sequencenumber\":74,\"key\":\"reviewing\",\"tenantterm\":\"reviewing\",\"frontiersdefaultterm\":\"reviewing\",\"category\":\"peer review process\"},{\"sequencenumber\":75,\"key\":\"reviewer\",\"tenantterm\":\"reviewer\",\"frontiersdefaultterm\":\"reviewer\",\"category\":\"label (role)\"},{\"sequencenumber\":76,\"key\":\"reviewers\",\"tenantterm\":\"reviewers\",\"frontiersdefaultterm\":\"reviewers\",\"category\":\"label (role)\"},{\"sequencenumber\":77,\"key\":\"review_finalized\",\"tenantterm\":\"review finalized\",\"frontiersdefaultterm\":\"review finalized\",\"category\":\"peer review process\"},{\"sequencenumber\":78,\"key\":\"final_decision\",\"tenantterm\":\"final decision\",\"frontiersdefaultterm\":\"final decision\",\"category\":\"peer review process\"},{\"sequencenumber\":79,\"key\":\"final_validation\",\"tenantterm\":\"final validation\",\"frontiersdefaultterm\":\"final validation\",\"category\":\"peer review process\"},{\"sequencenumber\":80,\"key\":\"ae_accept_manuscript\",\"tenantterm\":\"recommend to accept manuscript\",\"frontiersdefaultterm\":\"accept manuscript\",\"category\":\"process\"},{\"sequencenumber\":81,\"key\":\"fee\",\"tenantterm\":\"fee\",\"frontiersdefaultterm\":\"fee\",\"category\":\"accounting\"},{\"sequencenumber\":82,\"key\":\"fees\",\"tenantterm\":\"fees\",\"frontiersdefaultterm\":\"fees\",\"category\":\"accounting\"},{\"sequencenumber\":83,\"key\":\"guest_associate_editor\",\"tenantterm\":\"guest associate editor\",\"frontiersdefaultterm\":\"guest associate editor\",\"category\":\"label (role)\"},{\"sequencenumber\":84,\"key\":\"guest_associate_editors\",\"tenantterm\":\"guest associate editors\",\"frontiersdefaultterm\":\"guest associate editors\",\"category\":\"label (role)\"},{\"sequencenumber\":85,\"key\":\"in_review\",\"tenantterm\":\"in review\",\"frontiersdefaultterm\":\"in review\",\"category\":\"peer review process\"},{\"sequencenumber\":86,\"key\":\"institutional_member\",\"tenantterm\":\"institutional partner\",\"frontiersdefaultterm\":\"institutional partner\",\"category\":\"accounting\"},{\"sequencenumber\":87,\"key\":\"institutional_membership\",\"tenantterm\":\"institutional partnership\",\"frontiersdefaultterm\":\"institutional partnership\",\"category\":\"accounting\"},{\"sequencenumber\":88,\"key\":\"article_processing_charge\",\"tenantterm\":\"article processing charge\",\"frontiersdefaultterm\":\"article processing charge\",\"category\":\"accounting\"},{\"sequencenumber\":89,\"key\":\"article_processing_charges\",\"tenantterm\":\"article processing charges\",\"frontiersdefaultterm\":\"article processing charges\",\"category\":\"accounting\"},{\"sequencenumber\":90,\"key\":\"apcs\",\"tenantterm\":\"apcs\",\"frontiersdefaultterm\":\"apcs\",\"category\":\"accounting\"},{\"sequencenumber\":91,\"key\":\"apc\",\"tenantterm\":\"apc\",\"frontiersdefaultterm\":\"apc\",\"category\":\"accounting\"},{\"sequencenumber\":92,\"key\":\"received\",\"tenantterm\":\"received\",\"frontiersdefaultterm\":\"received\",\"description\":\"date manuscript was received on.\",\"category\":\"core\"},{\"sequencenumber\":93,\"key\":\"transferred\",\"tenantterm\":\"transferred\",\"frontiersdefaultterm\":\"transferred\",\"category\":\"core\"},{\"sequencenumber\":94,\"key\":\"transfer\",\"tenantterm\":\"transfer\",\"frontiersdefaultterm\":\"transfer\",\"category\":\"core\"},{\"sequencenumber\":95,\"key\":\"research_topic\",\"tenantterm\":\"research topic\",\"frontiersdefaultterm\":\"research topic\",\"category\":\"core\"},{\"sequencenumber\":96,\"key\":\"research_topics\",\"tenantterm\":\"research topics\",\"frontiersdefaultterm\":\"research topics\",\"category\":\"core\"},{\"sequencenumber\":97,\"key\":\"topic_editor\",\"tenantterm\":\"topic editor\",\"frontiersdefaultterm\":\"topic editor\",\"category\":\"label (role)\"},{\"sequencenumber\":98,\"key\":\"review_editor\",\"tenantterm\":\"community reviewer\",\"frontiersdefaultterm\":\"review editor\",\"category\":\"label (role)\"},{\"sequencenumber\":99,\"key\":\"title\",\"tenantterm\":\"title\",\"frontiersdefaultterm\":\"title\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":100,\"key\":\"running_title\",\"tenantterm\":\"running title\",\"frontiersdefaultterm\":\"running title\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":101,\"key\":\"submit\",\"tenantterm\":\"submit\",\"frontiersdefaultterm\":\"submit\",\"category\":\"process\"},{\"sequencenumber\":102,\"key\":\"submitted\",\"tenantterm\":\"submitted\",\"frontiersdefaultterm\":\"submitted\",\"category\":\"process\"},{\"sequencenumber\":103,\"key\":\"submitting\",\"tenantterm\":\"submitting\",\"frontiersdefaultterm\":\"submitting\",\"category\":\"process\"},{\"sequencenumber\":104,\"key\":\"t_e\",\"tenantterm\":\"te\",\"frontiersdefaultterm\":\"te\",\"category\":\"label (role)\"},{\"sequencenumber\":105,\"key\":\"topic\",\"tenantterm\":\"topic\",\"frontiersdefaultterm\":\"topic\",\"category\":\"process\"},{\"sequencenumber\":106,\"key\":\"topic_summary\",\"tenantterm\":\"topic summary\",\"frontiersdefaultterm\":\"topic summary\",\"category\":\"process\"},{\"sequencenumber\":107,\"key\":\"figure\",\"tenantterm\":\"figure\",\"frontiersdefaultterm\":\"figure\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":108,\"key\":\"figures\",\"tenantterm\":\"figures\",\"frontiersdefaultterm\":\"figures\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":109,\"key\":\"editorial_file\",\"tenantterm\":\"editorial file\",\"frontiersdefaultterm\":\"editorial file\",\"category\":\"core\"},{\"sequencenumber\":110,\"key\":\"editorial_files\",\"tenantterm\":\"editorial files\",\"frontiersdefaultterm\":\"editorial files\",\"category\":\"core\"},{\"sequencenumber\":111,\"key\":\"e_book\",\"tenantterm\":\"e-book\",\"frontiersdefaultterm\":\"e-book\",\"category\":\"core\"},{\"sequencenumber\":112,\"key\":\"organization\",\"tenantterm\":\"organization\",\"frontiersdefaultterm\":\"organization\",\"category\":\"core\"},{\"sequencenumber\":113,\"key\":\"institution\",\"tenantterm\":\"institution\",\"frontiersdefaultterm\":\"institution\",\"category\":\"core\"},{\"sequencenumber\":114,\"key\":\"reference\",\"tenantterm\":\"reference\",\"frontiersdefaultterm\":\"reference\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":115,\"key\":\"references\",\"tenantterm\":\"references\",\"frontiersdefaultterm\":\"references\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":116,\"key\":\"sce\",\"tenantterm\":\"sce\",\"frontiersdefaultterm\":\"sce\",\"description\":\"abbreviation for specialty chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":117,\"key\":\"submission\",\"tenantterm\":\"submission\",\"frontiersdefaultterm\":\"submission\",\"category\":\"process\"},{\"sequencenumber\":118,\"key\":\"submissions\",\"tenantterm\":\"submissions\",\"frontiersdefaultterm\":\"submissions\",\"category\":\"process\"},{\"sequencenumber\":119,\"key\":\"editing\",\"tenantterm\":\"editing\",\"frontiersdefaultterm\":\"editing\",\"category\":\"process\"},{\"sequencenumber\":120,\"key\":\"in_preparation\",\"tenantterm\":\"in preparation\",\"frontiersdefaultterm\":\"in preparation\",\"category\":\"process\"},{\"sequencenumber\":121,\"key\":\"country_region\",\"tenantterm\":\"country/region\",\"frontiersdefaultterm\":\"country/region\",\"description\":\"because of political issues, some of the country listings are actually classified as `regions` and we need to include this. however other clients may not want to do this.\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":122,\"key\":\"countries_regions\",\"tenantterm\":\"countries/regions\",\"frontiersdefaultterm\":\"countries/regions\",\"description\":\"because of political issues, some of the country listings are actually classified as `regions` and we need to include this. however other clients may not want to do this.\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":123,\"key\":\"specialty\",\"tenantterm\":\"specialty\",\"frontiersdefaultterm\":\"specialty\",\"category\":\"core\"},{\"sequencenumber\":124,\"key\":\"specialties\",\"tenantterm\":\"specialties\",\"frontiersdefaultterm\":\"specialties\",\"category\":\"core\"},{\"sequencenumber\":125,\"key\":\"associate_editors\",\"tenantterm\":\"associate editors\",\"frontiersdefaultterm\":\"associate editors\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":126,\"key\":\"reviewed\",\"tenantterm\":\"reviewed\",\"frontiersdefaultterm\":\"reviewed\",\"category\":\"peer review process\"},{\"sequencenumber\":127,\"key\":\"institutional_members\",\"tenantterm\":\"institutional partners\",\"frontiersdefaultterm\":\"institutional partners\",\"category\":\"accounting\"},{\"sequencenumber\":128,\"key\":\"institutional_memberships\",\"tenantterm\":\"institutional partnerships\",\"frontiersdefaultterm\":\"institutional partnerships\",\"category\":\"accounting\"},{\"sequencenumber\":129,\"key\":\"assistant_field_chief_editors\",\"tenantterm\":\"assistant field chief editors\",\"frontiersdefaultterm\":\"assistant field chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":130,\"key\":\"publications\",\"tenantterm\":\"publications\",\"frontiersdefaultterm\":\"publications\",\"category\":\"process\"},{\"sequencenumber\":131,\"key\":\"ae_accepted\",\"tenantterm\":\"recommended acceptance\",\"frontiersdefaultterm\":\"accepted\",\"category\":\"process\"},{\"sequencenumber\":132,\"key\":\"field_journal\",\"tenantterm\":\"field journal\",\"frontiersdefaultterm\":\"field journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":133,\"key\":\"field_journals\",\"tenantterm\":\"field journals\",\"frontiersdefaultterm\":\"field journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":134,\"key\":\"program_manager\",\"tenantterm\":\"program manager\",\"frontiersdefaultterm\":\"program manager\",\"category\":\"label (role)\"},{\"sequencenumber\":135,\"key\":\"journal_manager\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"journal manager\",\"category\":\"label (role)\"},{\"sequencenumber\":136,\"key\":\"journal_specialist\",\"tenantterm\":\"journal specialist\",\"frontiersdefaultterm\":\"journal specialist\",\"category\":\"label (role)\"},{\"sequencenumber\":137,\"key\":\"program_managers\",\"tenantterm\":\"program managers\",\"frontiersdefaultterm\":\"program managers\",\"category\":\"label (role)\"},{\"sequencenumber\":138,\"key\":\"journal_managers\",\"tenantterm\":\"journal managers\",\"frontiersdefaultterm\":\"journal managers\",\"category\":\"label (role)\"},{\"sequencenumber\":139,\"key\":\"journal_specialists\",\"tenantterm\":\"journal specialists\",\"frontiersdefaultterm\":\"journal specialists\",\"category\":\"label (role)\"},{\"sequencenumber\":140,\"key\":\"cover_letter\",\"tenantterm\":\"manuscript contribution to the field\",\"frontiersdefaultterm\":\"manuscript contribution to the field\",\"category\":\"process\"},{\"sequencenumber\":141,\"key\":\"ae_accepted_manuscript\",\"tenantterm\":\"recommended to accept manuscript\",\"frontiersdefaultterm\":\"accepted manuscript\",\"category\":\"process\"},{\"sequencenumber\":142,\"key\":\"recommend_for_rejection\",\"tenantterm\":\"recommend for rejection\",\"frontiersdefaultterm\":\"recommend for rejection\",\"category\":\"process\"},{\"sequencenumber\":143,\"key\":\"recommended_for_rejection\",\"tenantterm\":\"recommended for rejection\",\"frontiersdefaultterm\":\"recommended for rejection\",\"category\":\"process\"},{\"sequencenumber\":144,\"key\":\"ae\",\"tenantterm\":\"ae\",\"frontiersdefaultterm\":\"ae\",\"description\":\"associate editor - board member\",\"category\":\"label (role)\"},{\"sequencenumber\":145,\"key\":\"re\",\"tenantterm\":\"re\",\"frontiersdefaultterm\":\"re\",\"description\":\"review editor\",\"category\":\"label (role)\"},{\"sequencenumber\":146,\"key\":\"rev\",\"tenantterm\":\"rev\",\"frontiersdefaultterm\":\"rev\",\"description\":\"reviewer\",\"category\":\"label (role)\"},{\"sequencenumber\":147,\"key\":\"aut\",\"tenantterm\":\"aut\",\"frontiersdefaultterm\":\"aut\",\"description\":\"author\",\"category\":\"label (role)\"},{\"sequencenumber\":148,\"key\":\"coraut\",\"tenantterm\":\"coraut\",\"frontiersdefaultterm\":\"coraut\",\"description\":\"corresponding author\",\"category\":\"label (role)\"},{\"sequencenumber\":149,\"key\":\"saut\",\"tenantterm\":\"saut\",\"frontiersdefaultterm\":\"saut\",\"description\":\"submitting author\",\"category\":\"label (role)\"},{\"sequencenumber\":150,\"key\":\"coaut\",\"tenantterm\":\"coaut\",\"frontiersdefaultterm\":\"coaut\",\"description\":\"co-author\",\"category\":\"label (role)\"},{\"sequencenumber\":151,\"key\":\"tsof\",\"tenantterm\":\"tsof\",\"frontiersdefaultterm\":\"tsof\",\"description\":\"typesetter\",\"category\":\"label (role)\"},{\"sequencenumber\":152,\"key\":\"typesetting_office\",\"tenantterm\":\"typesetting office\",\"frontiersdefaultterm\":\"typesetting office\",\"category\":\"product\"},{\"sequencenumber\":153,\"key\":\"config\",\"tenantterm\":\"config\",\"frontiersdefaultterm\":\"config\",\"description\":\"configuration office role\",\"category\":\"label (role)\"},{\"sequencenumber\":154,\"key\":\"jm\",\"tenantterm\":\"jm\",\"frontiersdefaultterm\":\"jm\",\"description\":\"journal manager\",\"category\":\"label (role)\"},{\"sequencenumber\":155,\"key\":\"rte\",\"tenantterm\":\"rte\",\"frontiersdefaultterm\":\"rte\",\"description\":\"research topic editor\",\"category\":\"label (role)\"},{\"sequencenumber\":156,\"key\":\"organizations\",\"tenantterm\":\"organizations\",\"frontiersdefaultterm\":\"organizations\",\"category\":\"core\"},{\"sequencenumber\":157,\"key\":\"publishing\",\"tenantterm\":\"publishing\",\"frontiersdefaultterm\":\"publishing\",\"category\":\"core\"},{\"sequencenumber\":158,\"key\":\"acceptance\",\"tenantterm\":\"acceptance\",\"frontiersdefaultterm\":\"acceptance\",\"category\":\"process\"},{\"sequencenumber\":159,\"key\":\"preferred_associate_editor\",\"tenantterm\":\"preferred associate editor\",\"frontiersdefaultterm\":\"preferred associate editor\",\"category\":\"label (role)\"},{\"sequencenumber\":160,\"key\":\"topic_editors\",\"tenantterm\":\"topic editors\",\"frontiersdefaultterm\":\"topic editors\",\"category\":\"label (role)\"},{\"sequencenumber\":161,\"key\":\"institutions\",\"tenantterm\":\"institutions\",\"frontiersdefaultterm\":\"institutions\",\"category\":\"core\"},{\"sequencenumber\":162,\"key\":\"author(s)\",\"tenantterm\":\"author(s)\",\"frontiersdefaultterm\":\"author(s)\",\"category\":\"label (role)\"},{\"sequencenumber\":163,\"key\":\"figure(s)\",\"tenantterm\":\"figure(s)\",\"frontiersdefaultterm\":\"figure(s)\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":164,\"key\":\"co-authors\",\"tenantterm\":\"co-authors\",\"frontiersdefaultterm\":\"co-authors\",\"description\":\"co-authors\",\"category\":\"label (role)\"},{\"sequencenumber\":165,\"key\":\"editorial_board_members\",\"tenantterm\":\"editorial board members\",\"frontiersdefaultterm\":\"editorial board members\",\"description\":\"editorial board members\",\"category\":\"label (role)\"},{\"sequencenumber\":166,\"key\":\"editorial_board\",\"tenantterm\":\"editorial board\",\"frontiersdefaultterm\":\"editorial board\",\"description\":\"editorial board\",\"category\":\"product\"},{\"sequencenumber\":167,\"key\":\"co-authorship\",\"tenantterm\":\"co-authorship\",\"frontiersdefaultterm\":\"co-authorship\",\"description\":\"co-authorship\",\"category\":\"misc.\"},{\"sequencenumber\":168,\"key\":\"role_id_1\",\"tenantterm\":\"registration office\",\"frontiersdefaultterm\":\"registration office\",\"category\":\"user role\"},{\"sequencenumber\":169,\"key\":\"role_id_2\",\"tenantterm\":\"editorial office\",\"frontiersdefaultterm\":\"editorial office\",\"category\":\"user role\"},{\"sequencenumber\":170,\"key\":\"role_id_7\",\"tenantterm\":\"field chief editor\",\"frontiersdefaultterm\":\"field chief editor\",\"category\":\"user role\"},{\"sequencenumber\":171,\"key\":\"role_id_8\",\"tenantterm\":\"assistant field chief editor\",\"frontiersdefaultterm\":\"assistant field chief editor\",\"category\":\"user role\"},{\"sequencenumber\":172,\"key\":\"role_id_9\",\"tenantterm\":\"specialty chief editor\",\"frontiersdefaultterm\":\"specialty chief editor\",\"category\":\"user role\"},{\"sequencenumber\":173,\"key\":\"role_id_10\",\"tenantterm\":\"assistant specialty chief editor\",\"frontiersdefaultterm\":\"assistant specialty chief editor\",\"category\":\"user role\"},{\"sequencenumber\":174,\"key\":\"role_id_11\",\"tenantterm\":\"associate editor\",\"frontiersdefaultterm\":\"associate editor\",\"category\":\"user role\"},{\"sequencenumber\":175,\"key\":\"role_id_12\",\"tenantterm\":\"guest associate editor\",\"frontiersdefaultterm\":\"guest associate editor\",\"category\":\"user role\"},{\"sequencenumber\":176,\"key\":\"role_id_13\",\"tenantterm\":\"review editor\",\"frontiersdefaultterm\":\"review editor\",\"category\":\"user role\"},{\"sequencenumber\":177,\"key\":\"role_id_14\",\"tenantterm\":\"reviewer\",\"frontiersdefaultterm\":\"reviewer\",\"category\":\"user role\"},{\"sequencenumber\":178,\"key\":\"role_id_15\",\"tenantterm\":\"author\",\"frontiersdefaultterm\":\"author\",\"category\":\"user role\"},{\"sequencenumber\":179,\"key\":\"role_id_16\",\"tenantterm\":\"corresponding author\",\"frontiersdefaultterm\":\"corresponding author\",\"category\":\"user role\"},{\"sequencenumber\":180,\"key\":\"role_id_17\",\"tenantterm\":\"submitting author\",\"frontiersdefaultterm\":\"submitting author\",\"category\":\"user role\"},{\"sequencenumber\":181,\"key\":\"role_id_18\",\"tenantterm\":\"co-author\",\"frontiersdefaultterm\":\"co-author\",\"category\":\"user role\"},{\"sequencenumber\":182,\"key\":\"role_id_20\",\"tenantterm\":\"production office\",\"frontiersdefaultterm\":\"production office\",\"category\":\"user role\"},{\"sequencenumber\":183,\"key\":\"role_id_22\",\"tenantterm\":\"typesetting office (typesetter)\",\"frontiersdefaultterm\":\"typesetting office (typesetter)\",\"category\":\"user role\"},{\"sequencenumber\":184,\"key\":\"role_id_24\",\"tenantterm\":\"registered user\",\"frontiersdefaultterm\":\"registered user\",\"category\":\"user role\"},{\"sequencenumber\":185,\"key\":\"role_id_35\",\"tenantterm\":\"job office\",\"frontiersdefaultterm\":\"job office\",\"category\":\"user role\"},{\"sequencenumber\":186,\"key\":\"role_id_41\",\"tenantterm\":\"special event administrator\",\"frontiersdefaultterm\":\"special event administrator\",\"category\":\"user role\"},{\"sequencenumber\":187,\"key\":\"role_id_42\",\"tenantterm\":\"special event reviewer\",\"frontiersdefaultterm\":\"special event reviewer\",\"category\":\"user role\"},{\"sequencenumber\":188,\"key\":\"role_id_43\",\"tenantterm\":\"submit abstract\",\"frontiersdefaultterm\":\"submit abstract\",\"category\":\"user role\"},{\"sequencenumber\":189,\"key\":\"role_id_52\",\"tenantterm\":\"events office\",\"frontiersdefaultterm\":\"events office\",\"category\":\"user role\"},{\"sequencenumber\":190,\"key\":\"role_id_53\",\"tenantterm\":\"event administrator\",\"frontiersdefaultterm\":\"event administrator\",\"category\":\"user role\"},{\"sequencenumber\":191,\"key\":\"role_id_89\",\"tenantterm\":\"content management office\",\"frontiersdefaultterm\":\"content management office\",\"category\":\"user role\"},{\"sequencenumber\":192,\"key\":\"role_id_98\",\"tenantterm\":\"accounting office\",\"frontiersdefaultterm\":\"accounting office\",\"category\":\"user role\"},{\"sequencenumber\":193,\"key\":\"role_id_99\",\"tenantterm\":\"projects\",\"frontiersdefaultterm\":\"projects\",\"category\":\"user role\"},{\"sequencenumber\":194,\"key\":\"role_id_103\",\"tenantterm\":\"configuration office\",\"frontiersdefaultterm\":\"configuration office\",\"category\":\"user role\"},{\"sequencenumber\":195,\"key\":\"role_id_104\",\"tenantterm\":\"beta user\",\"frontiersdefaultterm\":\"beta user\",\"category\":\"user role\"},{\"sequencenumber\":196,\"key\":\"role_id_106\",\"tenantterm\":\"wfconf\",\"frontiersdefaultterm\":\"wfconf\",\"category\":\"user role\"},{\"sequencenumber\":197,\"key\":\"role_id_107\",\"tenantterm\":\"rt management beta user\",\"frontiersdefaultterm\":\"rt management beta user\",\"category\":\"user role\"},{\"sequencenumber\":198,\"key\":\"role_id_108\",\"tenantterm\":\"deo beta user\",\"frontiersdefaultterm\":\"deo beta user\",\"category\":\"user role\"},{\"sequencenumber\":199,\"key\":\"role_id_109\",\"tenantterm\":\"search beta user\",\"frontiersdefaultterm\":\"search beta user\",\"category\":\"user role\"},{\"sequencenumber\":200,\"key\":\"role_id_110\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"journal manager\",\"category\":\"user role\"},{\"sequencenumber\":201,\"key\":\"role_id_111\",\"tenantterm\":\"myfrontiers beta user\",\"frontiersdefaultterm\":\"myfrontiers beta user\",\"category\":\"user role\"},{\"sequencenumber\":202,\"key\":\"role_id_21\",\"tenantterm\":\"copy editor\",\"frontiersdefaultterm\":\"copy editor\",\"category\":\"user role\"},{\"sequencenumber\":203,\"key\":\"role_id_1_abr\",\"tenantterm\":\"rof\",\"frontiersdefaultterm\":\"rof\",\"category\":\"user role\"},{\"sequencenumber\":204,\"key\":\"role_id_2_abr\",\"tenantterm\":\"eof\",\"frontiersdefaultterm\":\"eof\",\"category\":\"user role\"},{\"sequencenumber\":205,\"key\":\"role_id_7_abr\",\"tenantterm\":\"fce\",\"frontiersdefaultterm\":\"fce\",\"category\":\"user role\"},{\"sequencenumber\":206,\"key\":\"role_id_8_abr\",\"tenantterm\":\"afce\",\"frontiersdefaultterm\":\"afce\",\"category\":\"user role\"},{\"sequencenumber\":207,\"key\":\"role_id_9_abr\",\"tenantterm\":\"sce\",\"frontiersdefaultterm\":\"sce\",\"category\":\"user role\"},{\"sequencenumber\":208,\"key\":\"role_id_10_abr\",\"tenantterm\":\"asce\",\"frontiersdefaultterm\":\"asce\",\"category\":\"user role\"},{\"sequencenumber\":209,\"key\":\"role_id_11_abr\",\"tenantterm\":\"ae\",\"frontiersdefaultterm\":\"ae\",\"category\":\"user role\"},{\"sequencenumber\":210,\"key\":\"role_id_12_abr\",\"tenantterm\":\"gae\",\"frontiersdefaultterm\":\"gae\",\"category\":\"user role\"},{\"sequencenumber\":211,\"key\":\"role_id_13_abr\",\"tenantterm\":\"re\",\"frontiersdefaultterm\":\"re\",\"category\":\"user role\"},{\"sequencenumber\":212,\"key\":\"role_id_14_abr\",\"tenantterm\":\"rev\",\"frontiersdefaultterm\":\"rev\",\"category\":\"user role\"},{\"sequencenumber\":213,\"key\":\"role_id_15_abr\",\"tenantterm\":\"aut\",\"frontiersdefaultterm\":\"aut\",\"category\":\"user role\"},{\"sequencenumber\":214,\"key\":\"role_id_16_abr\",\"tenantterm\":\"coraut\",\"frontiersdefaultterm\":\"coraut\",\"category\":\"user role\"},{\"sequencenumber\":215,\"key\":\"role_id_17_abr\",\"tenantterm\":\"saut\",\"frontiersdefaultterm\":\"saut\",\"category\":\"user role\"},{\"sequencenumber\":216,\"key\":\"role_id_18_abr\",\"tenantterm\":\"coaut\",\"frontiersdefaultterm\":\"coaut\",\"category\":\"user role\"},{\"sequencenumber\":217,\"key\":\"role_id_20_abr\",\"tenantterm\":\"pof\",\"frontiersdefaultterm\":\"pof\",\"category\":\"user role\"},{\"sequencenumber\":218,\"key\":\"role_id_22_abr\",\"tenantterm\":\"tsof\",\"frontiersdefaultterm\":\"tsof\",\"category\":\"user role\"},{\"sequencenumber\":219,\"key\":\"role_id_24_abr\",\"tenantterm\":\"ru\",\"frontiersdefaultterm\":\"ru\",\"category\":\"user role\"},{\"sequencenumber\":220,\"key\":\"role_id_35_abr\",\"tenantterm\":\"jof\",\"frontiersdefaultterm\":\"jof\",\"category\":\"user role\"},{\"sequencenumber\":221,\"key\":\"role_id_41_abr\",\"tenantterm\":\"se-adm\",\"frontiersdefaultterm\":\"se-adm\",\"category\":\"user role\"},{\"sequencenumber\":222,\"key\":\"role_id_42_abr\",\"tenantterm\":\"se-rev\",\"frontiersdefaultterm\":\"se-rev\",\"category\":\"user role\"},{\"sequencenumber\":223,\"key\":\"role_id_43_abr\",\"tenantterm\":\"se-aut\",\"frontiersdefaultterm\":\"se-aut\",\"category\":\"user role\"},{\"sequencenumber\":224,\"key\":\"role_id_52_abr\",\"tenantterm\":\"evof\",\"frontiersdefaultterm\":\"evof\",\"category\":\"user role\"},{\"sequencenumber\":225,\"key\":\"role_id_53_abr\",\"tenantterm\":\"ev-adm\",\"frontiersdefaultterm\":\"ev-adm\",\"category\":\"user role\"},{\"sequencenumber\":226,\"key\":\"role_id_89_abr\",\"tenantterm\":\"comof\",\"frontiersdefaultterm\":\"comof\",\"category\":\"user role\"},{\"sequencenumber\":227,\"key\":\"role_id_98_abr\",\"tenantterm\":\"aof\",\"frontiersdefaultterm\":\"aof\",\"category\":\"user role\"},{\"sequencenumber\":228,\"key\":\"role_id_99_abr\",\"tenantterm\":\"projects\",\"frontiersdefaultterm\":\"projects\",\"category\":\"user role\"},{\"sequencenumber\":229,\"key\":\"role_id_103_abr\",\"tenantterm\":\"config\",\"frontiersdefaultterm\":\"config\",\"category\":\"user role\"},{\"sequencenumber\":230,\"key\":\"role_id_104_abr\",\"tenantterm\":\"beta\",\"frontiersdefaultterm\":\"beta\",\"category\":\"user role\"},{\"sequencenumber\":231,\"key\":\"role_id_106_abr\",\"tenantterm\":\"wfconf\",\"frontiersdefaultterm\":\"wfconf\",\"category\":\"user role\"},{\"sequencenumber\":232,\"key\":\"role_id_107_abr\",\"tenantterm\":\"rtbeta\",\"frontiersdefaultterm\":\"rtbeta\",\"category\":\"user role\"},{\"sequencenumber\":233,\"key\":\"role_id_108_abr\",\"tenantterm\":\"deobeta\",\"frontiersdefaultterm\":\"deobeta\",\"category\":\"user role\"},{\"sequencenumber\":234,\"key\":\"role_id_109_abr\",\"tenantterm\":\"searchbeta\",\"frontiersdefaultterm\":\"searchbeta\",\"category\":\"user role\"},{\"sequencenumber\":235,\"key\":\"role_id_110_abr\",\"tenantterm\":\"jm\",\"frontiersdefaultterm\":\"jm\",\"category\":\"user role\"},{\"sequencenumber\":236,\"key\":\"role_id_111_abr\",\"tenantterm\":\"mfbeta\",\"frontiersdefaultterm\":\"mfbeta\",\"category\":\"user role\"},{\"sequencenumber\":237,\"key\":\"role_id_21_abr\",\"tenantterm\":\"coped\",\"frontiersdefaultterm\":\"coped\",\"category\":\"user role\"},{\"sequencenumber\":238,\"key\":\"reviewer_editorial_board\",\"tenantterm\":\"editorial board\",\"frontiersdefaultterm\":\"editorial board\",\"description\":\"this is the label for the review editorial board\",\"category\":\"label\"},{\"sequencenumber\":239,\"key\":\"field_chief_editor\",\"tenantterm\":\"field chief editor\",\"frontiersdefaultterm\":\"field chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":240,\"key\":\"field_chief_editors\",\"tenantterm\":\"field chief editors\",\"frontiersdefaultterm\":\"field chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":241,\"key\":\"editor\",\"tenantterm\":\"editor\",\"frontiersdefaultterm\":\"editor\",\"category\":\"label (role)\"},{\"sequencenumber\":242,\"key\":\"editors\",\"tenantterm\":\"editors\",\"frontiersdefaultterm\":\"editors\",\"category\":\"label (role)\"},{\"sequencenumber\":243,\"key\":\"board\",\"tenantterm\":\"board\",\"frontiersdefaultterm\":\"board\",\"category\":\"label\"},{\"sequencenumber\":244,\"key\":\"boards\",\"tenantterm\":\"boards\",\"frontiersdefaultterm\":\"boards\",\"category\":\"label\"},{\"sequencenumber\":245,\"key\":\"article_collection\",\"tenantterm\":\"article collection\",\"frontiersdefaultterm\":\"article collection\",\"category\":\"label\"},{\"sequencenumber\":246,\"key\":\"article_collections\",\"tenantterm\":\"article collections\",\"frontiersdefaultterm\":\"article collections\",\"category\":\"label\"},{\"sequencenumber\":247,\"key\":\"handling_editor\",\"tenantterm\":\"handling editor\",\"frontiersdefaultterm\":\"associate editor\",\"description\":\"this terminology key is for the person assigned to edit a manuscript. it is a label for the temporary handling editor assignment.\",\"category\":\"label (role)\"},{\"sequencenumber\":248,\"key\":\"handling_editors\",\"tenantterm\":\"handling editors\",\"frontiersdefaultterm\":\"associate editors\",\"description\":\"this terminology key is for the person assigned to edit a manuscript. it is a label for the temporary handling editor assignment.\",\"category\":\"label (role)\"},{\"sequencenumber\":249,\"key\":\"ae_accept\",\"tenantterm\":\"recommend acceptance\",\"frontiersdefaultterm\":\"accept\",\"category\":\"process\"},{\"sequencenumber\":250,\"key\":\"rtm\",\"tenantterm\":\"rtm\",\"frontiersdefaultterm\":\"rtm\",\"category\":\"product\"},{\"sequencenumber\":251,\"key\":\"frontiers_media_sa\",\"tenantterm\":\"frontiers media s.a\",\"frontiersdefaultterm\":\"frontiers media s.a\",\"category\":\"customer\"},{\"sequencenumber\":252,\"key\":\"review_editors\",\"tenantterm\":\"community reviewers\",\"frontiersdefaultterm\":\"review editors\",\"category\":\"label (role)\"},{\"sequencenumber\":253,\"key\":\"journal_card_chief_editor\",\"tenantterm\":\"chief editor\",\"frontiersdefaultterm\":\"chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":254,\"key\":\"journal_card_chief_editors\",\"tenantterm\":\"chief editors\",\"frontiersdefaultterm\":\"chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":255,\"key\":\"call_for_papers\",\"tenantterm\":\"call for papers\",\"frontiersdefaultterm\":\"call for papers\",\"category\":\"label\"},{\"sequencenumber\":256,\"key\":\"calls_for_papers\",\"tenantterm\":\"calls for papers\",\"frontiersdefaultterm\":\"calls for papers\",\"category\":\"label\"},{\"sequencenumber\":257,\"key\":\"supervising_editor\",\"tenantterm\":\"supervising editor\",\"frontiersdefaultterm\":\"supervising editor\",\"description\":\"a chief or assistant chief editor who is assigned to a manuscript to supervise.\",\"category\":\"role\",\"externalkey\":\"supervising_editor\"},{\"sequencenumber\":258,\"key\":\"supervising_editors\",\"tenantterm\":\"supervising editors\",\"frontiersdefaultterm\":\"supervising editors\",\"description\":\"a chief or assistant chief editor who is assigned to a manuscript to supervise.\",\"category\":\"role\",\"externalkey\":\"supervising_editors\"},{\"sequencenumber\":259,\"key\":\"reviewer_endorse\",\"tenantterm\":\"endorse\",\"frontiersdefaultterm\":\"endorse\",\"category\":\"label\"},{\"sequencenumber\":260,\"key\":\"reviewer_endorsed\",\"tenantterm\":\"endorsed\",\"frontiersdefaultterm\":\"endorsed\",\"category\":\"label\"},{\"sequencenumber\":261,\"key\":\"reviewer_endorse_publication\",\"tenantterm\":\"endorse publication\",\"frontiersdefaultterm\":\"endorse publication\",\"category\":\"label\"},{\"sequencenumber\":262,\"key\":\"reviewer_endorsed_publication\",\"tenantterm\":\"endorsed publication\",\"frontiersdefaultterm\":\"endorsed publication\",\"category\":\"label\"},{\"sequencenumber\":263,\"key\":\"editor_role\",\"tenantterm\":\"editor role\",\"frontiersdefaultterm\":\"editor role\",\"category\":\"label\"},{\"sequencenumber\":264,\"key\":\"editor_roles\",\"tenantterm\":\"editor roles\",\"frontiersdefaultterm\":\"editor roles\",\"category\":\"label\"},{\"sequencenumber\":265,\"key\":\"editorial_role\",\"tenantterm\":\"editorial role\",\"frontiersdefaultterm\":\"editorial role\",\"category\":\"label\"},{\"sequencenumber\":266,\"key\":\"editorial_roles\",\"tenantterm\":\"editorial roles\",\"frontiersdefaultterm\":\"editorial roles\",\"category\":\"label\"},{\"sequencenumber\":267,\"key\":\"call_for_paper\",\"tenantterm\":\"call for paper\",\"frontiersdefaultterm\":\"call for paper\",\"category\":\"label\"},{\"sequencenumber\":268,\"key\":\"research_topic_abstract\",\"tenantterm\":\"manuscript summary\",\"frontiersdefaultterm\":\"manuscript summary\",\"category\":\"process\"},{\"sequencenumber\":269,\"key\":\"research_topic_abstracts\",\"tenantterm\":\"manuscript summaries\",\"frontiersdefaultterm\":\"manuscript summaries\",\"category\":\"process\"},{\"sequencenumber\":270,\"key\":\"submissions_team_manager\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"content manager\",\"category\":\"process\"},{\"sequencenumber\":271,\"key\":\"submissions_team\",\"tenantterm\":\"journal team\",\"frontiersdefaultterm\":\"content team\",\"category\":\"process\"},{\"sequencenumber\":272,\"key\":\"topic_coordinator\",\"tenantterm\":\"topic coordinator\",\"frontiersdefaultterm\":\"topic coordinator\",\"category\":\"process\"},{\"sequencenumber\":273,\"key\":\"topic_coordinators\",\"tenantterm\":\"topic coordinators\",\"frontiersdefaultterm\":\"topic coordinators\",\"category\":\"process\"},{\"sequencenumber\":274,\"key\":\"frontiers_journal_team\",\"tenantterm\":\"frontiers journal team\",\"frontiersdefaultterm\":\"frontiers journal team\",\"category\":\"label (role)\"},{\"sequencenumber\":275,\"key\":\"research_topic_ic_submission\",\"tenantterm\":\"rt:ic submission\",\"frontiersdefaultterm\":\"rt:ic submission\",\"category\":\"label (role)\"},{\"sequencenumber\":276,\"key\":\"research_topic_ic_submission_participant\",\"tenantterm\":\"rt:ic submission participant\",\"frontiersdefaultterm\":\"rt:ic submission participant\",\"category\":\"label (role)\"}]}'\n",impactgtmid:"gtm-njf2ml9b",impactgtmauth:"fyo-7nodm9w37qgfms03sa",impactgtmpreview:"env-1",impactgooglemapsapikey:"aizasyctehx2eu8pwfi86gw9fi00ftcykk6ifr8",qualtricsv4tov3:"'{\"enabled\":true,\"interceptid\":\"si_er0e2ze69oz8yn4\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",qualtricsumux:"'{\"enabled\":false,\"interceptid\":\"si_89fphy3etxqbwtu\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",qualtricscontinuousbrand:"'{\"enabled\":true,\"interceptid\":\"si_9zut94ut81fcrh4\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",defaultarticletemplate:"v3"},app:{baseurl:"/",buildid:"f2d5b6d8-d01c-4534-90b9-d659f10f17ca",buildassetsdir:"ap-2024/_nuxt",cdnurl:""}}

## Discussion
and future work compared with existing methods, this study overcomes the limitation of traditional deep learning models relying on fixed convolution kernels, enabling adaptive capture of lesion features with multi-shaped convolution kernels to accurately extract edge textures of circular micro-lesions and regional contours of irregular lesions. although the constructed multi-source dataset covers diverse lighting conditions, cultivars, and lesion development stages, the homogeneous internal features of severe diseases lead to limited recall rate. meanwhile, when lesions are severely overlapped or mixed with mechanical damage, insect damage, or other types of injuries, the discriminative accuracy of the model is affected. for ultra-small lesions, the local feature information is too weak and easily overlooked, and the computational efficiency still needs optimization on some hardware platforms. furthermore, the adaptability of the current model in extreme field scenarios, such as high-density occlusion and compound damage from pests and diseases, remains to be further verified. in these scenarios, the coupling of complex interference factors exacerbates feature confusion, affecting grading reliability. to address these issues, future work will focus on the following research directions. future research will focus on enhancing the model&#x2019;s performance in complex field environments through multiple synergistic strategies. this includes constructing multi-interference factor coupled datasets that incorporating insect holes, lesions, and soil adhesion to strengthen robustness against extreme field disturbances; introducing attention-based multi-damage feature decoupling modules and dedicated micro-lesion enhancement modules combining super-resolution and feature interpolation to improve discriminability in complex scenarios and for tiny lesions;&#xa0;optimizing dynamic weight calculations via approximate computation or hardware-friendly reconstruction, while developing lightweight real-time deployment frameworks integrated with uav near-ground sensing technology to boost efficiency and practical monitoring coverage; and establishing cross-crop pathological transfer learning mechanisms, extending the model to multi-crop disease recognition tasks with self-supervised pre-training to enhance algorithm universality and low-contrast lesion feature mining capabilities. through the above research, it is expected to further improve the applicability and practicality of the model in complex field environments, promoting the development of intelligent plant disease grading technology towards more accurate, efficient, and universal directions. data availability statement the raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. author contributions yw: writing &#x2013; original draft. dz: writing &#x2013; original draft. lz:&#xa0;writing &#x2013; review &amp; editing. funding the author(s) declare financial support was received for the research and/or publication of this article. this work was supported in part by the science and technology key project of the xinjiang production and construction corps (2023ab063) development and application demonstration of green technology for online monitoring of fresh fruits and cold chain logistics in xinjiang. conflict of interest the authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. generative ai statement the author(s) declare that no generative ai was used in the creation of this manuscript. any alternative text (alt text) provided alongside figures in this article has been generated by frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. if you identify any issues, please contact us. publisher&#x2019;s note all claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. references acharya, p., burgers, t., and nguyen, k.-d. (2022). ai-enabled droplet detection and tracking for agricultural spraying systems. computers and electronics in agriculture , 202, 107325. doi:&#xa0;10.1016/j.compag.2022.107325 crossref full text | google scholar adaskaveg, j., f&#xf6;rster, h., thompson, d., driever, g., connell, j., buchner, r., et al. (2009). epidemiology and management of walnut blight. walnut res. rep , 94, 225&#x2013;256. doi:&#xa0;10.1016/j.inpa.2016.10.005 crossref full text | google scholar arivazhagan, s., shebiah, r. n., ananthi, s., and varthini, s. v. (2013). detection of unhealthy region of plant leaves and classification of plant leaf diseases using texture features. agricultural engineering international: cigr journal , 15, 211&#x2013;217. google scholar chaudhary, p., chaudhari, a. k., cheeran, a., and godara, s. (2012). color transform based approach for disease spot detection on plant leaf. international journal of computer science and telecommunications , 3, 65&#x2013;70. google scholar chen, s., zhang, k., zhao, y., sun, y., ban, w., chen, y., et al. (2021). an approach for rice bacterial leaf streak disease segmentation and disease severity estimation. agriculture , 11, 420. doi:&#xa0;10.3390/agriculture11050420 crossref full text | google scholar chiang, k.-s., bock, c., el jarroudi, m., delfosse, p., lee, i., and liu, h. (2016). effects of rater bias and assessment method on disease severity estimation with regard to hypothesis testing. phytopathology , 65, 523&#x2013;535. doi:&#xa0;10.1111/ppa.12435 crossref full text | google scholar cooke, b. (2006). &#x201c;disease assessment and yield loss,&#x201d; in the epidemiology of plant diseases (dordrecht: springer), 43&#x2013;80. google scholar ghaiwat, s. n. and arora, p. (2014). detection and classification of plant leaf diseases using image processing techniques: a review. international journal of recent advances in engineering &amp; technology , 2, 1&#x2013;7. google scholar hu, g., wang, h., zhang, y., and wan, m. (2021). detection and severity analysis of tea leaf blight based on deep learning. computers &amp; electrical engineering , 90, 107023. doi:&#xa0;10.1016/j.compeleceng.2021.107023 crossref full text | google scholar jadhav, s. b. and patil, s. b. (2016). grading of soybean leaf disease based on segmented image using k-means clustering. iaes international journal of artificial intelligence , 5, 13&#x2013;13. doi:&#xa0;10.11591/ijai.v5.i1.pp13-21 crossref full text | google scholar jo, w. j., kim, d. s., sim, h. s., ahn, s. r., lee, h. j., moon, y. h., et al. (2021). estimation of evapotranspiration and water requirements of strawberry plants in greenhouses using environmental data. frontiers in sustainable food systems , 5, 684808. doi:&#xa0;10.3389/fsufs.2021.684808 crossref full text | google scholar khan, m. a., ali, m., shah, m., mahmood, t., ahmad, m., jhanjhi, n., et al. (2021). machine learning-based detection and classification of walnut fungi diseases. intelligent automation &amp; soft computing , 30, 771&#x2013;785. doi:&#xa0;10.32604/iasc.2021.018039 crossref full text | google scholar kim, s.-k. and ahn, j.-g. (2021). tomato crop diseases classification models using deep cnn-based architectures. j korea acad-ind coop soc , 22, 7&#x2013;14. doi:&#xa0;10.5762/kais.2021.22.5.7 crossref full text | google scholar lamichhane, j. r. (2014). xanthomonas arboricola diseases of stone fruit, almond, and walnut trees: progress toward understanding and management. plant disease , 98, 1600&#x2013;1610. doi:&#xa0;10.1094/pdis-08-14-0831-fe pubmed abstract | crossref full text | google scholar li, x., zha, t., liu, p., bourque, c. p.-a., jia, x., and tian, y. (2023). interannual variation in gross ecosystem production and evapotranspiration in a temperate semiarid grassland undergoing vegetation recovery. agricultural and forest meteorology , 341, 109672. doi:&#xa0;10.1016/j.agrformet.2023.109672 crossref full text | google scholar lin, k., gong, l., huang, y., liu, c., and pan, j. (2019). deep learning-based segmentation and quantification of cucumber powdery mildew using convolutional neural network. frontiers in plant science , 10, 155. doi:&#xa0;10.3389/fpls.2019.00155 pubmed abstract | crossref full text | google scholar ma, j., du, k., zhang, l., zheng, f., chu, j., and sun, z. (2017). a segmentation method for greenhouse vegetable foliar disease spots images using color information and region growing. computers and electronics in agriculture , 142, 110&#x2013;117. doi:&#xa0;10.1016/j.compag.2017.08.023 crossref full text | google scholar mao, r., wang, z., li, f., zhou, j., chen, y., and hu, x. (2023). gseyolox-s: an improved lightweight network for identifying the severity of wheat fusarium head blight. agronomy , 13, 242. doi:&#xa0;10.3390/agronomy13010242 crossref full text | google scholar mcgranahan, g. and leslie, c. (1991). walnuts (juglans). molecules , 907&#x2013;924. doi:&#xa0;10.17660/actahortic.1991.290.20 crossref full text | google scholar mircetich, s. m., sanborn, r., and ramos, d. (1980). natural spread, graft-transmission, and possible etiology of walnut blackline disease. phytopathology , 70, 962&#x2013;968. doi:&#xa0;10.1094/phyto-70-962 crossref full text | google scholar moragrega, c., matias, j., alet&#xe0;, n., montesinos, e., and rovira, m. (2011). apical necrosis and premature drop of persian (english) walnut fruit caused by xanthomonas arboricola pv. juglandis. plant disease , 95, 1565&#x2013;1570. doi:&#xa0;10.1094/pdis-03-11-0259 pubmed abstract | crossref full text | google scholar ngugi, l. c., abdelwahab, m., and abo-zahhad, m. (2020). tomato leaf segmentation algorithms for mobile phone applications using deep learning. computers and electronics in agriculture , 178, 105788. doi:&#xa0;10.1016/j.compag.2020.105788 crossref full text | google scholar ozturk, o., sarica, b., and seker, d. z. (2025). interpretable and robust ensemble deep learning framework for tea leaf disease classification. horticulturae , 11, 437. doi:&#xa0;10.3390/horticulturae11040437 crossref full text | google scholar parashar, n., johri, p., khan, a. a., gaur, n., and kadry, s. (2024). an integrated analysis of yield prediction models: a comprehensive review of advancements and challenges. computers, materials &amp; continua , 80 (1). doi:&#xa0;10.32604/cmc.2024.050240 crossref full text | google scholar picon, a., alvarez-gila, a., seitz, m., ortiz-barredo, a., echazarra, j., and johannes, a. (2019). deep convolutional neural networks for mobile capture device-based crop disease classification in the wild. computers and electronics in agriculture , 161, 280&#x2013;290. doi:&#xa0;10.1016/j.compag.2018.04.002 crossref full text | google scholar rastogi, a., arora, r., and sharma, s. (2015). &#x201c;leaf disease detection and grading using computer vision technology &amp; fuzzy logic,&#x201d; in paper presented at the 2015 2nd international conference on signal processing and integrated networks (spin). google scholar rustia, d. j. a., lee, w.-c., lu, c.-y., wu, y.-f., shih, p.-y., and chen, s.-k. (2023). edge-based wireless imaging system for continuous monitoring of insect pests in a remote outdoor mango orchard. computers and electronics in agriculture , 211, 108019. doi:&#xa0;10.1016/j.compag.2023.108019 crossref full text | google scholar shi, t., liu, y., zheng, x., hu, k., huang, h., liu, h., et al. (2023). recent advances in plant disease severity assessment using convolutional neural networks. scientific reports , 13, 2336. doi:&#xa0;10.1038/s41598-023-29230-7 pubmed abstract | crossref full text | google scholar singh, u. p., chouhan, s. s., jain, s., and jain, s. (2019). multilayer convolution neural network for the classification of mango leaves infected by anthracnose disease. ieee access , 7, 43721&#x2013;43729. doi:&#xa0;10.1109/access.2019.2907383 crossref full text | google scholar su, y., wang, j., li, j., wang, l., wang, k., li, a., et al. (2023). spatiotemporal changes and driving factors of reference evapotranspiration and crop evapotranspiration for cotton production in china from 1960 to 2019. frontiers in environmental science , 11, 1251789. doi:&#xa0;10.3389/fenvs.2023.1251789 crossref full text | google scholar tripathi, r. (2021). &#x201c;a deep learning approach for plant material disease identification,&#x201d; in paper presented at the iop conference series: materials science and engineering. google scholar vishnoi, v. k., kumar, k., kumar, b., mohan, s., and khan, a. a. (2022). detection of apple plant diseases using leaf images through convolutional neural network . ieee access , 11, 6594&#x2013;6609. doi:&#xa0;10.1109/access.2022.3232917 crossref full text | google scholar wang, f., dun, c., tang, t., duan, y., guo, x., and you, j. (2022). boeremia exigua causes leaf spot of walnut trees (juglans regia) in china. plant disease , 106, 1993. doi:&#xa0;10.1094/pdis-10-21-2304-pdn crossref full text | google scholar wang, q.-h., fan, k., li, d.-w., han, c.-m., qu, y.-y., qi, y.-k., et al. (2020). identification, virulence and fungicide sensitivity of colletotrichum gloeosporioides ss responsible for walnut anthracnose disease in china. plant disease , 104, 1358&#x2013;1368. doi:&#xa0;10.1094/pdis-12-19-2569-re pubmed abstract | crossref full text | google scholar weber, b. c. (1980). how to diagnose black walnut damage vol. 57 (north central forest experiment station, forest service, us department of agriculture). google scholar xu, w. (2024). hcf-net: hierarchicalcontextfusion network forinfrared small object detection. arxiv . arxiv, 2403.10778. doi:&#xa0;10.1109/icme57554.2024.10687431 crossref full text | google scholar yang, c., deng, y., wang, f., yang, h., xu, x., zeng, q., et al. (2021). brown leaf spot on juglans sigillata caused by ophiognomonia leptostyla in sichuan, china. plant disease , 105, 4160. doi:&#xa0;10.1094/pdis-02-21-0344-pdn crossref full text | google scholar yang, q., duan, s., and wang, l. (2022). efficient identification of apple leaf diseases in the wild using convolutional neural networks. agronomy , 12, 2784. doi:&#xa0;10.3390/agronomy12112784 crossref full text | google scholar zarei, s., taghavi, s. m., banihashemi, z., hamzehzarghani, h., and osdaghi, e. (2019). etiology of leaf spot and fruit canker symptoms on stone fruits and nut trees in iran. journal of plant pathology , 101, 1133&#x2013;1142. doi:&#xa0;10.1007/s42161-019-00283-w crossref full text | google scholar zhang, y., li, x., &#x160;im&#x16f;nek, j., shi, h., chen, n., and hu, q. (2023). quantifying water and salt movement in a soil-plant system of a corn field using hydrus (2d/3d) and the stable isotope method. agricultural water management , 288, 108492. doi:&#xa0;10.1016/j.agwat.2023.108492 crossref full text | google scholar keywords: walnut, brown spot disease ( ophiognomonia leptostyla ), hierarchical feature selection, edge features perception, adaptive multi-scale dilated convolution, disease grading citation: wei y, zeng d and zheng l (2025) a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution. front. plant sci. 16:1641677. doi: 10.3389/fpls.2025.1641677 received: 05 june 2025; accepted: 09 september 2025; published: 03 october 2025. edited by: ravinder kumar , indian agricultural research institute (icar), india reviewed by: geza bujdoso , hungarian university of agricultural and life sciences, hungary vasudha vedula , university of texas of the permian basin, united states copyright &#xa9; 2025 wei, zeng and zheng. this is an open-access article distributed under the terms of the creative commons attribution license (cc by) . the use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. no use, distribution or reproduction is permitted which does not comply with these terms. *correspondence: liangfang zheng, mtgxnjaynta0othamtyzlmnvbq== &#x2020; these authors have contributed equally to this work disclaimer: all claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. download article download pdf readcube epub xml share on export citation endnote reference manager simple text file bibtex 1,063 total views 103 downloads citation numbers are available from dimensions view article impact view altmetric score share on edited by r k ravinder kumar reviewed by g b geza bujdoso v v vasudha vedula table of contents abstract 1 introduction 2 materials and methods 3 experiments and results analysis 4 conclusion 5 discussion and future work data availability statement author contributions funding conflict of interest generative ai statement publisher&#x2019;s note references export citation endnote reference manager simple text file bibtex check for updates frontiers&#39; impact articles published with frontiers have received 12 million total citations your research is the real superpower - learn how we maximise its impact through our leading community journals explore our impact metrics supplementary material download article download download pdf readcube epub xml guidelines author guidelines services for authors policies and publication ethics editor guidelines fee policy explore articles research topics journals how we publish outreach frontiers forum frontiers policy labs frontiers for young minds frontiers planet prize connect help center emails and alerts contact us submit career opportunities follow us © 2025 frontiers media s.a. all rights reserved privacy policy | terms and conditions [["shallowreactive",1],{"data":2,"state":4,"once":10,"_errors":11,"serverrendered":13,"path":14,"pinia":15},["shallowreactive",3],{},["reactive",5],{"$ssite-config":6},{"env":7,"name":8,"url":9},"production","frontiers articles","https://article-pages-2024.frontiersin.org/",["set"],["shallowreactive",12],{},true,"/journals/plant-science/articles/10.3389/fpls.2025.1641677/full",["reactive",16],{"main":17,"user":534,"article":535,"articlehub":760,"mainheader":764},{"ibar":18,"footer":284,"newslettercomponent":-1,"snackbaritem":366,"toggleshowsnackbar":367,"contentfuljournal":368,"graphjournal":434,"settingsfeaturesswitchers":438,"templatetogglebanner":439,"tenantconfig":499},{"tenantlogo":19,"journallogo":19,"aboutus":20,"submiturl":113,"showsubmitbutton":13,"journal":114,"sectionterm":215,"aboutjournal":216,"mainlinks":265,"journallinks":272,"helpcenterlink":281},"",[21,38,47,71,87],{"title":22,"links":23},"who we are",[24,29,32,35],{"text":25,"url":26,"target":27,"arialabel":28},"mission and values","https://www.frontiersin.org/about/mission","_self",null,{"text":30,"url":31,"target":27,"arialabel":28},"history","https://www.frontiersin.org/about/history",{"text":33,"url":34,"target":27,"arialabel":28},"leadership","https://www.frontiersin.org/about/leadership",{"text":36,"url":37,"target":27,"arialabel":28},"awards","https://www.frontiersin.org/about/awards",{"title":39,"links":40},"impact and progress",[41,44],{"text":42,"url":43,"target":27,"arialabel":28},"frontiers' impact","https://www.frontiersin.org/about/impact",{"text":45,"url":46,"target":27,"arialabel":28},"our annual reports","https://www.frontiersin.org/about/annual-reports",{"title":48,"links":49},"publishing model",[50,53,56,59,62,65,68],{"text":51,"url":52,"target":27,"arialabel":28},"how we publish","https://www.frontiersin.org/about/how-we-publish",{"text":54,"url":55,"target":27,"arialabel":28},"open access","https://www.frontiersin.org/about/open-access",{"text":57,"url":58,"target":27,"arialabel":28},"peer review","https://www.frontiersin.org/about/peer-review",{"text":60,"url":61,"target":27,"arialabel":28},"research integrity","https://www.frontiersin.org/about/research-integrity",{"text":63,"url":64,"target":27,"arialabel":28},"research topics","https://www.frontiersin.org/about/research-topics",{"text":66,"url":67,"target":27,"arialabel":28},"fair² data management","https://www.frontiersin.org/about/fair-data-management",{"text":69,"url":70,"target":27,"arialabel":28},"fee policy","https://www.frontiersin.org/about/fee-policy",{"title":72,"links":73},"services",[74,78,81,84],{"text":75,"url":76,"target":77,"arialabel":28},"societies","https://publishingpartnerships.frontiersin.org/","_blank",{"text":79,"url":80,"target":27,"arialabel":28},"national consortia","https://www.frontiersin.org/open-access-agreements/consortia",{"text":82,"url":83,"target":27,"arialabel":28},"institutional partnerships","https://www.frontiersin.org/about/open-access-agreements",{"text":85,"url":86,"target":27,"arialabel":28},"collaborators","https://www.frontiersin.org/about/collaborators",{"title":88,"links":89},"more from frontiers",[90,94,97,101,105,109],{"text":91,"url":92,"target":77,"arialabel":93},"frontiers forum","https://forum.frontiersin.org/","this link will take you to the frontiers forum website",{"text":95,"url":96,"target":27,"arialabel":28},"frontiers planet prize","https://www.frontiersin.org/about/frontiers-planet-prize",{"text":98,"url":99,"target":77,"arialabel":100},"press office","https://pressoffice.frontiersin.org/","this link will take you to the frontiers press office website",{"text":102,"url":103,"target":27,"arialabel":104},"sustainability","https://www.frontiersin.org/about/sustainability","link to information about frontiers' sustainability",{"text":106,"url":107,"target":77,"arialabel":108},"career opportunities","https://careers.frontiersin.org/","this link will take you to the frontiers careers website",{"text":110,"url":111,"target":27,"arialabel":112},"contact us","https://www.frontiersin.org/about/contact","this link will take you to the help pages to contact our support team","https://www.frontiersin.org/submission/submit?domainid=1&fieldid=66&specialtyid=0&entitytype=2&entityid=373",{"id":115,"name":116,"slug":117,"sections":118},373,"frontiers in plant science","plant-science",[119,123,127,131,135,139,143,147,151,155,159,163,167,171,175,179,183,187,191,195,199,203,207,211],{"id":120,"name":121,"slug":122},1553,"aquatic photosynthetic organisms","aquatic-photosynthetic-organisms",{"id":124,"name":125,"slug":126},1356,"crop and product physiology","crop-and-product-physiology",{"id":128,"name":129,"slug":130},467,"functional plant ecology","functional-plant-ecology",{"id":132,"name":133,"slug":134},2844,"functional and applied plant genomics","functional-and-applied-plant-genomics",{"id":136,"name":137,"slug":138},2843,"photosynthesis and photobiology","photosynthesis-and-photobiology",{"id":140,"name":141,"slug":142},1312,"plant abiotic stress","plant-abiotic-stress",{"id":144,"name":145,"slug":146},2183,"plant bioinformatics","plant-bioinformatics",{"id":148,"name":149,"slug":150},589,"plant biophysics and modeling","plant-biophysics-and-modeling",{"id":152,"name":153,"slug":154},560,"plant biotechnology","plant-biotechnology",{"id":156,"name":157,"slug":158},468,"plant breeding","plant-breeding",{"id":160,"name":161,"slug":162},577,"plant cell biology","plant-cell-biology",{"id":164,"name":165,"slug":166},474,"plant development and evodevo","plant-development-and-evodevo",{"id":168,"name":169,"slug":170},2845,"plant genetics, epigenetics and chromosome biology","plant-genetics-epigenetics-and-chromosome-biology",{"id":172,"name":173,"slug":174},479,"plant membrane traffic and transport","plant-membrane-traffic-and-transport",{"id":176,"name":177,"slug":178},481,"plant metabolism and chemodiversity","plant-metabolism-and-chemodiversity",{"id":180,"name":181,"slug":182},486,"plant nutrition","plant-nutrition",{"id":184,"name":185,"slug":186},485,"plant pathogen interactions","plant-pathogen-interactions",{"id":188,"name":189,"slug":190},226,"plant physiology","plant-physiology",{"id":192,"name":193,"slug":194},580,"plant proteomics and protein structural biology","plant-proteomics-and-protein-structural-biology",{"id":196,"name":197,"slug":198},1570,"plant symbiotic interactions","plant-symbiotic-interactions",{"id":200,"name":201,"slug":202},1428,"plant systematics and evolution","plant-systematics-and-evolution",{"id":204,"name":205,"slug":206},483,"plant systems and synthetic biology","plant-systems-and-synthetic-biology",{"id":208,"name":209,"slug":210},2277,"sustainable and intelligent phytoprotection","sustainable-and-intelligent-phytoprotection",{"id":212,"name":213,"slug":214},484,"technical advances in plant science","technical-advances-in-plant-science","sections",[217,241],{"title":218,"links":219},"scope",[220,223,226,229,232,235,238],{"text":221,"url":222,"target":27,"arialabel":28},"field chief editors","https://www.frontiersin.org/journals/plant-science/about#about-editors",{"text":224,"url":225,"target":27,"arialabel":28},"mission & scope","https://www.frontiersin.org/journals/plant-science/about#about-scope",{"text":227,"url":228,"target":27,"arialabel":28},"facts","https://www.frontiersin.org/journals/plant-science/about#about-facts",{"text":230,"url":231,"target":27,"arialabel":28},"journal sections","https://www.frontiersin.org/journals/plant-science/about#about-submission",{"text":233,"url":234,"target":27,"arialabel":28},"open access statement","https://www.frontiersin.org/journals/plant-science/about#about-open",{"text":236,"url":237,"target":27,"arialabel":28},"copyright statement","https://www.frontiersin.org/journals/plant-science/about#copyright-statement",{"text":239,"url":240,"target":27,"arialabel":28},"quality","https://www.frontiersin.org/journals/plant-science/about#about-quality",{"title":242,"links":243},"for authors",[244,247,250,253,256,259,262],{"text":245,"url":246,"target":27,"arialabel":28},"why submit?","https://www.frontiersin.org/journals/plant-science/for-authors/why-submit",{"text":248,"url":249,"target":27,"arialabel":28},"article types","https://www.frontiersin.org/journals/plant-science/for-authors/article-types",{"text":251,"url":252,"target":27,"arialabel":28},"author guidelines","https://www.frontiersin.org/journals/plant-science/for-authors/author-guidelines",{"text":254,"url":255,"target":27,"arialabel":28},"editor guidelines","https://www.frontiersin.org/journals/plant-science/for-authors/editor-guidelines",{"text":257,"url":258,"target":27,"arialabel":28},"publishing fees","https://www.frontiersin.org/journals/plant-science/for-authors/publishing-fees",{"text":260,"url":261,"target":27,"arialabel":28},"submission checklist","https://www.frontiersin.org/journals/plant-science/for-authors/submission-checklist",{"text":263,"url":264,"target":27,"arialabel":28},"contact editorial office","https://www.frontiersin.org/journals/plant-science/for-authors/contact-editorial-office",[266,269],{"text":267,"url":268,"target":27,"arialabel":28},"all journals","https://www.frontiersin.org/journals",{"text":270,"url":271,"target":27,"arialabel":28},"all articles","https://www.frontiersin.org/articles",[273,276,278],{"text":274,"url":275,"target":27,"arialabel":28},"articles","articles",{"text":63,"url":277,"target":27,"arialabel":28},"research-topics",{"text":279,"url":280,"target":27,"arialabel":28},"editorial board","editors",{"text":282,"url":283,"target":77,"arialabel":282},"help center","https://helpcenter.frontiersin.org",{"blocks":285,"sociallinks":339,"copyright":363,"termsandconditionsurl":364,"privacypolicyurl":365},[286,300,310,324],{"title":287,"links":288},"guidelines",[289,291,294,297,299],{"text":251,"url":290,"target":27,"arialabel":28},"https://www.frontiersin.org/guidelines/author-guidelines",{"text":292,"url":293,"target":27,"arialabel":28},"services for authors","https://www.frontiersin.org/for-authors/author-services",{"text":295,"url":296,"target":27,"arialabel":28},"policies and publication ethics","https://www.frontiersin.org/guidelines/policies-and-publication-ethics",{"text":254,"url":298,"target":27,"arialabel":28},"https://www.frontiersin.org/guidelines/editor-guidelines",{"text":69,"url":70,"target":27,"arialabel":28},{"title":301,"links":302},"explore",[303,304,307,309],{"text":274,"url":271,"target":27,"arialabel":28},{"text":305,"url":306,"target":27,"arialabel":28},"research topics ","https://www.frontiersin.org/research-topics",{"text":308,"url":268,"target":27,"arialabel":28},"journals",{"text":51,"url":52,"target":27,"arialabel":28},{"title":311,"links":312},"outreach",[313,316,319,323],{"text":314,"url":92,"target":77,"arialabel":315},"frontiers forum ","frontiers forum website",{"text":317,"url":318,"target":77,"arialabel":28},"frontiers policy labs ","https://policylabs.frontiersin.org/",{"text":320,"url":321,"target":77,"arialabel":322},"frontiers for young minds","https://kids.frontiersin.org/","frontiers for young minds journal",{"text":95,"url":96,"target":27,"arialabel":28},{"title":325,"links":326},"connect",[327,328,332,335,338],{"text":282,"url":283,"target":77,"arialabel":282},{"text":329,"url":330,"target":77,"arialabel":331},"emails and alerts ","https://subscription-management.frontiersin.org/emails/preferences#block0","subscribe to frontiers emails",{"text":333,"url":111,"target":27,"arialabel":334},"contact us ","subscribe to newsletter",{"text":336,"url":337,"target":27,"arialabel":28},"submit","https://www.frontiersin.org/submission/submit",{"text":106,"url":107,"target":77,"arialabel":28},[340,348,353,358],{"link":341,"type":344,"color":345,"icon":346,"size":347,"hiddentext":13},{"text":342,"url":343,"target":77,"arialabel":342},"frontiers facebook","https://www.facebook.com/frontiersin","link","grey","facebook","medium",{"link":349,"type":344,"color":345,"icon":352,"size":347,"hiddentext":13},{"text":350,"url":351,"target":77,"arialabel":28},"frontiers twitter","https://twitter.com/frontiersin","twitter",{"link":354,"type":344,"color":345,"icon":357,"size":347,"hiddentext":13},{"text":355,"url":356,"target":77,"arialabel":28},"frontiers linkedin","https://www.linkedin.com/company/frontiers","linkedin",{"link":359,"type":344,"color":345,"icon":362,"size":347,"hiddentext":13},{"text":360,"url":361,"target":77,"arialabel":28},"frontiers instagram","https://www.instagram.com/frontiersin_","instagram","frontiers media s.a. all rights reserved","https://www.frontiersin.org/legal/terms-and-conditions","https://www.frontiersin.org/legal/privacy-policy",{},false,{"__typename":369,"identifier":115,"name":116,"slug":117,"banner":370,"description":427,"mission":428,"palette":429,"impactfactor":430,"citescore":431,"citations":432,"showtagline":28,"twitter":433},"journal",[371],{"id":372,"src":373,"name":374,"tags":375,"type":385,"width":386,"height":387,"idhash":388,"archive":389,"brandid":390,"limited":389,"filesize":391,"ispublic":392,"original":393,"copyright":394,"extension":395,"thumbnails":397,"datecreated":405,"description":406,"orientation":407,"usercreated":408,"watermarked":389,"datemodified":405,"datepublished":409,"ecsarchivefiles":410,"propertyoptions":411,"property_channel":416,"property_sub-type":418,"property_asset_type":420,"activeoriginalfocuspoint":422,"property_office_department":425},"450e9326-0272-405c-b8d614c72bed9f89","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/webimage-00a7f7fd-61fc-4329-b3bfcc0119b4b276.jpg","fpls_main visual_green_website",[376,377,378,379,380,381,382,383,384],"medical","vitality","decoration","five","cosmetic","r","cure","aloe vera","spike","image",4928,3264,"720507a162925515",0,"22c10171-81b3-4da6-99342f272a32e8bb",11359471,1,"https://brand.frontiersin.org/m/720507a162925515/original/fpls_main-visual_green_website.jpeg","copyright (c) 2017 sabine hortebusch/shutterstock. no use without permission.",[396],"jpeg",{"mini":398,"thul":399,"webimage":373,"guidelines":400,"websitejpg_xl":401,"websitewebp_l":402,"websitewebp_m":403,"websitewebp_xl":404},"https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/mini-006feee0-2d70-4630-b9abfb0a0fa410aa.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/thul-c817db1b-00e8-4b29-b71c4e5a6058947f.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/guidelines-fpls_main visual_green_website.png","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitejpg_xl-fpls_main visual_green_website.jpg","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_l-fpls_main visual_green_website.webp","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_m-fpls_main visual_green_website.webp","https://d2csxpduxe849s.cloudfront.net/media/e32629c6-9347-4f84-81feaef7bfa342b3/450e9326-0272-405c-b8d614c72bed9f89/52f2110a-1caa-43c0-be84f352d8ab0835/websitewebp_xl-fpls_main visual_green_website.webp","2022-06-27t10:00:48z","spiral aloe vera with water drops, closeup","landscape","caroline sutter","2022-06-27t09:27:09z",[],[412,413,414,415],"414fb2d4-2283-43fd-be14e534eca67928","6c18119b-14bd-4951-b437696f4357bd33","7c692885-db25-4858-b1fb4ff47b241e9b","d88c0047-ec30-4506-a7df28a4d765e1cf",[417],"frontiersin_org",[419],"main_visual",[421],"photography",{"x":423,"y":424},2464,1632,[426],"publishing","the most cited plant science journal, advancing our understanding of plant biology for sustainable food security, functional ecosystems and human health.","\u003cp>frontiers in plant science is a leading, multidisciplinary journal that seeks to advance our understanding of fundamental processes in plant biology.\u003c/p>\n\n\u003cp>led by field chief editor prof. chun-ming liu (institute of botany, chinese academy of sciences) and indexed in pubmed, pubmed central, and scopus, among others, the journal seeks original and significant contributions that cultivate plant biology and its applications. the journal has the long-term goal of supporting sustainable development, food security, functional ecosystems, biotechnology (including biofuels and biomaterials), and human health.\u003c/p>\n\u003cp>frontiers in plant science welcomes original research, review, opinion, and perspective articles, among other submission types, covering the journal’s specialty sections:\u003c/p>\n\n\u003cdiv> &bull; aquatic photosynthetic organisms\u003c/div>\n\u003cdiv> &bull; crop and product physiology\u003c/div>\n\u003cdiv> &bull; functional plant ecology\u003c/div>\n\u003cdiv> &bull; functional and applied plant genomics\u003c/div>\n\u003cdiv> &bull; photosynthesis and photobiology\u003c/div>\n\u003cdiv> &bull; plant abiotic stress\u003c/div>\n\u003cdiv> &bull; plant bioinformatics\u003c/div>\n\u003cdiv> &bull; plant biophysics and modeling\u003c/div>\n\u003cdiv> &bull; plant biotechnology\u003c/div>\n\u003cdiv> &bull; plant breeding\u003c/div>\n\u003cdiv> &bull; plant cell biology\u003c/div>\n\u003cdiv> &bull; plant development and evodevo\u003c/div>\n\u003cdiv> &bull; plant genetics, epigenetics and chromosome biology\u003c/div>\n\u003cdiv> &bull; plant membrane traffic and transport\u003c/div>\n\u003cdiv> &bull; plant metabolism and chemodiversity\u003c/div>\n\u003cdiv> &bull; plant nutrition\u003c/div>\n\u003cdiv> &bull; plant pathogen interactions\u003c/div>\n\u003cdiv> &bull; plant physiology\u003c/div>\n\u003cdiv> &bull; plant proteomics and protein structural biology\u003c/div>\n\u003cdiv> &bull; plant symbiotic interactions\u003c/div>\n\u003cdiv> &bull; plant systematics and evolution\u003c/div>\n\u003cdiv> &bull; plant systems and synthetic biology\u003c/div>\n\u003cdiv> &bull; sustainable and intelligent phytoprotection\u003c/div>\n\u003cdiv> &bull; technical advances in plant science.\u003c/div>\n\u003cbr>\n\u003cp>furthermore, the journal welcomes submissions that support and advance the un's sustainable development goals (sdgs), notably sdg 13: climate action and sdg 15: life on land.\u003c/p> \n\n\u003cp>frontiers in plant science is committed to advancing developments in the field of plant biology by allowing unrestricted access to articles and communicating scientific knowledge to researchers and the public alike, to enable the scientific breakthroughs of the future.\u003c/p> \n \n\u003cp>requirements\u003cp>\n\u003cp>manuscripts that focus on non-plant-related microbiology, human or animal genetics, and medical and pharmacological research are not suitable for publication in this journal. pure field agriculture studies such as those focusing on fertilizer application or yield optimization, without relevance to plant science, are also not within the scope of this journal.\u003c/p>\nstudies falling into the categories below will not be considered for review in this journal unless they are expanded and provide insight into the biological process being studied:\u003c/p>\n\n\u003cp>i) descriptive collections of transcripts, proteins, or metabolites, including comparative sets as a result of different conditions or treatments;\u003c/p>\n\u003cp>ii) descriptive studies that define gene families using pure phylogenetics and the assignment of cursory functional attributions (e.g. expression profiles, promoter analysis, and bioinformatic parameters).\u003c/p>\n\n\u003cp>quantitative analysis needs to be performed on a minimum of three biological replicates in order to enable an assessment of significance. this includes quantitative omics studies (transcriptomics, proteomics, metabolomics) as well as phenotypic measurements, quantitative assays, and qpcr expression analysis. studies that do not comply with these replication requirements will not be considered for review.\u003cp>\n\n\u003cp>studies using transgenic or mutant lines (plants and algae), for example, t-dna, transposon, rnai, crispr/cas9, chemically induced, overexpressors and reporter fusions (gus, gfps, luc), should be based on data from multiple alleles (minimum of two) displaying a common and stable phenotype. qualitative data can be presented from a single allele but should be indicative of observations from multiple alleles which should be explicitly stated in the text. quantitative data should be derived from multiple alleles (at least two) and should be displayed separately for each allele (with at least three independent replications for each allele). studies reporting single alleles may be considered acceptable when:\u003c/p>\n\n\u003cp>i) complementation via transformation is used for confirmation;\u003c/p> \n\u003cp>ii) the allele has been previously characterized and published, and is representative of multiple independent lines;\u003c/p>\n\u003cp>iii) in situations where genetic transformation is difficult or not yet possible, alternative evidence should be presented\u003c/p>\n\n","green","5.6","7.1","927148","@frontplantsci",{"id":115,"name":116,"slug":117,"abbreviation":435,"isonline":13,"isopenforsubmissions":13,"citescore":436,"impactfactor":437},"fpls",8.8,4.8,{"displaytitlepilllabels":13,"displayrelatedarticlesbox":13,"showeditors":13,"showreviewers":13,"showloopimpactlink":13,"enablefigshare":367,"usexmlimages":13},{"ispublic":13,"allowcompanyusers":367,"whitelistemails":440,"enablealljournals":13,"whitelistjournals":462},[441,442,443,444,445,446,447,448,449,446,450,451,452,453,454,455,456,457,458,459,460,461],"y2fybg9zlmxvcmnhqgzyb250awvyc2lulm9yzw==","zgfuawvslmx1ekbmcm9udgllcnnpbi5vcmc=","bgf1cmeudgvuyubmcm9udgllcnnpbi5vcmc=","cmfmywvslnjpyw5jag9aznjvbnrpzxjzaw4ub3jn","amftzxmuc21hbgxib25lqgzyb250awvyc2lulm9yzw==","znjhbi5tb3jlbm9aznjvbnrpzxjzaw4ub3jn","c2ftdwvslmzlcm5hbmrlekbmcm9udgllcnnpbi5vcmc=","ymfyymfyys5jyxjjyw5naxvaznjvbnrpzxjzaw4ub3jn","axzhbi5zyw50yw1hcmlhqgzyb250awvyc2lulm9yzw==","ahvllnryyw5aznjvbnrpzxjzaw4ub3jn","z29uy2fsby52yxjnyxnaznjvbnrpzxjzaw4ub3jn","bhvjawuuc2vubkbmcm9udgllcnnpbi5vcmc=","bg9ybi5mcmfzzxjaznjvbnrpzxjzaw4ub3jn","zwxzys5jyxjyb25aznjvbnrpzxjzaw4ub3jn","bwfhcnrlbi52yw5kawpja0bmcm9udgllcnnpbi5vcmc=","ymluzgh1lmtyaxnobmfuqgzyb250awvyc2lulm9yzw==","bwf0dghldy5hdhr3yxrlcnnaznjvbnrpzxjzaw4ub3jn","z2l1bglhlnzhbhnly2noaubmcm9udgllcnnpbi5vcmc=","zmfiawfulmrlbgxhbw9ydgvaznjvbnrpzxjzaw4ub3jn","dmlqyxlhbi5wcebwaxrzb2x1dglvbnmuy29t","z3vpbgxhdw1llnnldxjhdebmcm9udgllcnnpbi5vcmc=",[463,464,465,466,467,468,392,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498],2232,1729,2357,2456,2176,2333,1843,602,106,616,310,657,3123,1468,2137,628,1566,2726,2086,1490,451,141,1335,2655,1238,604,698,1547,176,1440,403,1239,755,2136,609,1534,{"spaceid":392,"name":392,"availablejournalpages":500,"announcement":504},[275,280,277,501,502,503],"volumes","about","community-reviewers",{"__typename":505,"sys":506,"preheader":42,"title":508,"description":509,"image":510,"link":532},"announcement",{"id":507},"5ittnttqgazppgh6rx0alm","articles published with frontiers have received 12 million total citations","your research is the real superpower - learn how we maximise its impact through our leading community journals",[511],{"archive":389,"brandid":390,"copyright":28,"datecreated":512,"datemodified":513,"datepublished":514,"description":28,"extension":515,"filesize":517,"height":518,"id":519,"ispublic":389,"limited":389,"name":520,"orientation":407,"original":28,"thumbnails":521,"type":385,"watermarked":389,"width":528,"videopreviewurls":529,"tags":530,"textmetaproperties":531,"src":522},"2025-06-18t12:58:01z","2025-06-18t14:15:34z","2025-06-18t12:57:32z",[516],"png",1365026,920,"7c5269c4-3c83-4591-951a74d15b95daea","impact metrics banner for article pages",{"webimage":522,"thul":523,"mini":524,"websitewebp_l":525,"websitewebp_m":526,"guidelines":527},"https://brand.frontiersin.org/m/59ee6275cb9a849c/webimage-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/m/59ee6275cb9a849c/thul-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/m/59ee6275cb9a849c/mini-impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/websitewebp_l/impact-metrics-banner-for-article-pages.webp","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/websitewebp_m/impact-metrics-banner-for-article-pages.webp","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/guidelines/impact-metrics-banner-for-article-pages.png",1600,[],[],[],{"text":533,"url":43,"target":27,"arialabel":28},"explore our impact metrics",{"loggeduserinfo":-1},{"currentarticle":536,"ispreviewpage":367,"hassupplementaldata":367,"showcrossmarkwidget":13,"articletemplate":648,"currentarticlepagemetainfo":649,"xmlparsedarticlecontent":-1,"xmlparsingerror":-1},{"id":537,"doi":538,"title":539,"acceptancedate":540,"receptiondate":541,"publicationdate":542,"lastmodifieddate":543,"ispublished":13,"abstract":544,"researchtopic":545,"articletype":551,"stage":554,"keywords":556,"authors":563,"editors":587,"reviewers":595,"journal":610,"section":617,"impactmetrics":619,"volume":622,"articlevolume":623,"relatedarticles":624,"ispublishedv2":13,"contents":625,"files":628},1641677,"10.3389/fpls.2025.1641677","a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution","2025-09-09t14:48:37.000z","2025-06-05t09:39:42.000z","2025-10-03t00:00:00.000z","2025-10-21t02:25:15.907z","walnut leaf brown spot disease, caused by ophiognomonia leptostyla, is among the most destructive fungal diseases in walnut cultivation. in the development of smart agriculture, precision grading of plant diseases remains a core technical challenge; specifically, this disease is plagued by blurred lesion edges and inefficient extraction of complex features, which directly limits the accurate grading of the disease. to address these issues, this study proposes a disease grading method integrating hierarchical feature selection and adaptive multi-scale dilated convolution, and develops the cogfuse-mobilevit model. this model overcomes the limitations of the standard mobilevitv3 model in capturing blurred edges of tiny lesions via three innovative modules: specifically, the hierarchical feature screening module (hfsm) enables hierarchical screening of disease-related features; the edge feature focus module (ecfm) works in synergy with the hfsm to enhance the focus on lesion edge features; and the adaptive multi-scale dilated convolution fusion module (amsdidcm) achieves dynamic multi-scale fusion of lesion textures and global structures. experimental results demonstrate that the proposed model achieves an accuracy of 86.61% on the test set, representing an improvement of 7.8 percentage points compared with the original mobilevitv3 model and significantly outperforming other mainstream disease grading models. this study confirms that the cogfuse-mobilevit model can effectively resolve the issues of blurred edges and inefficient feature extraction in this disease, provides a reliable technical solution for its precision grading, and holds practical application value for the intelligent diagnosis of plant diseases in smart agriculture.",{"id":546,"title":547,"articlescount":548,"ismagazinepage":367,"slug":549,"isopenforsubmission":13,"views":550},66487,"innovative field diagnostics for real-time plant pathogen detection and management",9,"innovative-field-diagnostics-for-real-time-plant-pathogen-detection-and-management",14940,{"id":552,"name":553},24,"original research",{"id":555,"name":19},18,[557,558,559,560,561,562],"walnut","brown spot disease (ophiognomonia leptostyla)","hierarchical feature selection","edge features perception","adaptive multi-scale dilated convolution","disease grading",[564,575,581],{"id":565,"firstname":566,"middlename":19,"lastname":567,"givennames":568,"iscorresponding":367,"isprofilepublic":13,"userid":565,"email":-1,"affiliations":569},3089871,"yuting","wei","yuting ",[570,573],{"organizationname":571,"countryname":572,"cityname":19,"statename":19,"zipcode":19},"college of information engineering, tarim university","china",{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},"key laboratory of tarim oasis agriculture, ministry of education, tarim university",{"id":389,"firstname":576,"middlename":19,"lastname":577,"givennames":578,"iscorresponding":367,"isprofilepublic":367,"userid":389,"email":-1,"affiliations":579},"debin","zeng","debin ",[580],{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},{"id":389,"firstname":582,"middlename":19,"lastname":583,"givennames":584,"iscorresponding":13,"isprofilepublic":367,"userid":389,"email":19,"affiliations":585},"liangfang","zheng","liangfang ",[586],{"organizationname":574,"countryname":572,"cityname":19,"statename":19,"zipcode":19},[588],{"id":589,"firstname":590,"middlename":19,"lastname":591,"givennames":592,"iscorresponding":367,"isprofilepublic":13,"userid":589,"email":-1,"affiliations":593},1037951,"ravinder","kumar","ravinder ",[594],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},[596,603],{"id":597,"firstname":598,"middlename":19,"lastname":599,"givennames":600,"iscorresponding":367,"isprofilepublic":13,"userid":597,"email":-1,"affiliations":601},2082015,"geza","bujdoso","geza ",[602],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},{"id":604,"firstname":605,"middlename":19,"lastname":606,"givennames":607,"iscorresponding":367,"isprofilepublic":13,"userid":604,"email":-1,"affiliations":608},3078480,"vasudha","vedula","vasudha ",[609],{"organizationname":19,"countryname":19,"cityname":19,"statename":19,"zipcode":19},{"id":115,"slug":117,"name":116,"shortname":611,"electronicissn":612,"field":613,"specialtyid":28,"journalsectionpaths":615},"front. plant sci.","1664-462x",{"id":614,"domainid":392},66,[616],{"section":617},{"id":208,"name":209,"slug":210,"specialtyid":618},2685,{"views":620,"downloads":621,"citations":389},1063,103,16,"volume 16 - 2025",[],{"titlehtml":539,"fulltexthtml":626,"menuhtml":627},"\u003cdiv class=\"journalabstract\">\u003ca id=\"h1\" name=\"h1\">\u003c/a>\u003cdiv class=\"authors\">\u003cspan class=\"author-wrapper notranslate\">\u003ca href=\"https://loop.frontiersin.org/people/3089871/overview\" class=\"user-id-3089871/overview\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/images/profile/3089871/overview/74\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\" alt=\"yuting wei,&#x;\">yuting wei\u003c/a>\u003csup>1,2&#x2020;\u003c/sup>\u003c/span>\u003cspan class=\"author-wrapper notranslate\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/cdn/images/profile/default_32.jpg\" alt=\"debin zeng&#x;\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\">debin zeng\u003csup>2&#x2020;\u003c/sup>\u003c/span>\u003cspan class=\"author-wrapper notranslate\">\u003cimg class=\"pr5\" src=\"https://loop.frontiersin.org/cdn/images/profile/default_32.jpg\" alt=\"liangfang zheng*\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\">liangfang zheng\u003csup>2*\u003c/sup>\u003c/span>\u003c/div>\u003cul class=\"notes\">\u003cli>\u003cspan>\u003csup>1\u003c/sup>\u003c/span>college of information engineering, tarim university, alaer, china\u003c/li>\u003cli>\u003cspan>\u003csup>2\u003c/sup>\u003c/span>key laboratory of tarim oasis agriculture, ministry of education, tarim university, alaer, china\u003c/li>\u003c/ul>\u003cp>walnut leaf brown spot disease, caused by \u003ci>ophiognomonia leptostyla\u003c/i>, is among the most destructive fungal diseases in walnut cultivation. in the development of smart agriculture, precision grading of plant diseases remains a core technical challenge; specifically, this disease is plagued by blurred lesion edges and inefficient extraction of complex features, which directly limits the accurate grading of the disease. to address these issues, this study proposes a disease grading method integrating hierarchical feature selection and adaptive multi-scale dilated convolution, and develops the cogfuse-mobilevit model. this model overcomes the limitations of the standard mobilevitv3 model in capturing blurred edges of tiny lesions via three innovative modules: specifically, the hierarchical feature screening module (hfsm) enables hierarchical screening of disease-related features; the edge feature focus module (ecfm) works in synergy with the hfsm to enhance the focus on lesion edge features; and the adaptive multi-scale dilated convolution fusion module (amsdidcm) achieves dynamic multi-scale fusion of lesion textures and global structures. experimental results demonstrate that the proposed model achieves an accuracy of 86.61% on the test set, representing an improvement of 7.8 percentage points compared with the original mobilevitv3 model and significantly outperforming other mainstream disease grading models. this study confirms that the cogfuse-mobilevit model can effectively resolve the issues of blurred edges and inefficient feature extraction in this disease, provides a reliable technical solution for its precision grading, and holds practical application value for the intelligent diagnosis of plant diseases in smart agriculture.\u003c/p>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"journalfulltext\">\u003ca id=\"h2\" name=\"h2\">\u003c/a>\u003ch2>1 introduction\u003c/h2>\u003cp class=\"mb15\">walnut diseases pose a serious threat to walnut production in xinjiang. the spread of crop diseases significantly exacerbates the security risks of the walnut industry if timely prevention and control measures are not taken (\u003ca href=\"#b7\">cooke, 2006\u003c/a>; \u003ca href=\"#b12\">khan et&#xa0;al., 2021\u003c/a>). as a core component of the disease prevention and control system, early precise grading plays a critical role in agricultural production management (\u003ca href=\"#b2\">adaskaveg et&#xa0;al., 2009\u003c/a>; \u003ca href=\"#b6\">chiang et&#xa0;al., 2016\u003c/a>; \u003ca href=\"#b14\">lamichhane, 2014\u003c/a>). walnut brown spot, caused primarily by the fungus \u003ci>ophiognomonia leptostyla\u003c/i>, typically forms specific symptoms on organs such as leaves, flowers, and fruits. among them, leaves, as the primary carrier of plant diseases, exhibit characteristics such as lesion morphology and color changes on their surfaces, which often serve as important bases for disease grading (\u003ca href=\"#b8\">ghaiwat and arora, 2014\u003c/a>; \u003ca href=\"#b35\">weber, 1980\u003c/a>; \u003ca href=\"#b39\">zarei et&#xa0;al., 2019\u003c/a>). traditional disease identification relies on manual experience (\u003ca href=\"#b21\">moragrega et&#xa0;al., 2011\u003c/a>; \u003ca href=\"#b34\">wang et&#xa0;al., 2020\u003c/a>), a method that is not only time- color system to reduce interference from light and leaf veins, and lesion edge detection using the sobel operator, ultimately achieving fast and accurate grading based on the ratio of lesion area to leaf area. \u003ca href=\"#b10\">jadhav and patil (2016)\u003c/a> developed a leaf image partitioning technique based on k-means clustering and squared euclidean distance, automatically quantifying damaged leaf area through the pixel ratio of lesions to leaves for disease grading. \u003ca href=\"#b3\">arivazhagan et al. (2013)\u003c/a> proposed a four-step processing system involving color conversion, green pixel removal, segmentation, and texture feature classification for leaf disease grading. however, traditional methods for acquiring disease information suffer from insufficient segmentation accuracy for plant leaf disease images with complex textures and unclear lesion boundaries, thereby resulting in poor disease severity grading performance.\u003c/p>\u003cp class=\"mb15\">with the continuous advancement of computational power, deep learning has increasingly become a key technology for addressing complex lesion segmentation, primarily due to its superior capability in automatic feature extraction (\u003ca href=\"#b18\">mao et&#xa0;al., 2023\u003c/a>). \u003ca href=\"#b5\">chen et al. (2021)\u003c/a> proposed the blsnet method based on the unet semantic segmentation network, enhancing lesion segmentation accuracy through the introduction of attention mechanisms and multi-scale feature fusion. experiments showed that its segmentation and classification accuracy outperformed benchmark models such as deeplabv3+ and unet, preliminarily verifying the reliability of this method in automatic assessment of bls disease severity. \u003ca href=\"#b22\">ngugi et al. (2020)\u003c/a> developed the kijaninet segmentation network based on fully convolutional neural networks, demonstrating excellent performance in tomato leaf segmentation under complex backgrounds; \u003ca href=\"#b16\">lin et al. (2019)\u003c/a> developed a cnn semantic segmentation model that achieved high-precision pixel-level segmentation of cucumber powdery mildew on leaves. additionally, some studies have fused traditional features with deep learning: \u003ca href=\"#b31\">tripathi (2021)\u003c/a> proposed a convolutional neural network method based on alexnet, achieving disease grading by fusing features extracted by the model with external leaf segmentation features; \u003ca href=\"#b17\">ma et al. (2017)\u003c/a> introduced comprehensive color features (ccf) combining hyper-red index, hsv/h and lab/b components, and achieved lesion segmentation via interactive region growing, with experiments confirming that this method enables accurate grading of disease images such as cucumber downy mildew under actual field conditions. however, although such methods have improved grading accuracy to some extent, they still suffer from insufficient feature capture of blurred disease edges, making it difficult to achieve fine-grained differentiation of subtle differences between disease grades (\u003ca href=\"#b26\">rastogi et&#xa0;al., 2015\u003c/a>).\u003c/p>\u003cp class=\"mb15\">in recent years, the application of deep learning in plant disease classification has continued to expand. \u003ca href=\"#b24\">parashar et al. (2024)\u003c/a> systematically validated the capability of complex feature modeling for crop yield prediction, further substantiating the critical role of adaptive feature extraction in agricultural intelligent decision-making. concurrently, \u003ca href=\"#b32\">vishnoi et al. (2022)\u003c/a> enhanced small-target detection precision in apple disease identification through a spatial attention mechanism-based cnn architecture for leaf disease diagnosis. \u003ca href=\"#b23\">ozturk et al. (2025)\u003c/a> constructed an ensemble learning classification model based on resnet50, mobile net, efficientnetb0, and densenet121, enhancing generalization performance through statistical cross-validation and improving decision interpretability via grad-cam visualization. experimental results showed that the ensemble model achieved stable high-precision classification performance across multiple validation rounds. these studies provide robust and interpretable solutions for intelligent plant disease recognition through technical integration and architectural innovation. \u003ca href=\"#b9\">hu et al. (2021)\u003c/a> integrated retinex enhancement, faster r-cnn detection, and vgg16 classification to improve the grading and detection accuracy of blurred diseased leaves. \u003ca href=\"#b25\">picon et al. (2019)\u003c/a> proposed an adaptive deep residual neural network algorithm for the classification of three european wheat diseases (septoria leaf blotch, brown spot, and rust) in real-world scenarios, which effectively improved the classification accuracy of wheat diseases. \u003ca href=\"#b13\">kim and ahn (2021)\u003c/a> employed deep cnn architectures such as resnet, xception, and densenet, combined with transfer learning and fine-tuning, to classify 9 categories of pests, diseases, and healthy states in tomatoes. although densenet combined with the rmsprop algorithm achieved an accuracy of 98.63%, single cnn architectures have clear bottlenecks in handling complex plant lesions and overlapping multiple diseases, including insufficient specificity in feature extraction and limited ability to distinguish subtle differences between lesions. \u003ca href=\"#b28\">shi et al. (2023)\u003c/a> analyzed the application of cnns in evaluating the severity of plant diseases, pointing out that hybrid architectures such as classical cnns, improved cnns, and segmentation networks have limitations in handling complex plant lesions: classification errors caused by concurrent multiple diseases, as well as constraints on model generalization ability due to unbalanced datasets and insufficient annotation accuracy. these issues significantly restrict their precise application in practical agricultural scenarios.\u003c/p>\u003cp class=\"mb15\">although significant advancements have been made in plant lesion segmentation and disease classification using deep learning, the grading task for walnut leaf brown spot disease&#x2014;characterized by blurred edge representation and complex small lesions&#x2014;still faces notable challenges. current mobilevit-based hybrid models fall into two categories: general architectures (mobilevitv1/v2/v3) focus on optimizing the cnn-transformer balance for natural images; domain-improved models (mobile-former/edgevit) are oriented toward medical/industrial tasks, with their task focus on localization rather than grading, which mismatches the pain points and fails to address the blurriness of agricultural lesions, tissue interference, and morphological variations. thus, the present study proposes a novel cogfuse-mobilevit model specifically designed for disease grading, with its specific contributions as follows:\u003c/p>\u003cp style=\"margin-top:1em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">1. a diverse field-collected walnut leaf dataset was constructed, with images divided into four distinct severity levels based on qualitative assessment of disease severity, ensuring the accuracy of disease severity grading.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">2. the hierarchical feature selection module (hfsm) enhances the fusion of local details (such as lesion texture and color) with global context through local and global attention mechanisms and task-driven feature selection, while suppressing interference from healthy regions.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:0em;margin-left:1em;text-indent:-1em;text-align:left\">3. the edge convolution fusion module (ecfm) strengthens edge details through sobel convolution and residual connections, achieving effective integration of edge-specific features with general features, further enhancing edge details and enabling more precise capture of lesion contour details.\u003c/p>\u003cp style=\"margin-top:0em;margin-bottom:1em;margin-left:1em;text-indent:-1em;text-align:left\">4. the adaptive multi-scale dilated dense inception convolution module (amsddicm) extracts differentiated features using multi-shaped convolution kernels and adaptively fuses multi-scale information via a dynamic weight mechanism, capturing lesion morphologies at different pathogenesis stages.\u003c/p>\u003ca id=\"h3\" name=\"h3\">\u003c/a>\u003ch2>2 materials and methods\u003c/h2>\u003ch3>2.1 characteristics of walnut leaf brown spot and classification of disease severity levels\u003c/h3>\u003cp class=\"mb0\">in the local standard technical regulations for prevention and control of walnut brown spot (\u003ca href=\"#b19\">mcgranahan and leslie, 1991\u003c/a>), walnut brown spot is divided into four severity levels. the severity of plant leaf diseases is generally determined using the spot coverage method. in this study, walnut leaves with different disease severities were processed to separate disease-infected spots from healthy leaf areas, and the percentage of lesion area to total leaf area was calculated. with reference to the local standard, the disease grades specified in the standard were re-determined through calculations in this study, as shown in \u003ca href=\"#t1\">table&#xa0;1\u003c/a>. classification of different severity levels of walnut leaf brown spot was conducted in accordance with technical regulations for prevention and control of walnut brown spot (\u003ca href=\"#b33\">wang et&#xa0;al., 2022\u003c/a>; \u003ca href=\"#b37\">yang et&#xa0;al., 2021\u003c/a>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;1\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" name=\"table&#xa0;1\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t001.jpg\" alt=\"www.frontiersin.org\" id=\"t1\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;1\u003c/b>. walnut leaf brown spot disease severity grading criteria.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">walnut leaf brown spot is caused by infection with the fungus \u003ci>ophiognomonia leptostyla\u003c/i> (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>). in the early infection stage, near-circular or irregular small spots appear on the leaves, with a gray-brown center and dark yellow-green to purplish-brown edges, and diseased leaves tend to fall off prematurely. in the middle stage, elongated elliptical or irregular slightly sunken dark brown lesions form, with larger spot size and light brown edges; longitudinal cracks are often present in the center of the lesions. in the late stage, lesions often coalesce to form large scorched necrotic areas, surrounded by yellow to golden-yellow zones, and small black granules (conidiomata and conidia of the pathogen) are scattered on the surface of the diseased tissue (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>). according to the degree of color and texture feature changes in infected leaves, walnut leaf disease is divided into four stages: healthy, early, middle, and late stages, corresponding to disease severity levels: healthy (level 0), mild (level 1), moderate (level 2), and severe (level 3). the different severity levels of walnut leaf brown spot are shown in \u003ca href=\"#f1\">figure&#xa0;1\u003c/a>.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;1\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g001.jpg\" alt=\"four leaves are displayed, labeled a to d. a shows a healthy leaf with no spots. b features a mildly infected leaf with a few small spots. c presents a moderately infected leaf with numerous spots. d shows a severely infected leaf covered with dark spots.\" id=\"f1\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;1\u003c/b>. walnut leaves infected with brown spot disease at different severity levels. \u003cb>(a)\u003c/b> healthy leaves; \u003cb>(b)\u003c/b> mildly infected leaves; \u003cb>(c)\u003c/b> moderately infected leaves; \u003cb>(d)\u003c/b> severely infected leaves.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">\u003ca href=\"#t1\">table&#xa0;1\u003c/a> shows the ratio of walnut leaf brown spot lesion area to total leaf area. level 0 (healthy): healthy leaves without symptoms. level 1 (mild): near-circular or irregular small white spots appear on leaves, accounting for less than 5% of the leaf area. level 2 (moderate): lesions expand into irregular dark brown spots, covering 5% to 30% of the leaf area. level 3 (severe): abundant lesion coalesce to form large scorched necrotic areas, exceeding 30% of the leaf area.\u003c/p>\u003ch3>2.2 calculation algorithm for walnut leaf brown spot disease severity levels\u003c/h3>\u003cp class=\"mb0\">after capturing walnut leaf brown spot samples using a camera, to minimize subjective bias, this study strictly adhered to the objective quantitative criteria defined in the technical regulations for the control of walnut brown leaf spot (\u003ca href=\"#t1\">table&#xa0;1\u003c/a>), utilizing the lesion area percentage (k) as the primary grading indicator. we re-determined the severity levels through computational analysis, with the specific determination process shown in \u003ca href=\"#f2\">figure&#xa0;2\u003c/a>.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;2\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g002.jpg\" alt=\"flowchart illustrating a process for identifying disease areas on leaves. images of leaves are converted through morphological color space to grayscale. the grayscale is used to highlight disease area s1 and leaf area s2. the ratio s1/s2 is applied, producing a final analysis image showing leaf disease marked in red.\" id=\"f2\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;2\u003c/b>. computational method for different severity levels of brown spot infection in walnut leaves.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">first, a python-based color segmentation algorithm was employed to convert images from the bgr color space to the hsv color space for processing. the hsv color space offers inherent advantages in color segmentation tasks. by analyzing the hue, saturation, and value characteristics of diseased regions, the color threshold range in the hsv space was determined (\u003ca href=\"#b4\">chaudhary et&#xa0;al., 2012\u003c/a>). after generating an initial disease region mask based on this threshold range, morphological operations such as closing and opening were applied to optimize mask quality, effectively eliminating noise while preserving the integrity of lesion contours. subsequently, the original image was converted to grayscale mode, and the leaf region was extracted using an adaptive threshold binarization algorithm. leaf boundaries were localized via contour detection technology, and a complete leaf mask was generated through contour filling. pixel statistical analysis was performed on both the leaf mask and disease mask to calculate the total leaf area and diseased region area, respectively. when a valid leaf area (&gt;0) was detected, the disease severity index was calculated using the following formula: disease severity index \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im1\">\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmo mathsize=\"10.5pt\">%\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">s\u003c/mi>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003c/mrow>\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">s\u003c/mi>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmn mathsize=\"10.5pt\">100\u003c/mn>\u003cmo mathsize=\"10.5pt\">%\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;&#xa0;&#xa0;s\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>: total area of diseased regions \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im2\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>: total area of the complete leaf.\u003c/p>\u003ch3>2.3 dataset construction\u003c/h3>\u003cp class=\"mb0\">data were collected from the walnut plantation base of tarim university (alar, xinjiang) between may and october 2024. leaves from three locally dominant early-fruiting cultivars (&#x2018;wen 185&#x2019;, &#x2018;xinxin 2&#x2019;, &#x2018;zha 343&#x2019;)widely cultivated in southern xinjiang were vertically photographed using an iphone 13. this genotypic diversity ensures model generalizability by encompassing varied disease phenotypes. the orchard follows standardized cultivation with 4m plant spacing, 5m row spacing (&#x2248;50 plants/mu), and conventional management practices. sampled trees were in full fruiting stage. to capture disease traits across microenvironments, samples were collected from safely accessible crown layers. the dataset includes healthy and brown spot-infected leaves three severity levels, spanning varied time periods, light conditions, and angles. after removing duplicates and invalid images,5,120 high-quality jpgs were retained for analysis.\u003c/p>\u003ch4>2.3.1 development of the walnut leaf brown spot dataset\u003c/h4>\u003cp class=\"mb0\">disease severity grading was established through quantitative lesion area analysis (\u003ca href=\"#f2\">figure&#xa0;2\u003c/a>). a representative subset of 512 images (10% of the full 5,120-image dataset) was selected via stratified random sampling, accurately preserving the original severity distribution. to ensure labeling rigor, two plant protection specialists (&gt;5 years&#x2019; expertise in walnut pathology, certified in local protocols) executed standardized annotation: pre-annotation training unified diagnostic criteria for ambiguous cases, with formal annotation commencing only after achieving pre-calibration inter-rater agreement (kappa coefficient &#x2265;0.75). as validated in \u003ca href=\"#t2\">table&#xa0;2\u003c/a>, dual statistical metrics confirmed gold-standard reliability&#x2014;inter-rater cohen&#x2019;s kappa reached 0.71, meeting landis &amp; koch&#x2019;s &#x201c;substantial agreement&#x201d; threshold; algorithm-expert consensus attained a weighted fleiss&#x2019; kappa of 0.77, substantially outperforming conventional cohen&#x2019;s kappa in multi-annotator scenarios. a three-stage standardization pipeline eliminated preprocessing discrepancies. the final dataset, partitioned into training/testing sets (8:2 ratio), strictly adheres to plant disease survey protocols and provides benchmarked data for deep learning model development (\u003ca href=\"#t3\">table&#xa0;3\u003c/a>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;2\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" name=\"table&#xa0;2\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t002.jpg\" alt=\"www.frontiersin.org\" id=\"t2\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;2\u003c/b>. algorithm vs. expert consensus agreement evaluation.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;3\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" name=\"table&#xa0;3\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t003.jpg\" alt=\"www.frontiersin.org\" id=\"t3\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;3\u003c/b>. walnut leaf brown spot disease image acquisition data.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>2.4 construction of walnut leaf brown spot disease severity grading model\u003c/h3>\u003ch4>2.4.1 the optimized mobilevitv3 network: cogfuse-mobilevit\u003c/h4>\u003cp class=\"mb15\">to address the challenge of difficult feature capture for small lesions in walnut leaf brown spot severity grading, this study proposes an innovative model, cogfuse-mobilevit. mobilevitv3 was selected as the backbone network due to its suitability for walnut brown spot grading. its hybrid mobilenet-vit architecture integrates cnn-based local feature extraction essential for lesion detail capture with transformer-enabled global contextual modeling critical for analyzing lesion spatial distribution. this design aligns with pathological requirements for severity grading, necessitating concurrent attention to local lesion characteristics and global infection patterns. the lightweight architecture further supports real-time processing on embedded devices, enabling future field deployment.\u003c/p>\u003cp class=\"mb0\">the model embodies a &#x201c;grading task-driven&#x201d; design principle. conceptually, it establishes a disease-specific framework of &#x201c;hierarchical screening to edge enhancement to dynamic fusion.&#x201d; this framework elevates general feature extraction to targeted solutions addressing three major agricultural challenges: suppressing interference from healthy tissues via hierarchical screening resolving feature confusion; strengthening blurred lesion contours through edge enhancement overcoming the bottleneck of edge blurriness; adaptively handling multi-stage morphological variations using dynamic fusion addressing morphological variation challenges. structurally, as illustrated in \u003ca href=\"#f3\">figure&#xa0;3\u003c/a>, the network implements a progressive feature extraction strategy. the hierarchical feature selection module (hfsm) first fuses shallow and middle-layer features. it employs a hierarchical attention mechanism to enhance semantic consistency while preserving spatial details. the edge convolution fusion module (ecfm) then processes these features. it utilizes learnable sobel operators to extract edge information, which is fused with conventional convolutional features via residual connections to augment edge perception capability. finally, the adaptive multi-scale dilated inception convolution module (amsddicm) enables adaptive processing of multi-scale edge features. this module is capable of both capturing fine edge changes in minute lesions and grasping the overall contour structure of large lesions, thereby comprehensively covering edge features across different developmental stages. based on these enhanced edge features, the network accurately outputs the final disease severity grade.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;3\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g003.jpg\" alt=\"diagram illustrating a deep learning process for leaf classification. input images of leaves undergo convolution and mobilevit blocks, reducing dimensions and processing through hfsm, ecfm, and amsddicm modules. outputs are pooled and linearly transformed to logits.\" id=\"f3\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;3\u003c/b>. cogfuse-mobilevit architecture diagram (hfsm suppresses healthy regions through dynamic masks, ecfm explicitly extracts edges via sobel convolution, and amsddicm fuses multi-scale features through dynamic weights).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>2.4.2 the hierarchical feature selection module\u003c/h4>\u003cp class=\"mb0\">the hfsm (hierarchical feature selection module) is an innovative architecture proposed in this study. unlike mobilevit&#x2019;s direct feature concatenation, hfsm utilizes learnable prompt vectors (\u003ca href=\"#eq1\">equation 1\u003c/a>) to generate spatial masks, dynamically suppressing non-lesion regions. such task-driven selection is crucial for tiny objects. in the grading task of walnut leaf brown spot disease, this module utilizes a local attention mechanism to focus on micro-regions of walnut leaves, fusing shallow and middle-layer features. meanwhile, the hierarchical feature selection mechanism enables precise capture of local detail features such as lesion texture and color, effectively suppressing interference from healthy leaf regions and enhancing disease features. this provides high-discriminative feature representations for brown spot disease grading while preparing for subsequent lesion edge processing. as depicted in \u003ca href=\"#f4\">figure&#xa0;4\u003c/a>, the module processes two hierarchical feature maps. initial 1&#xd7;1 convolutions reduce both maps&#x2019; channels to half the output dimension curtailing computational load. one reduced map undergoes bilinear upsampling for spatial alignment with the other. these aligned features are summed and processed by a 3&#xd7;3 convolution to generate base-path features. simultaneously, the original reduced map and upsampled map are fed into dual local-global attention modules for parallel processing (\u003ca href=\"#b36\">xu, 2024\u003c/a>). each group contains local and global branches. during branch processing, the feature map is partitioned into p&#xd7;p non-overlapping patches \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im3\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">p\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmi mathsize=\"10.5pt\">j\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math> via the unfold operation. after calculating the mean of pixel features within each patch, the result is processed using the following core formula: attention distribution generation.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;4\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g004.jpg\" alt=\"diagram illustrating a neural network architecture with multiple components. the upper section shows two convolution layers, lga modules, concatenation, and rep blocks. the lower section features processes like unfold, mean calculation, softmax, and feature selection, highlighting token and channel selection. various operations and components like multiplication, addition, and dropout are labeled, with symbols explaining their functions.\" id=\"f4\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;4\u003c/b>. architecture diagram of the hierarchical feature selection module (hfsm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">z\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ml\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">layernor\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ml\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">mean\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>1\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">a\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">softmax\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">z\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">j\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>2\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq1\">equations 1\u003c/a> and \u003ca href=\"#eq2\">2\u003c/a> convert the mean value of patch features into a high-dimensional feature vector through multi-layer perceptrons (mlp) and layer normalization (layernorm) (wherein(\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im4\">\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">mlp\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;and&#xa0;&#xa0;mlp\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003c/mrow>\u003c/math>) \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im5\">\u003cmrow>\u003cmsup>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">achieve&#xa0;dimensional&#xa0;transformation&#xa0;p\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msup>\u003cmo mathsize=\"10.5pt\">&#x2192;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ouc\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmo mathsize=\"10.5pt\">&#x2192;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">ouc\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/mrow>\u003c/math>)), and then generates the attention distribution via the softmax function \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im6\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">a\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmi mathsize=\"10.5pt\">j\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>. this mechanism enables the model to focus on the key features of lesion regions, weaken the interference from healthy leaf areas, enhance the specificity of feature selection, and provide high-quality features for subsequent precise grading. after the local and global features output by the two modules are spliced along the channel dimension, they are combined with the basic path features to generate the output features of the final processing module.\u003c/p>\u003cp class=\"mb0\">for leaf images with over 80% healthy tissue, dynamic masks are generated via learnable prompt vectors to suppress green texture features while enhancing the gray-brown features of lesions (\u003ca href=\"#b19\">mcgranahan and leslie, 1991\u003c/a>).\u003c/p>\u003ch4>2.4.3 ecfm edge convolutional fusion module\u003c/h4>\u003cp class=\"mb0\">in the walnut leaf brown spot grading task, lesion edge features are crucial for accurately determining disease severity. the standard mobilevit relies on cnn-transformer blocks to implicitly learn edges, whereas the ecfm (edge convolution fusion module) incorporates sobel convolutions, conventional convolutions, and residual connections, effectively fusing edge features with general features and thereby enhancing edge details. as shown in \u003ca href=\"#f5\">figure&#xa0;5\u003c/a>, the sobel branch employs sobel convolution to extract image edge information, accurately capturing the contour details of brown spot lesions; the convolutional branch captures general features such as leaf color and texture through standard convolution. the two realize feature fusion via the following core formula (\u003ca href=\"#eq3\">equation 3\u003c/a>):\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;5\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g005.jpg\" alt=\"diagram showing a network flow. input x is split into two paths: one goes through a sobel filter and the other through a convolution (conv) layer. outputs are concatenated (concat) and followed by another conv layer. the result is summed with a bypass connection and passed through a final conv layer.\" id=\"f5\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;5\u003c/b>. architecture diagram of the edge convolutional fusion module (ecfm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">sobelconv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>3\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">processing the input feature map \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im7\">\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmi mathsize=\"10.5pt\">x\u003c/mi>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> through the sobel convolution operator (sobelconv) specifically extracts edge information of lesions (such as the boundary contours between lesions and healthy tissues, and the edge textures of small lesions). for the commonly seen blurred edges in brown spot disease, sobel convolution can enhance edge gradient changes, making the originally blurred lesion contours clearer, thus providing key edge feature support for subsequent grading. conventional feature extraction and fusion (\u003ca href=\"#eq4\">equation 4\u003c/a>):\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xa0;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>4\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im8\">\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;c\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> extracting the overall features of leaves (such as the color distribution of lesions and the overall texture of leaves) through standard convolution forms a complement to edge features, preventing the model from focusing only on local edges while ignoring global lesion information.\u003c/p>\u003cp class=\"mb15\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im9\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">s\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> concatenating the edge feature s and the conventional feature c along the channel dimension achieves the initial fusion of edge details and global features. this fusion enables the model to both identify the fine contours of lesions and judge the disease condition by combining the overall color and texture changes of lesions, thereby improving the accuracy of grading. the module also introduces feature addition and subsequent convolution operations: the fused features are first processed by the first convolution layer \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im10\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">concat\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math>, the corresponding operation results are added to the original input, and the final output is generated through the second convolution layer \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im11\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">final\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math>. in this process, the residual connection retains the original feature information, further strengthens the integration of edge features and conventional features, and ensures the effective transmission and enhancement of multi-level features.\u003c/p>\u003cp class=\"mb0\">the edge gradient information extracted by sobel convolution (such as grayscale differences between diseased spots and healthy tissues) and the texture features from conventional convolution (such as the roughness of necrotic areas) are fused via residual connections. this not only preserves the blurred edges of early-stage lesions but also prevents edge features from being disconnected from the overall texture (\u003ca href=\"#b8\">ghaiwat and arora, 2014\u003c/a>).\u003c/p>\u003ch4>2.4.4 amsddicm adaptive multi-scale dilated depthwise inseparable convolution module\u003c/h4>\u003cp class=\"mb0\">this module differs from mobilevit&#x2019;s single-scale convolution, leverages multi-shaped convolution kernels to accurately extract lesion features of circular, irregular, and other shapes from multiple dimensions, enabling the identification of subtle differences in lesion edges and internal colors to enrich feature dimensions. meanwhile, depth wise separable convolutions are employed to accommodate lesion scales at different stages of disease development. specifically, the input features are first split into two groups, each entering the amsddicm module to extract features via multi-scale depth wise separable convolutions. a dynamic weighting mechanism (global pooling + convolution + softmax) is used to generate weights for fusing multi-scale features. the processed features from the two groups are concatenated, and finally, cross-channel fusion is completed via 1&#xd7;1 convolution to output the final optimized features. the entire process integrates multi-scale convolution, dynamic weight allocation, and feature fusion to enhance the model&#x2019;s ability to capture complex features, as shown in \u003ca href=\"#f6\">figure&#xa0;6\u003c/a>. in response to the morphological differences between early-stage small spots (1-3mm in diameter) and late-stage fused spots (over 20mm in diameter), the dynamic weight mechanism enables the model to adaptively allocate the contributions of 3&#xd7;3 kernels for capturing local textures and 11&#xd7;1 kernels for extracting strip-shaped spreading features. this addresses the limitation of fixed weights in traditional inception architectures in adapting to multi-scale morphologies.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;6\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g006.jpg\" alt=\"diagram showing a neural network process. the top section involves splitting input channels, applying a dms 2d convolution, concatenation, and another convolution. the bottom section includes a pooling layer, convolution, rearrangement, softmax, summation, batch normalization, and activation. the process results in output \\(x^{''}\\).\" id=\"f6\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;6\u003c/b>. architecture diagram of the adaptive multi-scale dilated depth wise inseparable convolution module (amsddicm).\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">when the input feature tensor x with the number of channels c enters the amsddicm module, it first undergoes a feature grouping operation: the input features are evenly split into two groups along the channel dimension, with each group containing half of the original number of channels (c//2). this grouping strategy not only reduces computational complexity but also creates conditions for subsequent multi-scale feature extraction. each feature group is fed into an independent dmsconv2d module, which adopts different configurations such as 3&#xd7;3, 5&#xd7;5 square convolution kernels and 1&#xd7;11, 11&#xd7;1 strip-shaped convolution kernels &#x2014; the square kernels capture local textures, while the strip-shaped kernels extract direction-sensitive long-range dependencies. this dynamic weight allocation draws inspiration from adaptive strategies in agricultural iot systems. similar to how salinity-aware ets models prioritize soil conductivity features under salt stress (\u003ca href=\"#b40\">zhang et&#xa0;al., 2023\u003c/a>), our mechanism selectively amplifies lesion morphological features critical for severity grading. the dynamic weighting mechanism of dmsconv2d generates weights through global average pooling and 1&#xd7;1 convolution, with the core formulas as follows:\u003c/p>\u003cp class=\"mb15\">weighted basic feature generation:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">rearrange\u003c/mtext>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">conv\u003c/mtext>\u003c/mrow>\u003cmrow>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003c/msub>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">avgpoo\u003c/mtext>\u003cmn mathsize=\"10.5pt\">l2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">d\u003c/mtext>\u003cmrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>5\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq5\">equation 5\u003c/a> compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information (such as the overall distribution characteristics of lesions at different scales), and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmo mathsize=\"10.5pt\">.\u003c/mo>\u003cmtext mathsize=\"10.5pt\">softmax\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>6\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb15\">\u003ca href=\"#eq6\">equation 6\u003c/a> compresses the spatial dimensions of the input features through global average pooling (avgpool2d), retains global statistical information, such as the overall distribution characteristics of lesions at different scales, and then transforms the dimensions via 1&#xd7;1 convolution to generate base features for calculating dynamic weights. this step provides a basis for subsequent weight allocation, enabling the model to preliminarily evaluate the importance of features at different scales. weight normalization:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">0\u003c/mn>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003c/msubsup>\u003c/mrow>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">dwcon\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">v\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003cmo mathsize=\"10.5pt\">&#xd7;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>7\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cp class=\"mb0\">\u003ca href=\"#eq7\">equation 7\u003c/a> is based on dynamic weights \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im12\">\u003cmrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">dkw\u003c/mtext>\u003cmo mathsize=\"10.5pt\">,\u003c/mo>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>) for different convolution kernels \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im13\">\u003cmrow>\u003cmsub>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">&#xa0;dwconv\u003c/mtext>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">x\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/math> weighted fusion is performed on the extracted features. for walnut brown spot lesions exhibiting size heterogeneity and morphological complexity&#x2014;ranging from early-stage circular micro-lesions to late-stage coalesced irregular lesions&#x2014;this mechanism adaptively modulates multi-scale feature contributions. it prioritizes retention of morphology-discriminative features, local textures in small lesions or directional distributions in large lesions, thereby comprehensively capturing pathological characteristics across developmental stages. the processed features undergo channel-wise concatenation to generate optimized outputs (\u003ca href=\"#b20\">mircetich et&#xa0;al., 1980\u003c/a>).\u003c/p>\u003ch3>2.5 experimental process for severity grading of walnut leaf brown spot disease\u003c/h3>\u003cp class=\"mb0\">\u003ca href=\"#f7\">figure&#xa0;7\u003c/a> is the overall flow chart of severity grading of walnut leaf spot disease. first is the image acquisition stage, where raw images of walnut leaves are captured and collected to construct an image database containing pictures to be classified (\u003ca href=\"#b29\">singh et&#xa0;al., 2019\u003c/a>). next is the image preprocessing stage, which involves sequentially calculating and classifying disease severity levels, establishing image labels, and performing image preprocessing operations. subsequently, the processed data are used to train the constructed dataset. finally, in the model training and performance evaluation stage, the preprocessed images are input into the model for classifying walnut leaf brown spot disease, after which performance evaluation is conducted on the model&#x2019;s classification results to determine the model&#x2019;s effectiveness and accuracy.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;7\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g007.jpg\" alt=\"flowchart illustrating a process for walnut leaf brown spot disease classification. it consists of three main phases: image acquisition, preprocessing, and model training and evaluation. the image acquisition phase involves collecting walnut leaf images and storing them in a database. the preprocessing phase includes classification of disease grades, establishing image labels, and training the dataset. finally, the model classifies the disease, resulting in output images and a performance evaluation matrix for accuracy verification. the chart uses labeled boxes, arrows, and image samples for illustration.\" id=\"f7\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;7\u003c/b>. overall flow chart for severity grading of walnut leaf brown spot disease.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>2.6 experimental parameters and evaluation metrics\u003c/h3>\u003ch4>2.6.1 test environment and hyperparameter setting\u003c/h4>\u003cp class=\"mb15\">the experimental setup of this study is based on deep learning technology and leverages high-performance computing resources. all experiments were conducted on the windows 10 operating system. the hardware platform consists of an amd ryzen 7 3700x processor and an nvidia rtx 2080ti graphics card. the software environment was built using python 3.8, the pytorch 1.13.0 deep learning framework, and the cuda 11.3 parallel computing platform. additionally, pytorch 1.13.0&#x2014;a widely adopted open-source deep learning library renowned for its high flexibility&#x2014;was selected, making it well-suited for research and development.\u003c/p>\u003cp class=\"mb0\">during model training, multiple adjustments were made to the hyperparameters to compare test results and select the optimal hyperparameter combination. the model accepts input images sized at 224&#xd7;224 pixels, with a configured batch size of 32 during training and a maximum of 100 training epochs. the optimizer employs an initial learning rate of 0.003.\u003c/p>\u003ch4>2.6.2 evaluation metrics\u003c/h4>\u003cp class=\"mb15\">this paper aims to evaluate the performance of the model and verify the effectiveness of the improvement measures. we selected multiple evaluation metrics, and all subsequent experimental results adopt the method of calculating averages via 5-fold cross-validation, aiming to comprehensively and reliably evaluate the model performance. including precision (p), recall (r), etc. (\u003ca href=\"#eq8\">equations 8\u003c/a>&#x2013;\u003ca href=\"#eq16\">16\u003c/a>) these metrics can be calculated using the following formulas.\u003c/p>\u003cp class=\"mb15\">the arithmetic mean of the metric values across the five folds:\u003c/p>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmover accent=\"true\">\u003cmtext mathsize=\"10.5pt\">m\u003c/mtext>\u003cmo mathsize=\"10.5pt\">&#xaf;\u003c/mo>\u003c/mover>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmn mathsize=\"10.5pt\">5\u003c/mn>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmn mathsize=\"10.5pt\">5\u003c/mn>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">m\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>8\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">precision\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>9\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">recall\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">/\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fn\u003c/mtext>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>10\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">f\u003c/mtext>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">score\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmfrac>\u003cmrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003c/mrow>\u003cmrow>\u003cmn mathsize=\"10.5pt\">2\u003c/mn>\u003cmtext mathsize=\"10.5pt\">tp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fp\u003c/mtext>\u003cmo mathsize=\"10.5pt\">+\u003c/mo>\u003cmtext mathsize=\"10.5pt\">fn\u003c/mtext>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>11\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">macro&#xa0;precision=\u003c/mtext>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>12\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">macro&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/mfrac>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmsubsup>\u003cmrow>\u003c/mrow>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003c/mrow>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">r\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>13\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;precision=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&#xb7;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">precisio\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>14\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">(\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&#xb7;\u003c/mo>\u003cmtext mathsize=\"10.5pt\">precisio\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003cmo mathsize=\"10.5pt\" stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003cmrow>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>15\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003cdiv class=\"equationimageholder\">\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\">\u003cmtable class=\"eqnarray\" columnalign=\"left\">\u003cmtr>\u003cmtd>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">weighted&#xa0;avg&#xa0;recall=\u003c/mtext>\u003cmfrac>\u003cmrow>\u003cmsubsup>\u003cmo mathsize=\"10.5pt\">&#x2211;\u003c/mo>\u003cmrow>\u003cmtext mathsize=\"10.5pt\">i\u003c/mtext>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">c\u003c/mtext>\u003c/msubsup>\u003cmtext mathsize=\"10.5pt\">t\u003c/mtext>\u003cmsub>\u003cmtext mathsize=\"10.5pt\">p\u003c/mtext>\u003cmi mathsize=\"10.5pt\">i\u003c/mi>\u003c/msub>\u003c/mrow>\u003cmtext mathsize=\"10.5pt\">n\u003c/mtext>\u003c/mfrac>\u003c/mrow>\u003c/mtd>\u003cmtd>\u003cmrow>\u003cmo stretchy=\"false\">(\u003c/mo>\u003cmn>16\u003c/mn>\u003cmo stretchy=\"false\">)\u003c/mo>\u003c/mrow>\u003c/mtd>\u003c/mtr>\u003c/mtable>\u003c/math>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>\u003ca id=\"h4\" name=\"h4\">\u003c/a>\u003ch2>3 experiments and results analysis\u003c/h2>\u003ch3>3.1 core module design and validity experimental verification\u003c/h3>\u003ch4>3.1.1 comparative test of necessity of hfsm module\u003c/h4>\u003cp class=\"mb0\">to validate the necessity of the hierarchical feature selection module (hfsm) within the cogfuse-mobilevit framework, this study conducted comparative tests against two mainstream lightweight attention modules: se (squeeze-and-excitation) and cbam (convolutional block attention module). as illustrated in \u003ca href=\"#t4\">table&#xa0;4\u003c/a>, the hfsm module achieves a significantly higher accuracy of 86.61%, outperforming the se module and cbam module by 7.38% and 4.46%, respectively. this demonstrates that its hierarchical feature selection mechanism more effectively captures discriminative features. in terms of computational efficiency, the parameters and flops of hfsm remain comparable with those of the se module and cbam module, indicating that its performance breakthrough stems from innovative structural design under equivalent lightweight constraints. comprehensive results confirm that replacing hfsm with se or cbam modules would incur a performance degradation exceeding 4%, thereby validating the indispensable value of hfsm in enabling efficient feature selection within the cogfuse-mobilevit framework.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;4\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" name=\"table&#xa0;4\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t004.jpg\" alt=\"www.frontiersin.org\" id=\"t4\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;4\u003c/b>. performance comparison of attention modules within the cogfuse-mobilevit framework.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.2 convolutional kernel selection in amsddicm\u003c/h4>\u003cp class=\"mb0\">rigorous validation via kernel combination ablation studies (\u003ca href=\"#t5\">table&#xa0;5\u003c/a>) demonstrates. the hybrid configuration (3&#xd7;3 \u003cb>+\u003c/b> 5&#xd7;5 \u003cb>+\u003c/b> 1&#xd7;11) significantly outperformed square-only kernels (3&#xd7;3 \u003cb>+\u003c/b> 5&#xd7;5) accuracy 86.61% and 82.15% stage-specific recall gains. mid-stage lesions (level 2) raise 9.4 percentage points. late-stage lesions (level 3) raise 6.1 percentage points. this empirically validates the necessity of 1&#xd7;11 rectangular kernels for capturing linear pathological features, overcoming the limitation of isotropic kernels in detecting anisotropic structures.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;5\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" name=\"table&#xa0;5\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t005.jpg\" alt=\"www.frontiersin.org\" id=\"t5\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;5\u003c/b>. comparison of different convolution kernels in amsddicm.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.3 the impact of new modules on computational complexity\u003c/h4>\u003cp class=\"mb0\">\u003ca href=\"#f8\">figure&#xa0;8\u003c/a> shows that the hfsm module incurs a 169% flops increase to achieve a breakthrough improvement in early-stage lesion detection&#x2014;reducing level 0/1 misclassification by 24%, thereby establishing the pathological foundation for severity grading. the ecfm module contributes a mere 3% flops increment yet drives a 3.68 percentage point accuracy gain through enhanced edge feature representation. with only a 0.028g flops overhead 1.3%, the amsddicm module enables adaptive fusion of multiscale pathological deformations, culminating in a 7.80-pp accuracy leap. these modules form a cascaded optimization paradigm: hfsm&#x2019;s substantial cost resolves the core pathological bottleneck, while subsequent modules deliver superlinear returns&#x2014;harvesting 6.85-pp accuracy gain with just 14% additional flops&#x2014;collectively establishing the globally optimal computation-performance equilibrium.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;8\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g008.jpg\" alt=\"bar graphs comparing different configurations of mobilevit models across three metrics. panel a shows accuracy percentages, with values from 78.81% to 86.61%. panel b displays flops, ranging from 0.74g to 2.1g. panel c illustrates parameters in millions, with values between 1.94m and 2.02m. each graph compares mobilevit, mobilevit plus hfsm, mobilevit plus hfsm plus ecfm, and cogfuse-mobilevit.\" id=\"f8\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;8\u003c/b>. comparison of the impact of each newly added module on computational complexity \u003cb>(a)\u003c/b> accuracy (%); \u003cb>(b)\u003c/b> flops (g); \u003cb>(c)\u003c/b> params (m). measured on nvidia rtx 2080ti gpu (pytorch 1.13.0, cuda 11.3) with 224&#xd7;224 input.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.1.4 comparison of the influence of different module fusion on model performance\u003c/h4>\u003cp class=\"mb0\">to validate the effect of module fusion, \u003ca href=\"#t6\">table&#xa0;6\u003c/a> compares model performances with different combinations. when only hfsm (hierarchical feature selection module) is introduced, precision increases from the baseline of 82.23% to 83.77%, but recall decreases by 3 percentage points to 72.31%, causing the f1 score to slightly decline to 78.04%. accuracy rises to 82.15%, indicating improved overall classification correctness of the model, but with potential risk of missed detections.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;6\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" name=\"table&#xa0;6\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t006.jpg\" alt=\"www.frontiersin.org\" id=\"t6\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;6\u003c/b>. impact of fusion of different modules on model performance.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">when hfsm and ecfm (edge-context feature module) are synergistically introduced, precision increases to 86.34%, recall recovers to 74.69%, and both f1 score and accuracy are significantly optimized. hfsm fuses shallow and middle-layer features through a hierarchical attention mechanism, laying the foundation for subsequent edge feature processing; ecfm enhances lesion edge features. the combination of the two effectively improves the integrity of feature representation and the clarity of lesion boundaries.\u003c/p>\u003cp class=\"mb15\">the combination of hfsm and amsddicm (adaptive multi-scale dilated depthwise inseparable convolution module) further pushes recall to 76.24%, accuracy to 83.94%, and f1 score to 80.79%, outperforming the hfsm+ecfm combination. amsddicm compensates for hfsm&#x2019;s deficiency in local feature refinement through attention-guided multi-scale detail fusion, which integrates multi-layer detail feature weights, especially suitable for scenarios with small targets or blurred features.\u003c/p>\u003cp class=\"mb0\">when the three modules work synergistically, all indicators reach optimal levels: precision, recall, f1 score, and accuracy increase by 12.19%, 7.67%, 9.62%, and 10.00% respectively compared with the baseline. among them, hfsm lays the foundation for cross-layer feature fusion, ecfm effectively integrates edge-specific features with general features to enhance image edge information, and amsddicm adaptively fuses multi-scale and multi-type features through an attention mechanism, forming a progressive optimization chain from &#x201c;hierarchical feature extraction&#x201d; to &#x201c;edge semantic enhancement&#x201d; and then to &#x201c;multi-layer detail fusion&#x201d;. the experimental results show a balanced improvement in both precision and recall, indicating that the fusion of the three modules enables the model to focus more on learning the features of small brown spot lesions, thereby improving its classification performance. this synergistic task-specific design, combining hierarchical selection, explicit edge enhancement, and adaptive multi-scale fusion, fundamentally distinguishes cogfuse-mobilevit from prior mobilevit hybrids designed for general vision or localization tasks.\u003c/p>\u003ch4>3.1.5 influence of different module combinations on f1-score of level (0-3)\u003c/h4>\u003cp class=\"mb0\">in order to verify the targeted improvement of each module for a specific disease level, we conducted the following comparative experiments as shown in the \u003ca href=\"#t6\">table&#xa0;6\u003c/a>, when hfsm is introduced alone, the f1-score of level 0 increases from 83.10% to 85.60%, effectively reducing feature confusion between healthy leaves and early-stage lesions. after adding ecfm, the f1-score of level 1 rises from 73.20% (with hfsm alone) to 79.80%, significantly mitigating the recognition bias caused by blurred edges of small lesions. amsddicm increases the f1-score of level 3 from 80.37% to 85.54%, enhancing the adaptability to the morphology of large-scale fused scorched areas. with the collaboration of the three modules, the f1-scores of level 0&#x2013;3 reach 93.99%, 82.57%, 84.28%, and 85.54% respectively, achieving balanced optimization of performance across all levels.\u003c/p>\u003ch3>3.2 results comparison of different algorithms and statistical significance verification\u003c/h3>\u003ch4>3.2.1 comparison of grading results for different classification models\u003c/h4>\u003cp class=\"mb0\">to verify the effectiveness of the cogfuse-mobilevit model, we selected 9 commonly used classification models to compare with the optimized cogfuse-mobilevit model, and the results are the average of 5-fold cross-validation over 120 epochs. as shown in \u003ca href=\"#t7\">table&#xa0;7\u003c/a>, the proposed cogfuse-mobilevit model achieved the highest grading performance in terms of precision, recall, and f1-score for identifying walnut leaf brown spot disease at different severity levels among all compared models. the improved cogfuse-mobilevit model exhibits an accuracy of 86.61%, representing a 7.80-percentage-point improvement over the original model. overall, the experimental results highlight that the enhanced cogfuse-mobilevit model is more conducive to focusing on lesion edge details and accurately learning the features of different severity levels of walnut leaf brown spot disease, thereby improving the model&#x2019;s classification performance.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;7\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" name=\"table&#xa0;7\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t007.jpg\" alt=\"www.frontiersin.org\" id=\"t7\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;7\u003c/b>. comparison of grading results for different classification models.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.2.2 comparison of performance and reliability validation of different algorithms\u003c/h4>\u003cp class=\"mb0\">in model training, to quantify the reliability of results and avoid random biases from single experiments, this study employs 5-fold cross-validation to generate 95% confidence intervals, as shown in the \u003ca href=\"#f9\">figure&#xa0;9\u003c/a>. the results demonstrate that cogfuse-mobilevit takes a significant lead with an accuracy of 86.61% (95% ci: [85.24%, 87.89%]). its narrowest confidence interval range indicates the strongest generalization capability. among the comparative models, mobilevitv3 has a 95% ci of [77.20%, 80.42%]; its lower bound is significantly lower than that of cogfuse-mobilevit, confirming that the 7.8% performance improvement is not due to random fluctuations. furthermore, the confidence intervals constructed through 5-fold cross-validation further highlight the reliability of the experimental results.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;9\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g009.jpg\" alt=\"bar chart showing confidence intervals for accuracy rates of different classification models. models with higher accuracy include cogfuse-mobilevit at 86.61 and mobilevitv3 at 78.81. other models range from 73.63 to 68.72. each model is color-coded, with a corresponding legend on the right.\" id=\"f9\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;9\u003c/b>. confidence intervals of accuracy for different classification models in 5-fold cross-validation.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch4>3.2.3 statistical significance verification of model improvement\u003c/h4>\u003cp class=\"mb0\">to statistically evaluate the performance improvement of cogfuse-mobilevit over the baseline mobilevitv3, an independent samples t-test was conducted (\u003ca href=\"#t8\">table&#xa0;8\u003c/a>). for each model architecture, five independent training runs. the null hypothesis stated that the mean accuracy of cogfuse-mobilevit was equal to that of mobilevitv3 \u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im14\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">h\u003c/mi>\u003cmn mathsize=\"10.5pt\">0\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">:\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">c\u003c/mi>\u003cmtext mathsize=\"10.5pt\">og\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">=\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">m\u003c/mi>\u003cmi mathsize=\"10.5pt\">o\u003c/mi>\u003cmi mathsize=\"10.5pt\">b\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>), while the alternative hypothesis stated that cogfuse-mobilevit had a higher mean accuracy (\u003cmath xmlns=\"http://www.w3.org/1998/math/mathml\" display=\"inline\" id=\"im15\">\u003cmrow>\u003cmsub>\u003cmi mathsize=\"10.5pt\">h\u003c/mi>\u003cmn mathsize=\"10.5pt\">1\u003c/mn>\u003c/msub>\u003cmo mathsize=\"10.5pt\">:\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">c\u003c/mi>\u003cmtext mathsize=\"10.5pt\">og\u003c/mtext>\u003c/mrow>\u003c/msub>\u003cmo mathsize=\"10.5pt\">&gt;\u003c/mo>\u003cmsub>\u003cmi mathsize=\"10.5pt\">&#x3bc;\u003c/mi>\u003cmrow>\u003cmi mathsize=\"10.5pt\">m\u003c/mi>\u003cmi mathsize=\"10.5pt\">o\u003c/mi>\u003cmi mathsize=\"10.5pt\">b\u003c/mi>\u003c/mrow>\u003c/msub>\u003c/mrow>\u003c/math>)cogfuse-mobilevit achieved a mean accuracy, significantly outperforming mobilevitv3. the independent samples t-test confirmed this improvement (t(8)=18.92,p=3.7&#xd7;10 \u003csup>&#x2212;8\u003c/sup>).\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;8\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" name=\"table&#xa0;8\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t008.jpg\" alt=\"www.frontiersin.org\" id=\"t8\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;8\u003c/b>. comparison of accuracy results between mobilevitv3 and cogfuse-mobilevit using independent samples t-test.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">under 5-fold cross-validation (\u003ca href=\"#f10\">figure&#xa0;10\u003c/a>), cogfuse-mobilevit achieves rapid convergence within the first 20 epochs, demonstrating more efficient feature learning capability. upon entering the steady-state phase, its validation loss is reduced by over 5-fold compared to the baseline mobilevitv3, directly confirming smaller prediction errors and superior generalization capability. meanwhile, the smooth and minimally fluctuating loss curve of cogfuse-mobilevit reflects strong robustness against data noise and distribution variations. combined with the previous statistical findings from accuracy confidence intervals and independent t-tests, these results collectively validate the statistical significance of the improved model.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;10\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g010.jpg\" alt=\"line graph showing validation loss over epochs for two models: cogfuse-mobilevit (blue) and mobilevitv3 (orange). loss decreases sharply initially, then stabilizes, with cogfuse-mobilevit exhibiting a consistently lower loss.\" id=\"f10\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;10\u003c/b>. comparison of loss curves between the cogfuse-mobilevit model and the original model in 5-fold cross-validation within 120 epochs.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.3 model result analysis performance comparison of different models\u003c/h3>\u003cp class=\"mb0\">\u003ca href=\"#f11\">figure&#xa0;11a\u003c/a> shows that in the walnut leaf brown spot disease severity grading task, the classification accuracy of all models gradually improved and stabilized with the increase of training epochs, indicating that the models continuously optimized during learning until convergence. cogfuse-mobilevit stood out prominently: it achieved rapid accuracy improvement, reached a high level in relatively early training epochs with minimal subsequent fluctuations, and stably maintained the highest accuracy, demonstrating fast convergence and strong generalization ability to efficiently extract features distinguishing different disease levels. densenet also maintained high accuracy in the late training stage, but its accuracy improvement was slower, with slightly inferior convergence speed and final stability compared to cogfuse-mobilevit. models like resnet and regnet showed limited accuracy gains with gentle upward trends, and their final stable accuracy values were significantly lower, reflecting insufficient feature extraction capabilities possibly due to network architecture or parameter optimization efficiency.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;11\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g011.jpg\" alt=\"panel a shows a line graph comparing the accuracy of different neural networks over 100 epochs. cogfuse-mobilevit reaches the highest accuracy, achieving over 90 percent. panel b displays a line graph comparing loss over epochs. cogfuse-mobilevit has the lowest loss, declining swiftly to under 0.2. the legend lists networks like densenet, efficientnet, and mobilevitv3.\" id=\"f11\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;11\u003c/b>. comparison of accuracy and loss across different models \u003cb>(a)\u003c/b> accuracy line chart; \u003cb>(b)\u003c/b> loss line chart.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb0\">in \u003ca href=\"#f11\">figure&#xa0;11b\u003c/a>, all models showed high initial loss values that dropped rapidly and then stabilized, following the typical learning process of iterative optimization. cogfuse-mobilevit achieved significant early loss decline and stabilized near the minimum, indicating efficient feature learning and strong fitting capability. while densenet also reached a relatively low loss level, it remained slightly higher than cogfuse-mobilevit. other models had higher late-stage loss values: some showed fast initial decline but converged to higher levels, indicating shortcomings in capturing key disease features.\u003c/p>\u003ch3>3.4 analysis of detection results for different classification models\u003c/h3>\u003cp class=\"mb0\">the confusion matrix is a key indicator for evaluating classification models: the higher the diagonal values, the higher the classification accuracy for the corresponding category, and the lower the off-diagonal values, the fewer misjudgments. \u003ca href=\"#f12\">figure&#xa0;12\u003c/a> shows the classification results of different models for the four disease grades (0&#x2013;3) of walnut leaf brown spot disease. it is clearly evident that the overall classification performance, particularly the accuracy of cogfuse-mobilevit across all categories, is remarkably high with minimal misjudgments, demonstrating that the model has enhanced discrimination ability for disease grades and performs optimally in distinguishing the four disease levels.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;12\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g012.jpg\" alt=\"six normalized confusion matrices labeled a to f show predicted versus true values ranging from zero to three. each matrix illustrates classification performance with varying accuracy per class, indicated by different shades of red. brighter reds denote higher accuracy, while lighter shades indicate lower accuracy or misclassification.\" id=\"f12\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;12\u003c/b>. confusion matrices of the improved cogfuse-mobilevit model and traditional classification models \u003cb>(a)\u003c/b> densenet; \u003cb>(b)\u003c/b> efficientnet; \u003cb>(c)\u003c/b> efficientnetv2; \u003cb>(d)\u003c/b> swin transformer; \u003cb>(e)\u003c/b> mobilevitv3; \u003cb>(f)\u003c/b> cogfuse-mobilevit.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003cp class=\"mb15 w100pc float_left mt15\">the edge-enhanced feature module (ecfm) effectively addressed level 0 and level 1 misclassification by capturing incipient lesion features. baseline analysis revealed substantial false negatives for early-stage lesions, with level 1 and level 0 misclassification reaching 32%. ecfm reduced this rate to 8%, demonstrating its capacity to resolve texture confusion through enhanced edge contour delineation. similarly, the adaptive multi-scale dilated convolution module (amsddicm) significantly mitigated level 2 and level 3 mutual misclassification. amsddicm drastically reduced both errors by enhancing local fine-grained features in mid-stage lesions (level 2) while strengthening global fusion features in late-stage coalesced lesions (level 3). this resolves grading ambiguity arising from the morphological continuum of lesion progression.\u003c/p>\u003cp class=\"mb0\">to establish a comprehensive performance evaluation framework and quantify the model&#x2019;s overall discriminative capability, \u003ca href=\"#f13\">figure&#xa0;13\u003c/a> presents the receiver operating characteristic (roc) curves of cogfuse-mobilevit across four severity levels. these curves compare the true positive rate (tpr) against the false positive rate (fpr) by dynamically adjusting the classification threshold. the area under the curve (auc) serves as the primary performance metric, where a higher value indicates stronger classification ability. notably, the auc values for all categories significantly exceed the level of random guessing (auc=0.5), fully demonstrating that the model possesses robust discriminative capability across different severity levels.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;13\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g013.jpg\" alt=\"receiver operating characteristic (roc) curve chart showing true positive rate versus false positive rate for four classes: class 0 (blue), class 1 (orange), class 2 (green), and class 3 (red), along with an average roc curve (dashed purple). the curves are above the diagonal, indicating good model performance.\" id=\"f13\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;13\u003c/b>. roc curves of the four different severity levels for cogfuse-mobilevit.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.5 tsne visualization of features extracted by different models\u003c/h3>\u003cp class=\"mb0\">the tsne visualization of features extracted from the models is shown in \u003ca href=\"#f14\">figures&#xa0;14a&#x2013;d\u003c/a>. this study analyzed the features of the original model at the 20th, 50th, and 100th training epochs, as well as the improved model at the 100th epoch. after 20 epochs of training, the original model showed a highly discrete feature distribution. limited by the number of training epochs, the model failed to fully learn discriminative features, resulting in insufficient distinction between categories and significant overlap among different classes. this indicates that under this training intensity, the original model&#x2019;s ability to capture meaningful patterns was relatively limited. when the original model was trained to 50 epochs, compared with (a), the feature aggregation significantly improved. however, inter-class overlap still existed, suggesting that although prolonged training aided feature learning, the original model&#x2019;s architecture had inherent defects in achieving clear feature separation. at 100 epochs of training, the original model exhibited more prominent feature clustering. nevertheless, some regions still lacked clear separation between different categories, indicating that even after prolonged training, the original model faced challenges in maximizing inter-class distance and intra-class compactness. in figure (d), the improved model after 100 epochs of training demonstrated a more superior feature distribution: each category was tightly clustered, achieving high intra-class compactness, while distinct boundaries between different categories were established, resulting in significant inter-class distance. this suggests that the model improvements effectively enhanced its ability to extract discriminative features, greatly reduced feature ambiguity, and improved feature discriminative power. compared with the original model, it showed stronger feature discrimination and optimization potential.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;14\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g014.jpg\" alt=\"four scatter plots labeled a, b, c, and d illustrate data points grouped by colors representing levels zero to three. each plot displays varying cluster formations. plot a shows two distinct clusters. plot b has dispersed clusters, with one elongated group. plot c features overlapping clusters, and plot d displays four well-separated clusters.\" id=\"f14\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;14\u003c/b>. tsne visualization of features extracted by different models. after the three modules work synergistically, the boundaries of feature clustering between level 0 (healthy) and level 1 (early-stage) are significantly clearer, which verifies the effectiveness of edge-texture collaborative learning \u003cb>(a)\u003c/b> the original model was trained for 20 epochs; \u003cb>(b)\u003c/b> the original model was trained for 50 epochs; \u003cb>(c)\u003c/b> the original model was trained for 100 epochs; \u003cb>(d)\u003c/b> the improved model was trained for 100 epochs.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.6 radar chart for comparison of classification performance between original and improved models\u003c/h3>\u003cp class=\"mb0\">as shown in \u003ca href=\"#f15\">figure&#xa0;15\u003c/a>, a radar chart compares the performance of the original model and the improved cogfuse-mobilevit model across multiple evaluation metrics. in terms of accuracy, cogfuse-mobilevit exhibits significantly higher values than the original mobilevitv3 model, indicating that the improved model achieves overall higher classification accuracy. macro-average precision, which considers the average precision of each category, clearly shows that cogfuse-mobilevit also performs better, meaning it has superior precision across all categories. meanwhile, cogfuse-mobilevit also demonstrates better macro-average recall. when examining weighted-average precision and weighted-average recall&#x2014;metrics that account for class imbalance&#x2014;cogfuse-mobilevit outperforms mobilevitv3 in both, indicating that in practical applications, even with uneven class distribution, the cogfuse-mobilevit model can deliver better performance. these results further demonstrate the model&#x2019;s reliability and practicality.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">figure&#xa0;15\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" name=\"\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-g015.jpg\" alt=\"radar chart comparing two models: mobilevitv3 (red) and cogfuse-mobilevit (blue). metrics include accuracy, macro average precision, macro average recall, weighted average precision, and weighted average recall. each axis ranges from fifty to one hundred.\" id=\"f15\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>figure&#xa0;15\u003c/b>. multi-metric comparison between mobilevitv3 and improved cogfuse-mobilevit model.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ch3>3.7 public data set experiment\u003c/h3>\u003cp class=\"mb15\">to further validate the efficacy and generalizability of the proposed cogfuse-mobilevit model, experiments were conducted on the public dataset appleleaf9 (\u003ca href=\"#b38\">yang et&#xa0;al., 2022\u003c/a>). this dataset comprises healthy apple leaves and eight categories of apple leaf diseases captured in field environments without restrictions on imaging angles or noise interference. the dataset was partitioned into training and test sets at an 8:2 ratio. all images were resized to 224&#xd7;224 pixels to optimize deep learning model training efficiency. following the hyperparameter configurations specified in &#x201c;experimental parameters and evaluation metrics&#x201d;, both the baseline mobilevitv3 and cogfuse-mobilevit models were trained and evaluated.\u003c/p>\u003cp class=\"mb0\">cross-species validation demonstrates that the cogfuse-mobilevit model delivers exceptional performance on the appleleaf9 dataset. as presented in \u003ca href=\"#t9\">table&#xa0;9\u003c/a>, the model comprehensively surpasses the baseline mobilevitv3 across all four core metrics: precision increases by 0.16 percentage points, recall achieves a breakthrough improvement of 3.45 percentage points, f1-score rises by 1.18 percentage points, and accuracy elevates by 1.14 percentage points. the synergistic enhancement in both precision and recall signifies that performance gains stem from strengthened feature discriminability rather than metric trade-off compromises. the remarkable percentage points recall gain substantially mitigates leaf disease omission rates, while the holistic advance in f1-score further attests to the model&#x2019;s robustness. these results collectively validate the generalizability of cogfuse-mobilevit&#x2019;s core innovations in agricultural fine-grained disease grading, establishing a transferable paradigm for small-target pathology identification across plant species.\u003c/p>\u003cdiv class=\"dottedline\">\u003c/div>\u003cdiv class=\"imageheaders\">table&#xa0;9\u003c/div>\u003cdiv class=\"figuredesc\">\u003ca href=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" name=\"table&#xa0;9\" target=\"_blank\">\n \u003cpicture>\n \u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 563px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 1024px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"(max-width: 1441px)\">\u003csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"\">\u003csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" media=\"\"> \u003cimg src=\"https://www.frontiersin.org/files/articles/1641677/fpls-16-1641677-html-r1/image_m/fpls-16-1641677-t009.jpg\" alt=\"www.frontiersin.org\" id=\"t9\" loading=\"lazy\">\n \u003c/picture>\n\u003c/a>\u003cp>\u003cb>table&#xa0;9\u003c/b>. experimental results of the original model and cogfuse-mobilevit model on appleleaf9 data set.\u003c/p>\u003c/div>\u003cdiv class=\"dottedline\">\u003c/div>\u003ca id=\"h5\" name=\"h5\">\u003c/a>\u003ch2>4 conclusion\u003c/h2>\u003cp class=\"mb0\">this study addresses the challenges in precise grading of walnut leaf brown spot disease. by adopting dynamic feature filtering, edge gradient reinforcement, and multi-scale morphological adaptation, it effectively resolves three key limitations of existing hybrid architectures and the baseline model mobilevitv3 in plant disease grading weak capability in capturing blurred edges, poor multi-scale adaptability, and interference from healthy tissues. experimental results show that the model achieves an 86.61% grading accuracy on a dataset encompassing diverse lighting conditions, cultivars, and lesion stages, representing a 7.80% improvement over the baseline mobilevitv3 and outperforming nine state-of-the-art models. at the same time, this study confirmed that the performance improvement of cogfuse-mobilevit was statistically significant and stable through strict statistical tests, providing a reliable method for accurate classification of walnut leaf spot disease. the constructed image dataset and proposed grading re-measurement method lay the foundation for accuracy. this approach provides a new paradigm for small-target disease grading, with core modules transferable to multi-crop disease recognition. beyond disease classification, context-aware ai frameworks demonstrate significant efficacy in agricultural management (\u003ca href=\"#b1\">acharya et&#xa0;al., 2022\u003c/a>). iot systems dynamically adjust fertilization recommendations by integrating real-time soil-crop data, while salinity-corrected evapotranspiration (ets) models optimize irrigation strategies for saline-alkali soils (\u003ca href=\"#b15\">li et&#xa0;al., 2023\u003c/a>; \u003ca href=\"#b30\">su et&#xa0;al., 2023\u003c/a>). similarly, multimodal fusion techniques enhance reference evapotranspiration (eto) prediction accuracy, facilitating precision water allocation (\u003ca href=\"#b11\">jo et&#xa0;al., 2021\u003c/a>; \u003ca href=\"#b27\">rustia et&#xa0;al., 2023\u003c/a>). these breakthroughs collectively validate the robust capability of adaptive feature processing in complex agricultural environments&#x2014;a core principle that resonates with our dynamic weighting strategy for disease feature extraction. future research should therefore integrate cross-modal and cross-domain capabilities to establish multidimensional assessment systems and regional dynamic monitoring frameworks, thereby delivering comprehensive technical solutions for small-target disease control.\u003c/p>\u003ca id=\"h6\" name=\"h6\">\u003c/a>\u003ch2>5 discussion and future work\u003c/h2>\u003cp class=\"mb15\">compared with existing methods, this study overcomes the limitation of traditional deep learning models relying on fixed convolution kernels, enabling adaptive capture of lesion features with multi-shaped convolution kernels to accurately extract edge textures of circular micro-lesions and regional contours of irregular lesions. although the constructed multi-source dataset covers diverse lighting conditions, cultivars, and lesion development stages, the homogeneous internal features of severe diseases lead to limited recall rate. meanwhile, when lesions are severely overlapped or mixed with mechanical damage, insect damage, or other types of injuries, the discriminative accuracy of the model is affected. for ultra-small lesions, the local feature information is too weak and easily overlooked, and the computational efficiency still needs optimization on some hardware platforms. furthermore, the adaptability of the current model in extreme field scenarios, such as high-density occlusion and compound damage from pests and diseases, remains to be further verified. in these scenarios, the coupling of complex interference factors exacerbates feature confusion, affecting grading reliability.\u003c/p>\u003cp class=\"mb15\">to address these issues, future work will focus on the following research directions. future research will focus on enhancing the model&#x2019;s performance in complex field environments through multiple synergistic strategies. this includes constructing multi-interference factor coupled datasets that incorporating insect holes, lesions, and soil adhesion to strengthen robustness against extreme field disturbances; introducing attention-based multi-damage feature decoupling modules and dedicated micro-lesion enhancement modules combining super-resolution and feature interpolation to improve discriminability in complex scenarios and for tiny lesions;&#xa0;optimizing dynamic weight calculations via approximate computation or hardware-friendly reconstruction, while developing lightweight real-time deployment frameworks integrated with uav near-ground sensing technology to boost efficiency and practical monitoring coverage; and establishing cross-crop pathological transfer learning mechanisms, extending the model to multi-crop disease recognition tasks with self-supervised pre-training to enhance algorithm universality and low-contrast lesion feature mining capabilities. through the above research, it is expected to further improve the applicability and practicality of the model in complex field environments, promoting the development of intelligent plant disease grading technology towards more accurate, efficient, and universal directions.\u003c/p>\u003ca id=\"h7\" name=\"h7\">\u003c/a>\u003ch2>data availability statement\u003c/h2>\u003cp class=\"mb0\">the raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.\u003c/p>\u003ca id=\"h8\" name=\"h8\">\u003c/a>\u003ch2>author contributions\u003c/h2>\u003cp class=\"mb0\">yw: writing &#x2013; original draft. dz: writing &#x2013; original draft. lz:&#xa0;writing &#x2013; review &amp; editing.\u003c/p>\u003ca id=\"h9\" name=\"h9\">\u003c/a>\u003ch2>funding\u003c/h2>\u003cp class=\"mb0\">the author(s) declare financial support was received for the research and/or publication of this article. this work was supported in part by the science and technology key project of the xinjiang production and construction corps (2023ab063) development and application demonstration of green technology for online monitoring of fresh fruits and cold chain logistics in xinjiang.\u003c/p>\u003ca id=\"h10\" name=\"h10\">\u003c/a>\u003ch2>conflict of interest\u003c/h2>\u003cp class=\"mb0\">the authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\u003c/p>\u003ca id=\"h11\" name=\"h11\">\u003c/a>\u003ch2>generative ai statement\u003c/h2>\u003cp class=\"mb15\">the author(s) declare that no generative ai was used in the creation of this manuscript.\u003c/p>\u003cp class=\"mb0\">any alternative text (alt text) provided alongside figures in this article has been generated by frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. if you identify any issues, please contact us.\u003c/p>\u003ca id=\"h12\" name=\"h12\">\u003c/a>\u003ch2>publisher&#x2019;s note\u003c/h2>\u003cp class=\"mb15\">all claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.\u003c/p>\u003ca id=\"h13\" name=\"h13\">\u003c/a>\u003ch2>references\u003c/h2>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b1\" id=\"b1\">\u003c/a> acharya, p., burgers, t., and nguyen, k.-d. (2022). ai-enabled droplet detection and tracking for agricultural spraying systems. \u003ci>computers and electronics in agriculture\u003c/i>, 202, 107325. doi:&#xa0;10.1016/j.compag.2022.107325\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2022.107325\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=p.+acharya&amp;author=t.+burgers&amp;author=k.-d.+nguyen&amp;publication_year=2022&amp;title=ai-enabled%20droplet%20detection%20and%20tracking%20for%20agricultural%20spraying%20systems&amp;journal=computers+and+electronics+in+agriculture&amp;volume=202&amp;pages=107325\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b2\" id=\"b2\">\u003c/a> adaskaveg, j., f&#xf6;rster, h., thompson, d., driever, g., connell, j., buchner, r., et al. (2009). epidemiology and management of walnut blight. \u003ci>walnut res. rep\u003c/i>, 94, 225&#x2013;256. doi:&#xa0;10.1016/j.inpa.2016.10.005\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.inpa.2016.10.005\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.+adaskaveg&amp;author=h.+f%c3%b6rster&amp;author=d.+thompson&amp;author=g.+driever&amp;author=j.+connell&amp;author=r.+buchner&amp;publication_year=2009&amp;title=epidemiology%20and%20management%20of%20walnut%20blight&amp;journal=walnut+res.+rep&amp;volume=94&amp;pages=225\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b3\" id=\"b3\">\u003c/a> arivazhagan, s., shebiah, r. n., ananthi, s., and varthini, s. v. (2013). detection of unhealthy region of plant leaves and classification of plant leaf diseases using texture features. \u003ci>agricultural engineering international: cigr journal\u003c/i>, 15, 211&#x2013;217.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+arivazhagan&amp;author=r.%20n.+shebiah&amp;author=s.+ananthi&amp;author=s.%20v.+varthini&amp;publication_year=2013&amp;title=detection%20of%20unhealthy%20region%20of%20plant%20leaves%20and%20classification%20of%20plant%20leaf%20diseases%20using%20texture%20features&amp;journal=agricultural+engineering+international:+cigr+journal&amp;volume=15&amp;pages=211\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b4\" id=\"b4\">\u003c/a> chaudhary, p., chaudhari, a. k., cheeran, a., and godara, s. (2012). color transform based approach for disease spot detection on plant leaf. \u003ci>international journal of computer science and telecommunications\u003c/i>, 3, 65&#x2013;70.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=p.+chaudhary&amp;author=a.%20k.+chaudhari&amp;author=a.+cheeran&amp;author=s.+godara&amp;publication_year=2012&amp;title=color%20transform%20based%20approach%20for%20disease%20spot%20detection%20on%20plant%20leaf&amp;journal=international+journal+of+computer+science+and+telecommunications&amp;volume=3&amp;pages=65\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b5\" id=\"b5\">\u003c/a> chen, s., zhang, k., zhao, y., sun, y., ban, w., chen, y., et al. (2021). an approach for rice bacterial leaf streak disease segmentation and disease severity estimation. \u003ci>agriculture\u003c/i>, 11, 420. doi:&#xa0;10.3390/agriculture11050420\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agriculture11050420\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+chen&amp;author=k.+zhang&amp;author=y.+zhao&amp;author=y.+sun&amp;author=w.+ban&amp;author=y.+chen&amp;publication_year=2021&amp;title=an%20approach%20for%20rice%20bacterial%20leaf%20streak%20disease%20segmentation%20and%20disease%20severity%20estimation&amp;journal=agriculture&amp;volume=11&amp;pages=420\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b6\" id=\"b6\">\u003c/a> chiang, k.-s., bock, c., el jarroudi, m., delfosse, p., lee, i., and liu, h. (2016). effects of rater bias and assessment method on disease severity estimation with regard to hypothesis testing. \u003ci>phytopathology\u003c/i>, 65, 523&#x2013;535. doi:&#xa0;10.1111/ppa.12435\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1111/ppa.12435\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=k.-s.+chiang&amp;author=c.+bock&amp;author=m.+el%20jarroudi&amp;author=p.+delfosse&amp;author=i.+lee&amp;author=h.+liu&amp;publication_year=2016&amp;title=effects%20of%20rater%20bias%20and%20assessment%20method%20on%20disease%20severity%20estimation%20with%20regard%20to%20hypothesis%20testing&amp;journal=phytopathology&amp;volume=65&amp;pages=523\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b7\" id=\"b7\">\u003c/a> cooke, b. (2006). &#x201c;disease assessment and yield loss,&#x201d; in \u003ci>the epidemiology of plant diseases\u003c/i> (dordrecht: springer), 43&#x2013;80.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=b.+cooke&amp;publication_year=2006&amp;title=disease%20assessment%20and%20yield%20loss&amp;book=the+epidemiology+of+plant+diseases&amp;pages=43\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b8\" id=\"b8\">\u003c/a> ghaiwat, s. n. and arora, p. (2014). detection and classification of plant leaf diseases using image processing techniques: a review. \u003ci>international journal of recent advances in engineering &amp; technology\u003c/i>, 2, 1&#x2013;7.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20n.+ghaiwat&amp;author=p.+arora&amp;publication_year=2014&amp;title=detection%20and%20classification%20of%20plant%20leaf%20diseases%20using%20image%20processing%20techniques%3a%20a%20review&amp;journal=international+journal+of+recent+advances+in+engineering+&amp;+technology&amp;volume=2&amp;pages=1\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b9\" id=\"b9\">\u003c/a> hu, g., wang, h., zhang, y., and wan, m. (2021). detection and severity analysis of tea leaf blight based on deep learning. \u003ci>computers &amp; electrical engineering\u003c/i>, 90, 107023. doi:&#xa0;10.1016/j.compeleceng.2021.107023\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compeleceng.2021.107023\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=g.+hu&amp;author=h.+wang&amp;author=y.+zhang&amp;author=m.+wan&amp;publication_year=2021&amp;title=detection%20and%20severity%20analysis%20of%20tea%20leaf%20blight%20based%20on%20deep%20learning&amp;journal=computers+&amp;+electrical+engineering&amp;volume=90&amp;pages=107023\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b10\" id=\"b10\">\u003c/a> jadhav, s. b. and patil, s. b. (2016). grading of soybean leaf disease based on segmented image using k-means clustering. \u003ci>iaes international journal of artificial intelligence\u003c/i>, 5, 13&#x2013;13. doi:&#xa0;10.11591/ijai.v5.i1.pp13-21\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.11591/ijai.v5.i1.pp13-21\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20b.+jadhav&amp;author=s.%20b.+patil&amp;publication_year=2016&amp;title=grading%20of%20soybean%20leaf%20disease%20based%20on%20segmented%20image%20using%20k-means%20clustering&amp;journal=iaes+international+journal+of+artificial+intelligence&amp;volume=5&amp;pages=13\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b11\" id=\"b11\">\u003c/a> jo, w. j., kim, d. s., sim, h. s., ahn, s. r., lee, h. j., moon, y. h., et al. (2021). estimation of evapotranspiration and water requirements of strawberry plants in greenhouses using environmental data. \u003ci>frontiers in sustainable food systems\u003c/i>, 5, 684808. doi:&#xa0;10.3389/fsufs.2021.684808\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3389/fsufs.2021.684808\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=w.%20j.+jo&amp;author=d.%20s.+kim&amp;author=h.%20s.+sim&amp;author=s.%20r.+ahn&amp;author=h.%20j.+lee&amp;author=y.%20h.+moon&amp;publication_year=2021&amp;title=estimation%20of%20evapotranspiration%20and%20water%20requirements%20of%20strawberry%20plants%20in%20greenhouses%20using%20environmental%20data&amp;journal=frontiers+in+sustainable+food+systems&amp;volume=5&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b12\" id=\"b12\">\u003c/a> khan, m. a., ali, m., shah, m., mahmood, t., ahmad, m., jhanjhi, n., et al. (2021). machine learning-based detection and classification of walnut fungi diseases. \u003ci>intelligent automation &amp; soft computing\u003c/i>, 30, 771&#x2013;785. doi:&#xa0;10.32604/iasc.2021.018039\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.32604/iasc.2021.018039\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=m.%20a.+khan&amp;author=m.+ali&amp;author=m.+shah&amp;author=t.+mahmood&amp;author=m.+ahmad&amp;author=n.+jhanjhi&amp;publication_year=2021&amp;title=machine%20learning-based%20detection%20and%20classification%20of%20walnut%20fungi%20diseases&amp;journal=intelligent+automation+&amp;+soft+computing&amp;volume=30&amp;pages=771\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b13\" id=\"b13\">\u003c/a> kim, s.-k. and ahn, j.-g. (2021). tomato crop diseases classification models using deep cnn-based architectures. \u003ci>j korea acad-ind coop soc\u003c/i>, 22, 7&#x2013;14. doi:&#xa0;10.5762/kais.2021.22.5.7\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.5762/kais.2021.22.5.7\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.-k.+kim&amp;author=j.-g.+ahn&amp;publication_year=2021&amp;title=tomato%20crop%20diseases%20classification%20models%20using%20deep%20cnn-based%20architectures&amp;journal=j+korea+acad-ind+coop+soc&amp;volume=22&amp;pages=7\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b14\" id=\"b14\">\u003c/a> lamichhane, j. r. (2014). xanthomonas arboricola diseases of stone fruit, almond, and walnut trees: progress toward understanding and management. \u003ci>plant disease\u003c/i>, 98, 1600&#x2013;1610. doi:&#xa0;10.1094/pdis-08-14-0831-fe\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30703892/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-08-14-0831-fe\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.%20r.+lamichhane&amp;publication_year=2014&amp;title=xanthomonas%20arboricola%20diseases%20of%20stone%20fruit%2c%20almond%2c%20and%20walnut%20trees%3a%20progress%20toward%20understanding%20and%20management&amp;journal=plant+disease&amp;volume=98&amp;pages=1600\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b15\" id=\"b15\">\u003c/a> li, x., zha, t., liu, p., bourque, c. p.-a., jia, x., and tian, y. (2023). interannual variation in gross ecosystem production and evapotranspiration in a temperate semiarid grassland undergoing vegetation recovery. \u003ci>agricultural and forest meteorology\u003c/i>, 341, 109672. doi:&#xa0;10.1016/j.agrformet.2023.109672\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.agrformet.2023.109672\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=x.+li&amp;author=t.+zha&amp;author=p.+liu&amp;author=c.%20p.-a.+bourque&amp;author=x.+jia&amp;author=y.+tian&amp;publication_year=2023&amp;title=interannual%20variation%20in%20gross%20ecosystem%20production%20and%20evapotranspiration%20in%20a%20temperate%20semiarid%20grassland%20undergoing%20vegetation%20recovery&amp;journal=agricultural+and+forest+meteorology&amp;volume=341&amp;pages=109672\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b16\" id=\"b16\">\u003c/a> lin, k., gong, l., huang, y., liu, c., and pan, j. (2019). deep learning-based segmentation and quantification of cucumber powdery mildew using convolutional neural network. \u003ci>frontiers in plant science\u003c/i>, 10, 155. doi:&#xa0;10.3389/fpls.2019.00155\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30891048/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.3389/fpls.2019.00155\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=k.+lin&amp;author=l.+gong&amp;author=y.+huang&amp;author=c.+liu&amp;author=j.+pan&amp;publication_year=2019&amp;title=deep%20learning-based%20segmentation%20and%20quantification%20of%20cucumber%20powdery%20mildew%20using%20convolutional%20neural%20network&amp;journal=frontiers+in+plant+science&amp;volume=10&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b17\" id=\"b17\">\u003c/a> ma, j., du, k., zhang, l., zheng, f., chu, j., and sun, z. (2017). a segmentation method for greenhouse vegetable foliar disease spots images using color information and region growing. \u003ci>computers and electronics in agriculture\u003c/i>, 142, 110&#x2013;117. doi:&#xa0;10.1016/j.compag.2017.08.023\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2017.08.023\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=j.+ma&amp;author=k.+du&amp;author=l.+zhang&amp;author=f.+zheng&amp;author=j.+chu&amp;author=z.+sun&amp;publication_year=2017&amp;title=a%20segmentation%20method%20for%20greenhouse%20vegetable%20foliar%20disease%20spots%20images%20using%20color%20information%20and%20region%20growing&amp;journal=computers+and+electronics+in+agriculture&amp;volume=142&amp;pages=110\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b18\" id=\"b18\">\u003c/a> mao, r., wang, z., li, f., zhou, j., chen, y., and hu, x. (2023). gseyolox-s: an improved lightweight network for identifying the severity of wheat fusarium head blight. \u003ci>agronomy\u003c/i>, 13, 242. doi:&#xa0;10.3390/agronomy13010242\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agronomy13010242\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=r.+mao&amp;author=z.+wang&amp;author=f.+li&amp;author=j.+zhou&amp;author=y.+chen&amp;author=x.+hu&amp;publication_year=2023&amp;title=gseyolox-s%3a%20an%20improved%20lightweight%20network%20for%20identifying%20the%20severity%20of%20wheat%20fusarium%20head%20blight&amp;journal=agronomy&amp;volume=13&amp;pages=242\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b19\" id=\"b19\">\u003c/a> mcgranahan, g. and leslie, c. (1991). walnuts (juglans). \u003ci>molecules\u003c/i>, 907&#x2013;924. doi:&#xa0;10.17660/actahortic.1991.290.20\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.17660/actahortic.1991.290.20\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=g.+mcgranahan&amp;author=c.+leslie&amp;publication_year=1991&amp;title=walnuts%20%28juglans%29&amp;journal=molecules&amp;pages=907\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b20\" id=\"b20\">\u003c/a> mircetich, s. m., sanborn, r., and ramos, d. (1980). natural spread, graft-transmission, and possible etiology of walnut blackline disease. \u003ci>phytopathology\u003c/i>, 70, 962&#x2013;968. doi:&#xa0;10.1094/phyto-70-962\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/phyto-70-962\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.%20m.+mircetich&amp;author=r.+sanborn&amp;author=d.+ramos&amp;publication_year=1980&amp;title=natural%20spread%2c%20graft-transmission%2c%20and%20possible%20etiology%20of%20walnut%20blackline%20disease&amp;journal=phytopathology&amp;volume=70&amp;pages=962\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b21\" id=\"b21\">\u003c/a> moragrega, c., matias, j., alet&#xe0;, n., montesinos, e., and rovira, m. (2011). apical necrosis and premature drop of persian (english) walnut fruit caused by xanthomonas arboricola pv. juglandis. \u003ci>plant disease\u003c/i>, 95, 1565&#x2013;1570. doi:&#xa0;10.1094/pdis-03-11-0259\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30732020/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-03-11-0259\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=c.+moragrega&amp;author=j.+matias&amp;author=n.+alet%c3%a0&amp;author=e.+montesinos&amp;author=m.+rovira&amp;publication_year=2011&amp;title=apical%20necrosis%20and%20premature%20drop%20of%20persian%20%28english%29%20walnut%20fruit%20caused%20by%20xanthomonas%20arboricola%20pv.%20juglandis&amp;journal=plant+disease&amp;volume=95&amp;pages=1565\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b22\" id=\"b22\">\u003c/a> ngugi, l. c., abdelwahab, m., and abo-zahhad, m. (2020). tomato leaf segmentation algorithms for mobile phone applications using deep learning. \u003ci>computers and electronics in agriculture\u003c/i>, 178, 105788. doi:&#xa0;10.1016/j.compag.2020.105788\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2020.105788\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=l.%20c.+ngugi&amp;author=m.+abdelwahab&amp;author=m.+abo-zahhad&amp;publication_year=2020&amp;title=tomato%20leaf%20segmentation%20algorithms%20for%20mobile%20phone%20applications%20using%20deep%20learning&amp;journal=computers+and+electronics+in+agriculture&amp;volume=178&amp;pages=105788\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b23\" id=\"b23\">\u003c/a> ozturk, o., sarica, b., and seker, d. z. (2025). interpretable and robust ensemble deep learning framework for tea leaf disease classification. \u003ci>horticulturae\u003c/i>, 11, 437. doi:&#xa0;10.3390/horticulturae11040437\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/horticulturae11040437\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=o.+ozturk&amp;author=b.+sarica&amp;author=d.%20z.+seker&amp;publication_year=2025&amp;title=interpretable%20and%20robust%20ensemble%20deep%20learning%20framework%20for%20tea%20leaf%20disease%20classification&amp;journal=horticulturae&amp;volume=11&amp;pages=437\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b24\" id=\"b24\">\u003c/a> parashar, n., johri, p., khan, a. a., gaur, n., and kadry, s. (2024). an integrated analysis of yield prediction models: a comprehensive review of advancements and challenges. \u003ci>computers, materials &amp; continua\u003c/i>, 80 (1). doi:&#xa0;10.32604/cmc.2024.050240\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.32604/cmc.2024.050240\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=n.+parashar&amp;author=p.+johri&amp;author=a.%20a.+khan&amp;author=n.+gaur&amp;author=s.+kadry&amp;publication_year=2024&amp;title=an%20integrated%20analysis%20of%20yield%20prediction%20models%3a%20a%20comprehensive%20review%20of%20advancements%20and%20challenges&amp;journal=computers,+materials+&amp;+continua&amp;volume=80&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b25\" id=\"b25\">\u003c/a> picon, a., alvarez-gila, a., seitz, m., ortiz-barredo, a., echazarra, j., and johannes, a. (2019). deep convolutional neural networks for mobile capture device-based crop disease classification in the wild. \u003ci>computers and electronics in agriculture\u003c/i>, 161, 280&#x2013;290. doi:&#xa0;10.1016/j.compag.2018.04.002\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2018.04.002\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=a.+picon&amp;author=a.+alvarez-gila&amp;author=m.+seitz&amp;author=a.+ortiz-barredo&amp;author=j.+echazarra&amp;author=a.+johannes&amp;publication_year=2019&amp;title=deep%20convolutional%20neural%20networks%20for%20mobile%20capture%20device-based%20crop%20disease%20classification%20in%20the%20wild&amp;journal=computers+and+electronics+in+agriculture&amp;volume=161&amp;pages=280\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b26\" id=\"b26\">\u003c/a> rastogi, a., arora, r., and sharma, s. (2015). &#x201c;leaf disease detection and grading using computer vision technology &amp; fuzzy logic,&#x201d; in paper presented at the 2015 2nd international conference on signal processing and integrated networks (spin).\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=a.+rastogi&amp;author=r.+arora&amp;author=s.+sharma&amp;publication_year=2015&amp;title=leaf%20disease%20detection%20and%20grading%20using%20computer%20vision%20technology%20%26%20fuzzy%20logic&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b27\" id=\"b27\">\u003c/a> rustia, d. j. a., lee, w.-c., lu, c.-y., wu, y.-f., shih, p.-y., and chen, s.-k. (2023). edge-based wireless imaging system for continuous monitoring of insect pests in a remote outdoor mango orchard. \u003ci>computers and electronics in agriculture\u003c/i>, 211, 108019. doi:&#xa0;10.1016/j.compag.2023.108019\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.compag.2023.108019\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=d.%20j.%20a.+rustia&amp;author=w.-c.+lee&amp;author=c.-y.+lu&amp;author=y.-f.+wu&amp;author=p.-y.+shih&amp;author=s.-k.+chen&amp;publication_year=2023&amp;title=edge-based%20wireless%20imaging%20system%20for%20continuous%20monitoring%20of%20insect%20pests%20in%20a%20remote%20outdoor%20mango%20orchard&amp;journal=computers+and+electronics+in+agriculture&amp;volume=211&amp;pages=108019\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b28\" id=\"b28\">\u003c/a> shi, t., liu, y., zheng, x., hu, k., huang, h., liu, h., et al. (2023). recent advances in plant disease severity assessment using convolutional neural networks. \u003ci>scientific reports\u003c/i>, 13, 2336. doi:&#xa0;10.1038/s41598-023-29230-7\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/36759626/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1038/s41598-023-29230-7\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=t.+shi&amp;author=y.+liu&amp;author=x.+zheng&amp;author=k.+hu&amp;author=h.+huang&amp;author=h.+liu&amp;publication_year=2023&amp;title=recent%20advances%20in%20plant%20disease%20severity%20assessment%20using%20convolutional%20neural%20networks&amp;journal=scientific+reports&amp;volume=13&amp;pages=2336\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b29\" id=\"b29\">\u003c/a> singh, u. p., chouhan, s. s., jain, s., and jain, s. (2019). multilayer convolution neural network for the classification of mango leaves infected by anthracnose disease. \u003ci>ieee access\u003c/i>, 7, 43721&#x2013;43729. doi:&#xa0;10.1109/access.2019.2907383\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/access.2019.2907383\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=u.%20p.+singh&amp;author=s.%20s.+chouhan&amp;author=s.+jain&amp;author=s.+jain&amp;publication_year=2019&amp;title=multilayer%20convolution%20neural%20network%20for%20the%20classification%20of%20mango%20leaves%20infected%20by%20anthracnose%20disease&amp;journal=ieee+access&amp;volume=7&amp;pages=43721\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b30\" id=\"b30\">\u003c/a> su, y., wang, j., li, j., wang, l., wang, k., li, a., et al. (2023). spatiotemporal changes and driving factors of reference evapotranspiration and crop evapotranspiration for cotton production in china from 1960 to 2019. \u003ci>frontiers in environmental science\u003c/i>, 11, 1251789. doi:&#xa0;10.3389/fenvs.2023.1251789\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3389/fenvs.2023.1251789\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=y.+su&amp;author=j.+wang&amp;author=j.+li&amp;author=l.+wang&amp;author=k.+wang&amp;author=a.+li&amp;publication_year=2023&amp;title=spatiotemporal%20changes%20and%20driving%20factors%20of%20reference%20evapotranspiration%20and%20crop%20evapotranspiration%20for%20cotton%20production%20in%20china%20from%201960%20to%202019&amp;journal=frontiers+in+environmental+science&amp;volume=11&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b31\" id=\"b31\">\u003c/a> tripathi, r. (2021). &#x201c;a deep learning approach for plant material disease identification,&#x201d; in paper presented at the iop conference series: materials science and engineering.\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=r.+tripathi&amp;publication_year=2021&amp;title=a%20deep%20learning%20approach%20for%20plant%20material%20disease%20identification&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b32\" id=\"b32\">\u003c/a> vishnoi, v. k., kumar, k., kumar, b., mohan, s., and khan, a. a. (2022). detection of apple plant diseases using leaf images through convolutional neural network . \u003ci>ieee access\u003c/i>, 11, 6594&#x2013;6609. doi:&#xa0;10.1109/access.2022.3232917\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/access.2022.3232917\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=v.%20k.+vishnoi&amp;author=k.+kumar&amp;author=b.+kumar&amp;author=s.+mohan&amp;author=a.%20a.+khan&amp;publication_year=2022&amp;title=detection%20of%20apple%20plant%20diseases%20using%20leaf%20images%20through%20convolutional%20neural%20network&amp;journal=ieee+access&amp;volume=11&amp;pages=6594\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b33\" id=\"b33\">\u003c/a> wang, f., dun, c., tang, t., duan, y., guo, x., and you, j. (2022). boeremia exigua causes leaf spot of walnut trees (juglans regia) in china. \u003ci>plant disease\u003c/i>, 106, 1993. doi:&#xa0;10.1094/pdis-10-21-2304-pdn\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/pdis-10-21-2304-pdn\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=f.+wang&amp;author=c.+dun&amp;author=t.+tang&amp;author=y.+duan&amp;author=x.+guo&amp;author=j.+you&amp;publication_year=2022&amp;title=boeremia%20exigua%20causes%20leaf%20spot%20of%20walnut%20trees%20%28juglans%20regia%29%20in%20china&amp;journal=plant+disease&amp;volume=106&amp;pages=1993\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b34\" id=\"b34\">\u003c/a> wang, q.-h., fan, k., li, d.-w., han, c.-m., qu, y.-y., qi, y.-k., et al. (2020). identification, virulence and fungicide sensitivity of colletotrichum gloeosporioides ss responsible for walnut anthracnose disease in china. \u003ci>plant disease\u003c/i>, 104, 1358&#x2013;1368. doi:&#xa0;10.1094/pdis-12-19-2569-re\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/32196416/\" target=\"_blank\">pubmed abstract\u003c/a> | \u003ca href=\"https://doi.org/10.1094/pdis-12-19-2569-re\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=q.-h.+wang&amp;author=k.+fan&amp;author=d.-w.+li&amp;author=c.-m.+han&amp;author=y.-y.+qu&amp;author=y.-k.+qi&amp;publication_year=2020&amp;title=identification%2c%20virulence%20and%20fungicide%20sensitivity%20of%20colletotrichum%20gloeosporioides%20ss%20responsible%20for%20walnut%20anthracnose%20disease%20in%20china&amp;journal=plant+disease&amp;volume=104&amp;pages=1358\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b35\" id=\"b35\">\u003c/a> weber, b. c. (1980). \u003ci>how to diagnose black walnut damage\u003c/i> vol. 57 (north central forest experiment station, forest service, us department of agriculture).\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"http://scholar.google.com/scholar_lookup?author=b.%20c.+weber&amp;publication_year=1980&amp;book=how+to+diagnose+black+walnut+damage&amp;volume=57&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b36\" id=\"b36\">\u003c/a> xu, w. (2024). hcf-net: hierarchicalcontextfusion network forinfrared small object detection. \u003ci>arxiv\u003c/i>. arxiv, 2403.10778. doi:&#xa0;10.1109/icme57554.2024.10687431\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1109/icme57554.2024.10687431\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=w.+xu&amp;publication_year=2024&amp;title=hcf-net%3a%20hierarchicalcontextfusion%20network%20forinfrared%20small%20object%20detection&amp;journal=arxiv&amp;volume=arxiv&amp;\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b37\" id=\"b37\">\u003c/a> yang, c., deng, y., wang, f., yang, h., xu, x., zeng, q., et al. (2021). brown leaf spot on \u003ci>juglans sigillata\u003c/i> caused by \u003ci>ophiognomonia leptostyla\u003c/i> in sichuan, china. \u003ci>plant disease\u003c/i>, 105, 4160. doi:&#xa0;10.1094/pdis-02-21-0344-pdn\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1094/pdis-02-21-0344-pdn\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=c.+yang&amp;author=y.+deng&amp;author=f.+wang&amp;author=h.+yang&amp;author=x.+xu&amp;author=q.+zeng&amp;publication_year=2021&amp;title=brown%20leaf%20spot%20on%20juglans%20sigillata%20caused%20by%20ophiognomonia%20leptostyla%20in%20sichuan%2c%20china&amp;journal=plant+disease&amp;volume=105&amp;pages=4160\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b38\" id=\"b38\">\u003c/a> yang, q., duan, s., and wang, l. (2022). efficient identification of apple leaf diseases in the wild using convolutional neural networks. \u003ci>agronomy\u003c/i>, 12, 2784. doi:&#xa0;10.3390/agronomy12112784\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.3390/agronomy12112784\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=q.+yang&amp;author=s.+duan&amp;author=l.+wang&amp;publication_year=2022&amp;title=efficient%20identification%20of%20apple%20leaf%20diseases%20in%20the%20wild%20using%20convolutional%20neural%20networks&amp;journal=agronomy&amp;volume=12&amp;pages=2784\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b39\" id=\"b39\">\u003c/a> zarei, s., taghavi, s. m., banihashemi, z., hamzehzarghani, h., and osdaghi, e. (2019). etiology of leaf spot and fruit canker symptoms on stone fruits and nut trees in iran. \u003ci>journal of plant pathology\u003c/i>, 101, 1133&#x2013;1142. doi:&#xa0;10.1007/s42161-019-00283-w\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1007/s42161-019-00283-w\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=s.+zarei&amp;author=s.%20m.+taghavi&amp;author=z.+banihashemi&amp;author=h.+hamzehzarghani&amp;author=e.+osdaghi&amp;publication_year=2019&amp;title=etiology%20of%20leaf%20spot%20and%20fruit%20canker%20symptoms%20on%20stone%20fruits%20and%20nut%20trees%20in%20iran&amp;journal=journal+of+plant+pathology&amp;volume=101&amp;pages=1133\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003cdiv class=\"references\" style=\"margin-bottom:0.5em;\">\u003cp class=\"referencescopy1\">\u003ca name=\"b40\" id=\"b40\">\u003c/a> zhang, y., li, x., &#x160;im&#x16f;nek, j., shi, h., chen, n., and hu, q. (2023). quantifying water and salt movement in a soil-plant system of a corn field using hydrus (2d/3d) and the stable isotope method. \u003ci>agricultural water management\u003c/i>, 288, 108492. doi:&#xa0;10.1016/j.agwat.2023.108492\u003c/p>\u003cp class=\"referencescopy2\">\u003ca href=\"https://doi.org/10.1016/j.agwat.2023.108492\" target=\"_blank\">crossref full text\u003c/a> | \u003ca href=\"http://scholar.google.com/scholar_lookup?author=y.+zhang&amp;author=x.+li&amp;author=j.+%c5%a0im%c5%afnek&amp;author=h.+shi&amp;author=n.+chen&amp;author=q.+hu&amp;publication_year=2023&amp;title=quantifying%20water%20and%20salt%20movement%20in%20a%20soil-plant%20system%20of%20a%20corn%20field%20using%20hydrus%20%282d%2f3d%29%20and%20the%20stable%20isotope%20method&amp;journal=agricultural+water+management&amp;volume=288&amp;pages=108492\" target=\"_blank\">google scholar\u003c/a>\u003c/p>\u003c/div>\u003c/div>\u003cdiv class=\"thinlinem20\">\u003c/div>\u003cdiv class=\"abstractsummary\">\u003cp>\u003cspan>keywords:\u003c/span> walnut, brown spot disease (\u003ci>ophiognomonia leptostyla\u003c/i>), hierarchical feature selection, edge features perception, adaptive multi-scale dilated convolution, disease grading\u003c/p>\u003cp>\u003cspan>citation:\u003c/span> wei y, zeng d and zheng l (2025) a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution. \u003ci>front. plant sci.\u003c/i> 16:1641677. doi: 10.3389/fpls.2025.1641677\u003c/p>\u003cp id=\"timestamps\">\u003cspan>received:\u003c/span> 05 june 2025; \u003cspan>accepted:\u003c/span> 09 september 2025;\u003cbr>\u003cspan>published:\u003c/span> 03 october 2025.\u003c/p>\u003cdiv>\u003cp>edited by:\u003c/p>\u003ca href=\"https://loop.frontiersin.org/people/1037951\">ravinder kumar\u003c/a>, indian agricultural research institute (icar), india\u003c/div>\u003cdiv>\u003cp>reviewed by:\u003c/p>\u003ca href=\"https://loop.frontiersin.org/people/2082015\">geza bujdoso\u003c/a>, hungarian university of agricultural and life sciences, hungary\u003cbr>\r\n\u003ca href=\"https://loop.frontiersin.org/people/3078480\">vasudha vedula\u003c/a>, university of texas of the permian basin, united states\u003c/div>\u003cp>\u003cspan>copyright\u003c/span> &#xa9; 2025 wei, zeng and zheng. this is an open-access article distributed under the terms of the \u003ca rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\" target=\"_blank\">creative commons attribution license (cc by)\u003c/a>. the use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. no use, distribution or reproduction is permitted which does not comply with these terms.\u003c/p>\u003cp>\u003cspan>*correspondence:\u003c/span> liangfang zheng, \u003ca id=\"encmail\">mtgxnjaynta0othamtyzlmnvbq==\u003c/a>\u003c/p>\u003cp>\u003cspan>\u003csup>&#x2020;\u003c/sup>\u003c/span>these authors have contributed equally to this work\u003c/p>\u003cdiv class=\"clear\">\u003c/div>\u003c/div>","\u003cul class=\"flyoutjournal\">\u003cli>\u003ca href=\"#h1\">abstract\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h2\">1 introduction\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h3\">2 materials and methods\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h4\">3 experiments and results analysis\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h5\">4 conclusion\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h6\">5 discussion and future work\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h7\">data availability statement\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h8\">author contributions\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h9\">funding\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h10\">conflict of interest\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h11\">generative ai statement\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h12\">publisher&#x2019;s note\u003c/a>\u003c/li>\u003cli>\u003ca href=\"#h13\">references\u003c/a>\u003c/li>\u003c/ul>",[629,636,642],{"name":630,"fileserverpackageentryid":19,"fileserverid":631,"fileserverversionnumber":632,"type":633},"epub.epub","1641677/epub",2,{"code":634,"name":635},"epub","epub",{"name":637,"fileserverpackageentryid":637,"fileserverid":638,"fileserverversionnumber":632,"type":639},"fpls-16-1641677.xml","1641677/xml",{"code":640,"name":641},"nlm_xml","xml",{"name":643,"fileserverpackageentryid":19,"fileserverid":644,"fileserverversionnumber":632,"type":645},"publishers-proof.pdf","1641677/publishers-proof",{"code":646,"name":647},"pdf","pdf","v3",{"title":650,"link":651,"meta":655,"script":748},"frontiers | a precision grading method for walnut leaf brown spot disease integrating hierarchical feature selection and dynamic multi-scale convolution",[652],{"rel":653,"href":654},"canonical","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/full",[656,659,662,664,667,671,673,677,680,683,686,688,690,692,694,696,699,702,704,707,709,711,714,717,720,723,726,730,734,737,740,743,745],{"hid":657,"property":657,"name":657,"content":658},"description","walnut leaf brown spot disease, caused by ophiognomonia leptostyla, is among the most destructive fungal diseases in walnut cultivation. in the development o...",{"hid":660,"property":660,"name":661,"content":650},"og:title","title",{"hid":663,"property":663,"name":657,"content":658},"og:description",{"hid":665,"name":665,"content":666},"keywords","walnut,brown spot disease (ophiognomonia leptostyla),hierarchical feature selection,edge features perception,adaptive multi-scale dilated convolution,disease grading",{"hid":668,"property":668,"name":669,"content":670},"og:site_name","site_name","frontiers",{"hid":672,"property":672,"name":385,"content":404},"og:image",{"hid":674,"property":674,"name":675,"content":676},"og:type","type","article",{"hid":678,"property":678,"name":679,"content":654},"og:url","url",{"hid":681,"name":681,"content":682},"twitter:card","summary_large_image",{"hid":684,"name":684,"content":685},"citation_volume","16",{"hid":687,"name":687,"content":116},"citation_journal_title",{"hid":689,"name":689,"content":670},"citation_publisher",{"hid":691,"name":691,"content":611},"citation_journal_abbrev",{"hid":693,"name":693,"content":612},"citation_issn",{"hid":695,"name":695,"content":538},"citation_doi",{"hid":697,"name":697,"content":698},"citation_firstpage","1641677",{"hid":700,"name":700,"content":701},"citation_language","english",{"hid":703,"name":703,"content":539},"citation_title",{"hid":705,"name":705,"content":706},"citation_keywords","walnut; brown spot disease (ophiognomonia leptostyla); hierarchical feature selection; edge features perception; adaptive multi-scale dilated convolution; disease grading",{"hid":708,"name":708,"content":544},"citation_abstract",{"hid":710,"name":710,"content":553},"citation_article_type",{"hid":712,"name":712,"content":713},"citation_pdf_url","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/pdf",{"hid":715,"name":715,"content":716},"citation_xml_url","https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2025.1641677/xml",{"hid":718,"name":718,"content":719},"citation_fulltext_world_readable","yes",{"hid":721,"name":721,"content":722},"citation_online_date","2025/09/09",{"hid":724,"name":724,"content":725},"citation_publication_date","2025/10/03",{"hid":727,"name":728,"content":729},"citation_author_0","citation_author","wei, yuting ",{"hid":731,"name":732,"content":733},"citation_author_institution_0","citation_author_institution","college of information engineering, tarim university, china",{"hid":735,"name":728,"content":736},"citation_author_1","zeng, debin ",{"hid":738,"name":732,"content":739},"citation_author_institution_1","key laboratory of tarim oasis agriculture, ministry of education, tarim university, china",{"hid":741,"name":728,"content":742},"citation_author_2","zheng, liangfang ",{"hid":744,"name":732,"content":739},"citation_author_institution_2",{"hid":746,"name":746,"content":747},"dc.identifier","doi:10.3389/fpls.2025.1641677",[749,752,754,756,758],{"src":750,"body":13,"type":751,"async":13},"https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6","text/javascript",{"src":753,"body":13,"type":751,"async":13},"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/mathjax.js?config=tex-mml-am_chtml",{"src":755,"body":13,"type":751,"async":13},"https://d1bxh8uas1mnw7.cloudfront.net/assets/altmetric_badges-f0bc9b243ff5677d05460c1eb71834ca998946d764eb3bc244ab4b18ba50d21e.js",{"src":757,"body":13,"type":751,"async":13},"https://api.altmetric.com/v1/doi/10.3389/fpls.2025.1641677?callback=_altmetric.embed_callback&domain=www.frontiersin.org&key=3c130976ca2b8f2e88f8377633751ba1&cache_until=14-15",{"src":759,"body":13,"type":751,"async":13},"https://crossmark-cdn.crossref.org/widget/v2.0/widget.js",{"articlehubslug":19,"articlehubpage":761,"articlehubarticleslist":762,"canjournalhasarticlehub":367,"articledoilist":763},{},[],[],{"title":19,"image":-1,"breadcrumbs":765,"linkscollection":766,"metricscollection":768},[],{"total":389,"items":767},[],{"total":389,"items":769},[]] window.__nuxt__={};window.__nuxt__.config={public:{isdevmode:false,appname:"article-pages-2024",baseurl:"https://www.frontiersin.org",domain:"frontiersin.org",loopurl:"https://loop.frontiersin.org",ssmaindomain:"frontiersin.org",contentfulurl:"https://graphql.contentful.com/content/v1/spaces",spaceid:1,spacename:"frontiers",livespaceid:1,tenantlogourl:"",frontiersgraphurl:"https://apollo-federation-gateway.frontiersin.org",registrationapiurl:"https://onboarding-ui.frontiersin.org",emaildigestapiurl:"https://api-email-digest.frontiersin.org",journalfilesapiurl:"https://files-journal-api.frontiersin.org",searchapiurl:"https://search-api.frontiersin.org",productionforumapiurl:"https://production-forum-api-v2.frontiersin.org",gtmserverurl:"https://tag-manager.frontiersin.org",gtmid:"gtm-m322fv2",gtmauth:"owvbwxfajr21yqv1fe1caq",gtmpreview:"env-1",figsharepluginurl:"https://widgets.figshare.com/static/figshare.js",figshareapiurl:"https://api.figshare.com/v2/collections/search",figsharetimeout:3000,crossmarkpublisheddate:"2015/08/24",googlerecaptchasitekey:"6ldg3i0uaaaaaoc4quh35ubhgjotehp_stxhgr_v",googlerecaptchakeyname:"frontiersrecaptchav2",faviconsize16:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_16-tenantfavicon-frontiers.png",faviconsize32:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_32-tenantfavicon-frontiers.png",faviconsize180:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_180-tenantfavicon-frontiers.png",faviconsize192:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_192-tenantfavicon-frontiers.png",faviconsize512:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_512-tenantfavicon-frontiers.png",socialmediaimg:"https://brand.frontiersin.org/m/1c8bcb536c789e11/guidelines-frontiers_logo_1200x628_1-91to1.png",linkedarticlecopytext:"'{\"articletypecopytext\":[{\"articletypeid\":0,\"originalarticlecopytext\":\"part of this article's content has been mentioned in:\",\"linkedarticlecopytext\":\"this article mentions parts of:\"},{\"articletypeid\":122,\"originalarticlecopytext\":\"parts of this article's content have been modified or rectified in:\",\"linkedarticlecopytext\":\"this article is an erratum on:\"},{\"articletypeid\":129,\"originalarticlecopytext\":\"parts of this article's content have been modified or rectified in:\",\"linkedarticlecopytext\":\"this article is an addendum to:\"},{\"articletypeid\":128,\"originalarticlecopytext\":\"a correction has been applied to this article in:\",\"linkedarticlecopytext\":\"this article is a correction to:\"},{\"articletypeid\":134,\"originalarticlecopytext\":\"a retraction of this article was approved in:\",\"linkedarticlecopytext\":\"this article is a retraction of:\"},{\"articletypeid\":29,\"originalarticlecopytext\":\"a commentary has been posted on this article:\",\"linkedarticlecopytext\":\"this article is a commentary on:\"},{\"articletypeid\":30,\"originalarticlecopytext\":\"a commentary has been posted on this article:\",\"linkedarticlecopytext\":\"this article is a commentary on:\"}],\"articleidcopytext\":[]}'\n",acceptedarticleexcludedjournalids:"'[2766,979]'\n",articletypeconfigurablelabel:"\u003c\u003carticle-type:uppercase>> article",settingsfeaturesswitchers:"'{\"displaytitlepilllabels\":true,\"displayrelatedarticlesbox\":true,\"showeditors\":true,\"showreviewers\":true,\"showloopimpactlink\":true,\"enablefigshare\":false,\"usexmlimages\":true}'\n",journalswitharticlehub:"'{\"strings\":[\"science\"]}'\n",terminologysettings:"'{\"terms\":[{\"sequencenumber\":1,\"key\":\"frontiers\",\"tenantterm\":\"frontiers\",\"frontiersdefaultterm\":\"frontiers\",\"category\":\"customer\"},{\"sequencenumber\":2,\"key\":\"submission_system\",\"tenantterm\":\"submission system\",\"frontiersdefaultterm\":\"submission system\",\"category\":\"product\"},{\"sequencenumber\":3,\"key\":\"public_pages\",\"tenantterm\":\"public pages\",\"frontiersdefaultterm\":\"public pages\",\"category\":\"product\"},{\"sequencenumber\":4,\"key\":\"my_frontiers\",\"tenantterm\":\"my frontiers\",\"frontiersdefaultterm\":\"my frontiers\",\"category\":\"product\"},{\"sequencenumber\":5,\"key\":\"digital_editorial_office\",\"tenantterm\":\"digital editorial office\",\"frontiersdefaultterm\":\"digital editorial office\",\"category\":\"product\"},{\"sequencenumber\":6,\"key\":\"deo\",\"tenantterm\":\"deo\",\"frontiersdefaultterm\":\"deo\",\"category\":\"product\"},{\"sequencenumber\":7,\"key\":\"digital_editorial_office_for_chiefs\",\"tenantterm\":\"digital editorial office for chiefs\",\"frontiersdefaultterm\":\"digital editorial office for chiefs\",\"category\":\"product\"},{\"sequencenumber\":8,\"key\":\"digital_editorial_office_for_eof\",\"tenantterm\":\"digital editorial office for eof\",\"frontiersdefaultterm\":\"digital editorial office for eof\",\"category\":\"product\"},{\"sequencenumber\":9,\"key\":\"editorial_office\",\"tenantterm\":\"editorial office\",\"frontiersdefaultterm\":\"editorial office\",\"category\":\"product\"},{\"sequencenumber\":10,\"key\":\"eof\",\"tenantterm\":\"eof\",\"frontiersdefaultterm\":\"eof\",\"category\":\"product\"},{\"sequencenumber\":11,\"key\":\"research_topic_management\",\"tenantterm\":\"research topic management\",\"frontiersdefaultterm\":\"research topic management\",\"category\":\"product\"},{\"sequencenumber\":12,\"key\":\"review_forum\",\"tenantterm\":\"review forum\",\"frontiersdefaultterm\":\"review forum\",\"category\":\"product\"},{\"sequencenumber\":13,\"key\":\"accounting_office\",\"tenantterm\":\"accounting office\",\"frontiersdefaultterm\":\"accounting office\",\"category\":\"product\"},{\"sequencenumber\":14,\"key\":\"aof\",\"tenantterm\":\"aof\",\"frontiersdefaultterm\":\"aof\",\"category\":\"product\"},{\"sequencenumber\":15,\"key\":\"publishing_office\",\"tenantterm\":\"publishing office\",\"frontiersdefaultterm\":\"publishing office\",\"category\":\"product\"},{\"sequencenumber\":16,\"key\":\"production_office\",\"tenantterm\":\"production office forum\",\"frontiersdefaultterm\":\"production office forum\",\"category\":\"product\"},{\"sequencenumber\":17,\"key\":\"pof\",\"tenantterm\":\"pof\",\"frontiersdefaultterm\":\"pof\",\"category\":\"product\"},{\"sequencenumber\":18,\"key\":\"book_office_forum\",\"tenantterm\":\"book office forum\",\"frontiersdefaultterm\":\"book office forum\",\"category\":\"product\"},{\"sequencenumber\":19,\"key\":\"bof\",\"tenantterm\":\"bof\",\"frontiersdefaultterm\":\"bof\",\"category\":\"product\"},{\"sequencenumber\":20,\"key\":\"aira\",\"tenantterm\":\"aira\",\"frontiersdefaultterm\":\"aira\",\"category\":\"product\"},{\"sequencenumber\":21,\"key\":\"editorial_board_management\",\"tenantterm\":\"editorial board management\",\"frontiersdefaultterm\":\"editorial board management\",\"category\":\"product\"},{\"sequencenumber\":22,\"key\":\"ebm\",\"tenantterm\":\"ebm\",\"frontiersdefaultterm\":\"ebm\",\"category\":\"product\"},{\"sequencenumber\":23,\"key\":\"domain\",\"tenantterm\":\"domain\",\"frontiersdefaultterm\":\"domain\",\"category\":\"taxonomy\"},{\"sequencenumber\":24,\"key\":\"journal\",\"tenantterm\":\"journal\",\"frontiersdefaultterm\":\"journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":25,\"key\":\"section\",\"tenantterm\":\"section\",\"frontiersdefaultterm\":\"section\",\"category\":\"taxonomy\"},{\"sequencenumber\":26,\"key\":\"domains\",\"tenantterm\":\"domains\",\"frontiersdefaultterm\":\"domains\",\"category\":\"taxonomy\"},{\"sequencenumber\":27,\"key\":\"specialty_section\",\"tenantterm\":\"specialty section\",\"frontiersdefaultterm\":\"specialty section\",\"category\":\"taxonomy\"},{\"sequencenumber\":28,\"key\":\"specialty_journal\",\"tenantterm\":\"specialty journal\",\"frontiersdefaultterm\":\"specialty journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":29,\"key\":\"journals\",\"tenantterm\":\"journals\",\"frontiersdefaultterm\":\"journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":30,\"key\":\"sections\",\"tenantterm\":\"sections\",\"frontiersdefaultterm\":\"sections\",\"category\":\"taxonomy\"},{\"sequencenumber\":31,\"key\":\"specialty_sections\",\"tenantterm\":\"specialty sections\",\"frontiersdefaultterm\":\"specialty sections\",\"category\":\"taxonomy\"},{\"sequencenumber\":32,\"key\":\"specialty_journals\",\"tenantterm\":\"specialty journals\",\"frontiersdefaultterm\":\"specialty journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":33,\"key\":\"manuscript\",\"tenantterm\":\"manuscript\",\"frontiersdefaultterm\":\"manuscript\",\"category\":\"core\"},{\"sequencenumber\":34,\"key\":\"manuscripts\",\"tenantterm\":\"manuscripts\",\"frontiersdefaultterm\":\"manuscripts\",\"category\":\"core\"},{\"sequencenumber\":35,\"key\":\"article\",\"tenantterm\":\"article\",\"frontiersdefaultterm\":\"article\",\"category\":\"core\"},{\"sequencenumber\":36,\"key\":\"articles\",\"tenantterm\":\"articles\",\"frontiersdefaultterm\":\"articles\",\"category\":\"core\"},{\"sequencenumber\":37,\"key\":\"article_type\",\"tenantterm\":\"article type\",\"frontiersdefaultterm\":\"article type\",\"category\":\"core\"},{\"sequencenumber\":38,\"key\":\"article_types\",\"tenantterm\":\"article types\",\"frontiersdefaultterm\":\"article types\",\"category\":\"core\"},{\"sequencenumber\":39,\"key\":\"author\",\"tenantterm\":\"author\",\"frontiersdefaultterm\":\"author\",\"category\":\"label (role)\"},{\"sequencenumber\":40,\"key\":\"authors\",\"tenantterm\":\"authors\",\"frontiersdefaultterm\":\"authors\",\"category\":\"label (role)\"},{\"sequencenumber\":41,\"key\":\"authoring\",\"tenantterm\":\"authoring\",\"frontiersdefaultterm\":\"authoring\",\"category\":\"core\"},{\"sequencenumber\":42,\"key\":\"authored\",\"tenantterm\":\"authored\",\"frontiersdefaultterm\":\"authored\",\"category\":\"core\"},{\"sequencenumber\":43,\"key\":\"accept\",\"tenantterm\":\"accept\",\"frontiersdefaultterm\":\"accept\",\"category\":\"process\"},{\"sequencenumber\":44,\"key\":\"accepted\",\"tenantterm\":\"accepted\",\"frontiersdefaultterm\":\"accepted\",\"category\":\"process\"},{\"sequencenumber\":45,\"key\":\"assistant_field_chief_editor\",\"tenantterm\":\"assistant field chief editor\",\"frontiersdefaultterm\":\"assistant field chief editor\",\"description\":\"an editorial role on a field journal that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":46,\"key\":\"assistant_specialty_chief_editor\",\"tenantterm\":\"assistant specialty chief editor\",\"frontiersdefaultterm\":\"assistant specialty chief editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":47,\"key\":\"assistant_specialty_chief_editors\",\"tenantterm\":\"assistant specialty chief editors\",\"frontiersdefaultterm\":\"assistant specialty chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":48,\"key\":\"associate_editor\",\"tenantterm\":\"associate editor\",\"frontiersdefaultterm\":\"associate editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":49,\"key\":\"specialty_chief_editor\",\"tenantterm\":\"specialty chief editor\",\"frontiersdefaultterm\":\"specialty chief editor\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":50,\"key\":\"specialty_chief_editors\",\"tenantterm\":\"specialty chief editors\",\"frontiersdefaultterm\":\"specialty chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":51,\"key\":\"chief_editor\",\"tenantterm\":\"chief editor\",\"frontiersdefaultterm\":\"chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":52,\"key\":\"chief_editors\",\"tenantterm\":\"chief editors\",\"frontiersdefaultterm\":\"chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":53,\"key\":\"call_for_participation\",\"tenantterm\":\"call for participation\",\"frontiersdefaultterm\":\"call for participation\",\"category\":\"process\"},{\"sequencenumber\":54,\"key\":\"citation\",\"tenantterm\":\"citation\",\"frontiersdefaultterm\":\"citation\",\"category\":\"misc.\"},{\"sequencenumber\":55,\"key\":\"citations\",\"tenantterm\":\"citations\",\"frontiersdefaultterm\":\"citations\",\"category\":\"misc.\"},{\"sequencenumber\":56,\"key\":\"contributor\",\"tenantterm\":\"contributor\",\"frontiersdefaultterm\":\"contributor\",\"category\":\"label (role)\"},{\"sequencenumber\":57,\"key\":\"contributors\",\"tenantterm\":\"contributors\",\"frontiersdefaultterm\":\"contributors\",\"category\":\"label (role)\"},{\"sequencenumber\":58,\"key\":\"corresponding_author\",\"tenantterm\":\"corresponding author\",\"frontiersdefaultterm\":\"corresponding author\",\"category\":\"label (role)\"},{\"sequencenumber\":59,\"key\":\"corresponding_authors\",\"tenantterm\":\"corresponding authors\",\"frontiersdefaultterm\":\"corresponding authors\",\"category\":\"label (role)\"},{\"sequencenumber\":60,\"key\":\"decline\",\"tenantterm\":\"decline\",\"frontiersdefaultterm\":\"decline\",\"category\":\"process\"},{\"sequencenumber\":61,\"key\":\"declined\",\"tenantterm\":\"declined\",\"frontiersdefaultterm\":\"declined\",\"category\":\"process\"},{\"sequencenumber\":62,\"key\":\"reject\",\"tenantterm\":\"reject\",\"frontiersdefaultterm\":\"reject\",\"category\":\"process\"},{\"sequencenumber\":63,\"key\":\"rejected\",\"tenantterm\":\"rejected\",\"frontiersdefaultterm\":\"rejected\",\"category\":\"process\"},{\"sequencenumber\":64,\"key\":\"publish\",\"tenantterm\":\"publish\",\"frontiersdefaultterm\":\"publish\",\"category\":\"core\"},{\"sequencenumber\":65,\"key\":\"published\",\"tenantterm\":\"published\",\"frontiersdefaultterm\":\"published\",\"category\":\"core\"},{\"sequencenumber\":66,\"key\":\"publication\",\"tenantterm\":\"publication\",\"frontiersdefaultterm\":\"publication\",\"category\":\"core\"},{\"sequencenumber\":67,\"key\":\"peer_review\",\"tenantterm\":\"peer review\",\"frontiersdefaultterm\":\"peer review\",\"category\":\"peer review process\"},{\"sequencenumber\":68,\"key\":\"peer_reviewed\",\"tenantterm\":\"peer reviewed\",\"frontiersdefaultterm\":\"peer reviewed\",\"category\":\"peer review process\"},{\"sequencenumber\":69,\"key\":\"initial_validation\",\"tenantterm\":\"initial validation\",\"frontiersdefaultterm\":\"initial validation\",\"category\":\"peer review process\"},{\"sequencenumber\":70,\"key\":\"editorial_assignment\",\"tenantterm\":\"editorial assignment\",\"frontiersdefaultterm\":\"editorial assignment\",\"category\":\"peer review process\"},{\"sequencenumber\":71,\"key\":\"independent_review\",\"tenantterm\":\"independent review\",\"frontiersdefaultterm\":\"independent review\",\"category\":\"peer review process\"},{\"sequencenumber\":72,\"key\":\"interactive_review\",\"tenantterm\":\"interactive review\",\"frontiersdefaultterm\":\"interactive review\",\"category\":\"peer review process\"},{\"sequencenumber\":73,\"key\":\"review\",\"tenantterm\":\"review\",\"frontiersdefaultterm\":\"review\",\"category\":\"peer review process\"},{\"sequencenumber\":74,\"key\":\"reviewing\",\"tenantterm\":\"reviewing\",\"frontiersdefaultterm\":\"reviewing\",\"category\":\"peer review process\"},{\"sequencenumber\":75,\"key\":\"reviewer\",\"tenantterm\":\"reviewer\",\"frontiersdefaultterm\":\"reviewer\",\"category\":\"label (role)\"},{\"sequencenumber\":76,\"key\":\"reviewers\",\"tenantterm\":\"reviewers\",\"frontiersdefaultterm\":\"reviewers\",\"category\":\"label (role)\"},{\"sequencenumber\":77,\"key\":\"review_finalized\",\"tenantterm\":\"review finalized\",\"frontiersdefaultterm\":\"review finalized\",\"category\":\"peer review process\"},{\"sequencenumber\":78,\"key\":\"final_decision\",\"tenantterm\":\"final decision\",\"frontiersdefaultterm\":\"final decision\",\"category\":\"peer review process\"},{\"sequencenumber\":79,\"key\":\"final_validation\",\"tenantterm\":\"final validation\",\"frontiersdefaultterm\":\"final validation\",\"category\":\"peer review process\"},{\"sequencenumber\":80,\"key\":\"ae_accept_manuscript\",\"tenantterm\":\"recommend to accept manuscript\",\"frontiersdefaultterm\":\"accept manuscript\",\"category\":\"process\"},{\"sequencenumber\":81,\"key\":\"fee\",\"tenantterm\":\"fee\",\"frontiersdefaultterm\":\"fee\",\"category\":\"accounting\"},{\"sequencenumber\":82,\"key\":\"fees\",\"tenantterm\":\"fees\",\"frontiersdefaultterm\":\"fees\",\"category\":\"accounting\"},{\"sequencenumber\":83,\"key\":\"guest_associate_editor\",\"tenantterm\":\"guest associate editor\",\"frontiersdefaultterm\":\"guest associate editor\",\"category\":\"label (role)\"},{\"sequencenumber\":84,\"key\":\"guest_associate_editors\",\"tenantterm\":\"guest associate editors\",\"frontiersdefaultterm\":\"guest associate editors\",\"category\":\"label (role)\"},{\"sequencenumber\":85,\"key\":\"in_review\",\"tenantterm\":\"in review\",\"frontiersdefaultterm\":\"in review\",\"category\":\"peer review process\"},{\"sequencenumber\":86,\"key\":\"institutional_member\",\"tenantterm\":\"institutional partner\",\"frontiersdefaultterm\":\"institutional partner\",\"category\":\"accounting\"},{\"sequencenumber\":87,\"key\":\"institutional_membership\",\"tenantterm\":\"institutional partnership\",\"frontiersdefaultterm\":\"institutional partnership\",\"category\":\"accounting\"},{\"sequencenumber\":88,\"key\":\"article_processing_charge\",\"tenantterm\":\"article processing charge\",\"frontiersdefaultterm\":\"article processing charge\",\"category\":\"accounting\"},{\"sequencenumber\":89,\"key\":\"article_processing_charges\",\"tenantterm\":\"article processing charges\",\"frontiersdefaultterm\":\"article processing charges\",\"category\":\"accounting\"},{\"sequencenumber\":90,\"key\":\"apcs\",\"tenantterm\":\"apcs\",\"frontiersdefaultterm\":\"apcs\",\"category\":\"accounting\"},{\"sequencenumber\":91,\"key\":\"apc\",\"tenantterm\":\"apc\",\"frontiersdefaultterm\":\"apc\",\"category\":\"accounting\"},{\"sequencenumber\":92,\"key\":\"received\",\"tenantterm\":\"received\",\"frontiersdefaultterm\":\"received\",\"description\":\"date manuscript was received on.\",\"category\":\"core\"},{\"sequencenumber\":93,\"key\":\"transferred\",\"tenantterm\":\"transferred\",\"frontiersdefaultterm\":\"transferred\",\"category\":\"core\"},{\"sequencenumber\":94,\"key\":\"transfer\",\"tenantterm\":\"transfer\",\"frontiersdefaultterm\":\"transfer\",\"category\":\"core\"},{\"sequencenumber\":95,\"key\":\"research_topic\",\"tenantterm\":\"research topic\",\"frontiersdefaultterm\":\"research topic\",\"category\":\"core\"},{\"sequencenumber\":96,\"key\":\"research_topics\",\"tenantterm\":\"research topics\",\"frontiersdefaultterm\":\"research topics\",\"category\":\"core\"},{\"sequencenumber\":97,\"key\":\"topic_editor\",\"tenantterm\":\"topic editor\",\"frontiersdefaultterm\":\"topic editor\",\"category\":\"label (role)\"},{\"sequencenumber\":98,\"key\":\"review_editor\",\"tenantterm\":\"community reviewer\",\"frontiersdefaultterm\":\"review editor\",\"category\":\"label (role)\"},{\"sequencenumber\":99,\"key\":\"title\",\"tenantterm\":\"title\",\"frontiersdefaultterm\":\"title\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":100,\"key\":\"running_title\",\"tenantterm\":\"running title\",\"frontiersdefaultterm\":\"running title\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":101,\"key\":\"submit\",\"tenantterm\":\"submit\",\"frontiersdefaultterm\":\"submit\",\"category\":\"process\"},{\"sequencenumber\":102,\"key\":\"submitted\",\"tenantterm\":\"submitted\",\"frontiersdefaultterm\":\"submitted\",\"category\":\"process\"},{\"sequencenumber\":103,\"key\":\"submitting\",\"tenantterm\":\"submitting\",\"frontiersdefaultterm\":\"submitting\",\"category\":\"process\"},{\"sequencenumber\":104,\"key\":\"t_e\",\"tenantterm\":\"te\",\"frontiersdefaultterm\":\"te\",\"category\":\"label (role)\"},{\"sequencenumber\":105,\"key\":\"topic\",\"tenantterm\":\"topic\",\"frontiersdefaultterm\":\"topic\",\"category\":\"process\"},{\"sequencenumber\":106,\"key\":\"topic_summary\",\"tenantterm\":\"topic summary\",\"frontiersdefaultterm\":\"topic summary\",\"category\":\"process\"},{\"sequencenumber\":107,\"key\":\"figure\",\"tenantterm\":\"figure\",\"frontiersdefaultterm\":\"figure\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":108,\"key\":\"figures\",\"tenantterm\":\"figures\",\"frontiersdefaultterm\":\"figures\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":109,\"key\":\"editorial_file\",\"tenantterm\":\"editorial file\",\"frontiersdefaultterm\":\"editorial file\",\"category\":\"core\"},{\"sequencenumber\":110,\"key\":\"editorial_files\",\"tenantterm\":\"editorial files\",\"frontiersdefaultterm\":\"editorial files\",\"category\":\"core\"},{\"sequencenumber\":111,\"key\":\"e_book\",\"tenantterm\":\"e-book\",\"frontiersdefaultterm\":\"e-book\",\"category\":\"core\"},{\"sequencenumber\":112,\"key\":\"organization\",\"tenantterm\":\"organization\",\"frontiersdefaultterm\":\"organization\",\"category\":\"core\"},{\"sequencenumber\":113,\"key\":\"institution\",\"tenantterm\":\"institution\",\"frontiersdefaultterm\":\"institution\",\"category\":\"core\"},{\"sequencenumber\":114,\"key\":\"reference\",\"tenantterm\":\"reference\",\"frontiersdefaultterm\":\"reference\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":115,\"key\":\"references\",\"tenantterm\":\"references\",\"frontiersdefaultterm\":\"references\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":116,\"key\":\"sce\",\"tenantterm\":\"sce\",\"frontiersdefaultterm\":\"sce\",\"description\":\"abbreviation for specialty chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":117,\"key\":\"submission\",\"tenantterm\":\"submission\",\"frontiersdefaultterm\":\"submission\",\"category\":\"process\"},{\"sequencenumber\":118,\"key\":\"submissions\",\"tenantterm\":\"submissions\",\"frontiersdefaultterm\":\"submissions\",\"category\":\"process\"},{\"sequencenumber\":119,\"key\":\"editing\",\"tenantterm\":\"editing\",\"frontiersdefaultterm\":\"editing\",\"category\":\"process\"},{\"sequencenumber\":120,\"key\":\"in_preparation\",\"tenantterm\":\"in preparation\",\"frontiersdefaultterm\":\"in preparation\",\"category\":\"process\"},{\"sequencenumber\":121,\"key\":\"country_region\",\"tenantterm\":\"country/region\",\"frontiersdefaultterm\":\"country/region\",\"description\":\"because of political issues, some of the country listings are actually classified as `regions` and we need to include this. however other clients may not want to do this.\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":122,\"key\":\"countries_regions\",\"tenantterm\":\"countries/regions\",\"frontiersdefaultterm\":\"countries/regions\",\"description\":\"because of political issues, some of the country listings are actually classified as `regions` and we need to include this. however other clients may not want to do this.\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":123,\"key\":\"specialty\",\"tenantterm\":\"specialty\",\"frontiersdefaultterm\":\"specialty\",\"category\":\"core\"},{\"sequencenumber\":124,\"key\":\"specialties\",\"tenantterm\":\"specialties\",\"frontiersdefaultterm\":\"specialties\",\"category\":\"core\"},{\"sequencenumber\":125,\"key\":\"associate_editors\",\"tenantterm\":\"associate editors\",\"frontiersdefaultterm\":\"associate editors\",\"description\":\"an editorial role on a specialty that a registered user may hold. this gives them rights to different functionality and parts of the platform\",\"category\":\"label (role)\"},{\"sequencenumber\":126,\"key\":\"reviewed\",\"tenantterm\":\"reviewed\",\"frontiersdefaultterm\":\"reviewed\",\"category\":\"peer review process\"},{\"sequencenumber\":127,\"key\":\"institutional_members\",\"tenantterm\":\"institutional partners\",\"frontiersdefaultterm\":\"institutional partners\",\"category\":\"accounting\"},{\"sequencenumber\":128,\"key\":\"institutional_memberships\",\"tenantterm\":\"institutional partnerships\",\"frontiersdefaultterm\":\"institutional partnerships\",\"category\":\"accounting\"},{\"sequencenumber\":129,\"key\":\"assistant_field_chief_editors\",\"tenantterm\":\"assistant field chief editors\",\"frontiersdefaultterm\":\"assistant field chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":130,\"key\":\"publications\",\"tenantterm\":\"publications\",\"frontiersdefaultterm\":\"publications\",\"category\":\"process\"},{\"sequencenumber\":131,\"key\":\"ae_accepted\",\"tenantterm\":\"recommended acceptance\",\"frontiersdefaultterm\":\"accepted\",\"category\":\"process\"},{\"sequencenumber\":132,\"key\":\"field_journal\",\"tenantterm\":\"field journal\",\"frontiersdefaultterm\":\"field journal\",\"category\":\"taxonomy\"},{\"sequencenumber\":133,\"key\":\"field_journals\",\"tenantterm\":\"field journals\",\"frontiersdefaultterm\":\"field journals\",\"category\":\"taxonomy\"},{\"sequencenumber\":134,\"key\":\"program_manager\",\"tenantterm\":\"program manager\",\"frontiersdefaultterm\":\"program manager\",\"category\":\"label (role)\"},{\"sequencenumber\":135,\"key\":\"journal_manager\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"journal manager\",\"category\":\"label (role)\"},{\"sequencenumber\":136,\"key\":\"journal_specialist\",\"tenantterm\":\"journal specialist\",\"frontiersdefaultterm\":\"journal specialist\",\"category\":\"label (role)\"},{\"sequencenumber\":137,\"key\":\"program_managers\",\"tenantterm\":\"program managers\",\"frontiersdefaultterm\":\"program managers\",\"category\":\"label (role)\"},{\"sequencenumber\":138,\"key\":\"journal_managers\",\"tenantterm\":\"journal managers\",\"frontiersdefaultterm\":\"journal managers\",\"category\":\"label (role)\"},{\"sequencenumber\":139,\"key\":\"journal_specialists\",\"tenantterm\":\"journal specialists\",\"frontiersdefaultterm\":\"journal specialists\",\"category\":\"label (role)\"},{\"sequencenumber\":140,\"key\":\"cover_letter\",\"tenantterm\":\"manuscript contribution to the field\",\"frontiersdefaultterm\":\"manuscript contribution to the field\",\"category\":\"process\"},{\"sequencenumber\":141,\"key\":\"ae_accepted_manuscript\",\"tenantterm\":\"recommended to accept manuscript\",\"frontiersdefaultterm\":\"accepted manuscript\",\"category\":\"process\"},{\"sequencenumber\":142,\"key\":\"recommend_for_rejection\",\"tenantterm\":\"recommend for rejection\",\"frontiersdefaultterm\":\"recommend for rejection\",\"category\":\"process\"},{\"sequencenumber\":143,\"key\":\"recommended_for_rejection\",\"tenantterm\":\"recommended for rejection\",\"frontiersdefaultterm\":\"recommended for rejection\",\"category\":\"process\"},{\"sequencenumber\":144,\"key\":\"ae\",\"tenantterm\":\"ae\",\"frontiersdefaultterm\":\"ae\",\"description\":\"associate editor - board member\",\"category\":\"label (role)\"},{\"sequencenumber\":145,\"key\":\"re\",\"tenantterm\":\"re\",\"frontiersdefaultterm\":\"re\",\"description\":\"review editor\",\"category\":\"label (role)\"},{\"sequencenumber\":146,\"key\":\"rev\",\"tenantterm\":\"rev\",\"frontiersdefaultterm\":\"rev\",\"description\":\"reviewer\",\"category\":\"label (role)\"},{\"sequencenumber\":147,\"key\":\"aut\",\"tenantterm\":\"aut\",\"frontiersdefaultterm\":\"aut\",\"description\":\"author\",\"category\":\"label (role)\"},{\"sequencenumber\":148,\"key\":\"coraut\",\"tenantterm\":\"coraut\",\"frontiersdefaultterm\":\"coraut\",\"description\":\"corresponding author\",\"category\":\"label (role)\"},{\"sequencenumber\":149,\"key\":\"saut\",\"tenantterm\":\"saut\",\"frontiersdefaultterm\":\"saut\",\"description\":\"submitting author\",\"category\":\"label (role)\"},{\"sequencenumber\":150,\"key\":\"coaut\",\"tenantterm\":\"coaut\",\"frontiersdefaultterm\":\"coaut\",\"description\":\"co-author\",\"category\":\"label (role)\"},{\"sequencenumber\":151,\"key\":\"tsof\",\"tenantterm\":\"tsof\",\"frontiersdefaultterm\":\"tsof\",\"description\":\"typesetter\",\"category\":\"label (role)\"},{\"sequencenumber\":152,\"key\":\"typesetting_office\",\"tenantterm\":\"typesetting office\",\"frontiersdefaultterm\":\"typesetting office\",\"category\":\"product\"},{\"sequencenumber\":153,\"key\":\"config\",\"tenantterm\":\"config\",\"frontiersdefaultterm\":\"config\",\"description\":\"configuration office role\",\"category\":\"label (role)\"},{\"sequencenumber\":154,\"key\":\"jm\",\"tenantterm\":\"jm\",\"frontiersdefaultterm\":\"jm\",\"description\":\"journal manager\",\"category\":\"label (role)\"},{\"sequencenumber\":155,\"key\":\"rte\",\"tenantterm\":\"rte\",\"frontiersdefaultterm\":\"rte\",\"description\":\"research topic editor\",\"category\":\"label (role)\"},{\"sequencenumber\":156,\"key\":\"organizations\",\"tenantterm\":\"organizations\",\"frontiersdefaultterm\":\"organizations\",\"category\":\"core\"},{\"sequencenumber\":157,\"key\":\"publishing\",\"tenantterm\":\"publishing\",\"frontiersdefaultterm\":\"publishing\",\"category\":\"core\"},{\"sequencenumber\":158,\"key\":\"acceptance\",\"tenantterm\":\"acceptance\",\"frontiersdefaultterm\":\"acceptance\",\"category\":\"process\"},{\"sequencenumber\":159,\"key\":\"preferred_associate_editor\",\"tenantterm\":\"preferred associate editor\",\"frontiersdefaultterm\":\"preferred associate editor\",\"category\":\"label (role)\"},{\"sequencenumber\":160,\"key\":\"topic_editors\",\"tenantterm\":\"topic editors\",\"frontiersdefaultterm\":\"topic editors\",\"category\":\"label (role)\"},{\"sequencenumber\":161,\"key\":\"institutions\",\"tenantterm\":\"institutions\",\"frontiersdefaultterm\":\"institutions\",\"category\":\"core\"},{\"sequencenumber\":162,\"key\":\"author(s)\",\"tenantterm\":\"author(s)\",\"frontiersdefaultterm\":\"author(s)\",\"category\":\"label (role)\"},{\"sequencenumber\":163,\"key\":\"figure(s)\",\"tenantterm\":\"figure(s)\",\"frontiersdefaultterm\":\"figure(s)\",\"category\":\"manuscript metadata\"},{\"sequencenumber\":164,\"key\":\"co-authors\",\"tenantterm\":\"co-authors\",\"frontiersdefaultterm\":\"co-authors\",\"description\":\"co-authors\",\"category\":\"label (role)\"},{\"sequencenumber\":165,\"key\":\"editorial_board_members\",\"tenantterm\":\"editorial board members\",\"frontiersdefaultterm\":\"editorial board members\",\"description\":\"editorial board members\",\"category\":\"label (role)\"},{\"sequencenumber\":166,\"key\":\"editorial_board\",\"tenantterm\":\"editorial board\",\"frontiersdefaultterm\":\"editorial board\",\"description\":\"editorial board\",\"category\":\"product\"},{\"sequencenumber\":167,\"key\":\"co-authorship\",\"tenantterm\":\"co-authorship\",\"frontiersdefaultterm\":\"co-authorship\",\"description\":\"co-authorship\",\"category\":\"misc.\"},{\"sequencenumber\":168,\"key\":\"role_id_1\",\"tenantterm\":\"registration office\",\"frontiersdefaultterm\":\"registration office\",\"category\":\"user role\"},{\"sequencenumber\":169,\"key\":\"role_id_2\",\"tenantterm\":\"editorial office\",\"frontiersdefaultterm\":\"editorial office\",\"category\":\"user role\"},{\"sequencenumber\":170,\"key\":\"role_id_7\",\"tenantterm\":\"field chief editor\",\"frontiersdefaultterm\":\"field chief editor\",\"category\":\"user role\"},{\"sequencenumber\":171,\"key\":\"role_id_8\",\"tenantterm\":\"assistant field chief editor\",\"frontiersdefaultterm\":\"assistant field chief editor\",\"category\":\"user role\"},{\"sequencenumber\":172,\"key\":\"role_id_9\",\"tenantterm\":\"specialty chief editor\",\"frontiersdefaultterm\":\"specialty chief editor\",\"category\":\"user role\"},{\"sequencenumber\":173,\"key\":\"role_id_10\",\"tenantterm\":\"assistant specialty chief editor\",\"frontiersdefaultterm\":\"assistant specialty chief editor\",\"category\":\"user role\"},{\"sequencenumber\":174,\"key\":\"role_id_11\",\"tenantterm\":\"associate editor\",\"frontiersdefaultterm\":\"associate editor\",\"category\":\"user role\"},{\"sequencenumber\":175,\"key\":\"role_id_12\",\"tenantterm\":\"guest associate editor\",\"frontiersdefaultterm\":\"guest associate editor\",\"category\":\"user role\"},{\"sequencenumber\":176,\"key\":\"role_id_13\",\"tenantterm\":\"review editor\",\"frontiersdefaultterm\":\"review editor\",\"category\":\"user role\"},{\"sequencenumber\":177,\"key\":\"role_id_14\",\"tenantterm\":\"reviewer\",\"frontiersdefaultterm\":\"reviewer\",\"category\":\"user role\"},{\"sequencenumber\":178,\"key\":\"role_id_15\",\"tenantterm\":\"author\",\"frontiersdefaultterm\":\"author\",\"category\":\"user role\"},{\"sequencenumber\":179,\"key\":\"role_id_16\",\"tenantterm\":\"corresponding author\",\"frontiersdefaultterm\":\"corresponding author\",\"category\":\"user role\"},{\"sequencenumber\":180,\"key\":\"role_id_17\",\"tenantterm\":\"submitting author\",\"frontiersdefaultterm\":\"submitting author\",\"category\":\"user role\"},{\"sequencenumber\":181,\"key\":\"role_id_18\",\"tenantterm\":\"co-author\",\"frontiersdefaultterm\":\"co-author\",\"category\":\"user role\"},{\"sequencenumber\":182,\"key\":\"role_id_20\",\"tenantterm\":\"production office\",\"frontiersdefaultterm\":\"production office\",\"category\":\"user role\"},{\"sequencenumber\":183,\"key\":\"role_id_22\",\"tenantterm\":\"typesetting office (typesetter)\",\"frontiersdefaultterm\":\"typesetting office (typesetter)\",\"category\":\"user role\"},{\"sequencenumber\":184,\"key\":\"role_id_24\",\"tenantterm\":\"registered user\",\"frontiersdefaultterm\":\"registered user\",\"category\":\"user role\"},{\"sequencenumber\":185,\"key\":\"role_id_35\",\"tenantterm\":\"job office\",\"frontiersdefaultterm\":\"job office\",\"category\":\"user role\"},{\"sequencenumber\":186,\"key\":\"role_id_41\",\"tenantterm\":\"special event administrator\",\"frontiersdefaultterm\":\"special event administrator\",\"category\":\"user role\"},{\"sequencenumber\":187,\"key\":\"role_id_42\",\"tenantterm\":\"special event reviewer\",\"frontiersdefaultterm\":\"special event reviewer\",\"category\":\"user role\"},{\"sequencenumber\":188,\"key\":\"role_id_43\",\"tenantterm\":\"submit abstract\",\"frontiersdefaultterm\":\"submit abstract\",\"category\":\"user role\"},{\"sequencenumber\":189,\"key\":\"role_id_52\",\"tenantterm\":\"events office\",\"frontiersdefaultterm\":\"events office\",\"category\":\"user role\"},{\"sequencenumber\":190,\"key\":\"role_id_53\",\"tenantterm\":\"event administrator\",\"frontiersdefaultterm\":\"event administrator\",\"category\":\"user role\"},{\"sequencenumber\":191,\"key\":\"role_id_89\",\"tenantterm\":\"content management office\",\"frontiersdefaultterm\":\"content management office\",\"category\":\"user role\"},{\"sequencenumber\":192,\"key\":\"role_id_98\",\"tenantterm\":\"accounting office\",\"frontiersdefaultterm\":\"accounting office\",\"category\":\"user role\"},{\"sequencenumber\":193,\"key\":\"role_id_99\",\"tenantterm\":\"projects\",\"frontiersdefaultterm\":\"projects\",\"category\":\"user role\"},{\"sequencenumber\":194,\"key\":\"role_id_103\",\"tenantterm\":\"configuration office\",\"frontiersdefaultterm\":\"configuration office\",\"category\":\"user role\"},{\"sequencenumber\":195,\"key\":\"role_id_104\",\"tenantterm\":\"beta user\",\"frontiersdefaultterm\":\"beta user\",\"category\":\"user role\"},{\"sequencenumber\":196,\"key\":\"role_id_106\",\"tenantterm\":\"wfconf\",\"frontiersdefaultterm\":\"wfconf\",\"category\":\"user role\"},{\"sequencenumber\":197,\"key\":\"role_id_107\",\"tenantterm\":\"rt management beta user\",\"frontiersdefaultterm\":\"rt management beta user\",\"category\":\"user role\"},{\"sequencenumber\":198,\"key\":\"role_id_108\",\"tenantterm\":\"deo beta user\",\"frontiersdefaultterm\":\"deo beta user\",\"category\":\"user role\"},{\"sequencenumber\":199,\"key\":\"role_id_109\",\"tenantterm\":\"search beta user\",\"frontiersdefaultterm\":\"search beta user\",\"category\":\"user role\"},{\"sequencenumber\":200,\"key\":\"role_id_110\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"journal manager\",\"category\":\"user role\"},{\"sequencenumber\":201,\"key\":\"role_id_111\",\"tenantterm\":\"myfrontiers beta user\",\"frontiersdefaultterm\":\"myfrontiers beta user\",\"category\":\"user role\"},{\"sequencenumber\":202,\"key\":\"role_id_21\",\"tenantterm\":\"copy editor\",\"frontiersdefaultterm\":\"copy editor\",\"category\":\"user role\"},{\"sequencenumber\":203,\"key\":\"role_id_1_abr\",\"tenantterm\":\"rof\",\"frontiersdefaultterm\":\"rof\",\"category\":\"user role\"},{\"sequencenumber\":204,\"key\":\"role_id_2_abr\",\"tenantterm\":\"eof\",\"frontiersdefaultterm\":\"eof\",\"category\":\"user role\"},{\"sequencenumber\":205,\"key\":\"role_id_7_abr\",\"tenantterm\":\"fce\",\"frontiersdefaultterm\":\"fce\",\"category\":\"user role\"},{\"sequencenumber\":206,\"key\":\"role_id_8_abr\",\"tenantterm\":\"afce\",\"frontiersdefaultterm\":\"afce\",\"category\":\"user role\"},{\"sequencenumber\":207,\"key\":\"role_id_9_abr\",\"tenantterm\":\"sce\",\"frontiersdefaultterm\":\"sce\",\"category\":\"user role\"},{\"sequencenumber\":208,\"key\":\"role_id_10_abr\",\"tenantterm\":\"asce\",\"frontiersdefaultterm\":\"asce\",\"category\":\"user role\"},{\"sequencenumber\":209,\"key\":\"role_id_11_abr\",\"tenantterm\":\"ae\",\"frontiersdefaultterm\":\"ae\",\"category\":\"user role\"},{\"sequencenumber\":210,\"key\":\"role_id_12_abr\",\"tenantterm\":\"gae\",\"frontiersdefaultterm\":\"gae\",\"category\":\"user role\"},{\"sequencenumber\":211,\"key\":\"role_id_13_abr\",\"tenantterm\":\"re\",\"frontiersdefaultterm\":\"re\",\"category\":\"user role\"},{\"sequencenumber\":212,\"key\":\"role_id_14_abr\",\"tenantterm\":\"rev\",\"frontiersdefaultterm\":\"rev\",\"category\":\"user role\"},{\"sequencenumber\":213,\"key\":\"role_id_15_abr\",\"tenantterm\":\"aut\",\"frontiersdefaultterm\":\"aut\",\"category\":\"user role\"},{\"sequencenumber\":214,\"key\":\"role_id_16_abr\",\"tenantterm\":\"coraut\",\"frontiersdefaultterm\":\"coraut\",\"category\":\"user role\"},{\"sequencenumber\":215,\"key\":\"role_id_17_abr\",\"tenantterm\":\"saut\",\"frontiersdefaultterm\":\"saut\",\"category\":\"user role\"},{\"sequencenumber\":216,\"key\":\"role_id_18_abr\",\"tenantterm\":\"coaut\",\"frontiersdefaultterm\":\"coaut\",\"category\":\"user role\"},{\"sequencenumber\":217,\"key\":\"role_id_20_abr\",\"tenantterm\":\"pof\",\"frontiersdefaultterm\":\"pof\",\"category\":\"user role\"},{\"sequencenumber\":218,\"key\":\"role_id_22_abr\",\"tenantterm\":\"tsof\",\"frontiersdefaultterm\":\"tsof\",\"category\":\"user role\"},{\"sequencenumber\":219,\"key\":\"role_id_24_abr\",\"tenantterm\":\"ru\",\"frontiersdefaultterm\":\"ru\",\"category\":\"user role\"},{\"sequencenumber\":220,\"key\":\"role_id_35_abr\",\"tenantterm\":\"jof\",\"frontiersdefaultterm\":\"jof\",\"category\":\"user role\"},{\"sequencenumber\":221,\"key\":\"role_id_41_abr\",\"tenantterm\":\"se-adm\",\"frontiersdefaultterm\":\"se-adm\",\"category\":\"user role\"},{\"sequencenumber\":222,\"key\":\"role_id_42_abr\",\"tenantterm\":\"se-rev\",\"frontiersdefaultterm\":\"se-rev\",\"category\":\"user role\"},{\"sequencenumber\":223,\"key\":\"role_id_43_abr\",\"tenantterm\":\"se-aut\",\"frontiersdefaultterm\":\"se-aut\",\"category\":\"user role\"},{\"sequencenumber\":224,\"key\":\"role_id_52_abr\",\"tenantterm\":\"evof\",\"frontiersdefaultterm\":\"evof\",\"category\":\"user role\"},{\"sequencenumber\":225,\"key\":\"role_id_53_abr\",\"tenantterm\":\"ev-adm\",\"frontiersdefaultterm\":\"ev-adm\",\"category\":\"user role\"},{\"sequencenumber\":226,\"key\":\"role_id_89_abr\",\"tenantterm\":\"comof\",\"frontiersdefaultterm\":\"comof\",\"category\":\"user role\"},{\"sequencenumber\":227,\"key\":\"role_id_98_abr\",\"tenantterm\":\"aof\",\"frontiersdefaultterm\":\"aof\",\"category\":\"user role\"},{\"sequencenumber\":228,\"key\":\"role_id_99_abr\",\"tenantterm\":\"projects\",\"frontiersdefaultterm\":\"projects\",\"category\":\"user role\"},{\"sequencenumber\":229,\"key\":\"role_id_103_abr\",\"tenantterm\":\"config\",\"frontiersdefaultterm\":\"config\",\"category\":\"user role\"},{\"sequencenumber\":230,\"key\":\"role_id_104_abr\",\"tenantterm\":\"beta\",\"frontiersdefaultterm\":\"beta\",\"category\":\"user role\"},{\"sequencenumber\":231,\"key\":\"role_id_106_abr\",\"tenantterm\":\"wfconf\",\"frontiersdefaultterm\":\"wfconf\",\"category\":\"user role\"},{\"sequencenumber\":232,\"key\":\"role_id_107_abr\",\"tenantterm\":\"rtbeta\",\"frontiersdefaultterm\":\"rtbeta\",\"category\":\"user role\"},{\"sequencenumber\":233,\"key\":\"role_id_108_abr\",\"tenantterm\":\"deobeta\",\"frontiersdefaultterm\":\"deobeta\",\"category\":\"user role\"},{\"sequencenumber\":234,\"key\":\"role_id_109_abr\",\"tenantterm\":\"searchbeta\",\"frontiersdefaultterm\":\"searchbeta\",\"category\":\"user role\"},{\"sequencenumber\":235,\"key\":\"role_id_110_abr\",\"tenantterm\":\"jm\",\"frontiersdefaultterm\":\"jm\",\"category\":\"user role\"},{\"sequencenumber\":236,\"key\":\"role_id_111_abr\",\"tenantterm\":\"mfbeta\",\"frontiersdefaultterm\":\"mfbeta\",\"category\":\"user role\"},{\"sequencenumber\":237,\"key\":\"role_id_21_abr\",\"tenantterm\":\"coped\",\"frontiersdefaultterm\":\"coped\",\"category\":\"user role\"},{\"sequencenumber\":238,\"key\":\"reviewer_editorial_board\",\"tenantterm\":\"editorial board\",\"frontiersdefaultterm\":\"editorial board\",\"description\":\"this is the label for the review editorial board\",\"category\":\"label\"},{\"sequencenumber\":239,\"key\":\"field_chief_editor\",\"tenantterm\":\"field chief editor\",\"frontiersdefaultterm\":\"field chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":240,\"key\":\"field_chief_editors\",\"tenantterm\":\"field chief editors\",\"frontiersdefaultterm\":\"field chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":241,\"key\":\"editor\",\"tenantterm\":\"editor\",\"frontiersdefaultterm\":\"editor\",\"category\":\"label (role)\"},{\"sequencenumber\":242,\"key\":\"editors\",\"tenantterm\":\"editors\",\"frontiersdefaultterm\":\"editors\",\"category\":\"label (role)\"},{\"sequencenumber\":243,\"key\":\"board\",\"tenantterm\":\"board\",\"frontiersdefaultterm\":\"board\",\"category\":\"label\"},{\"sequencenumber\":244,\"key\":\"boards\",\"tenantterm\":\"boards\",\"frontiersdefaultterm\":\"boards\",\"category\":\"label\"},{\"sequencenumber\":245,\"key\":\"article_collection\",\"tenantterm\":\"article collection\",\"frontiersdefaultterm\":\"article collection\",\"category\":\"label\"},{\"sequencenumber\":246,\"key\":\"article_collections\",\"tenantterm\":\"article collections\",\"frontiersdefaultterm\":\"article collections\",\"category\":\"label\"},{\"sequencenumber\":247,\"key\":\"handling_editor\",\"tenantterm\":\"handling editor\",\"frontiersdefaultterm\":\"associate editor\",\"description\":\"this terminology key is for the person assigned to edit a manuscript. it is a label for the temporary handling editor assignment.\",\"category\":\"label (role)\"},{\"sequencenumber\":248,\"key\":\"handling_editors\",\"tenantterm\":\"handling editors\",\"frontiersdefaultterm\":\"associate editors\",\"description\":\"this terminology key is for the person assigned to edit a manuscript. it is a label for the temporary handling editor assignment.\",\"category\":\"label (role)\"},{\"sequencenumber\":249,\"key\":\"ae_accept\",\"tenantterm\":\"recommend acceptance\",\"frontiersdefaultterm\":\"accept\",\"category\":\"process\"},{\"sequencenumber\":250,\"key\":\"rtm\",\"tenantterm\":\"rtm\",\"frontiersdefaultterm\":\"rtm\",\"category\":\"product\"},{\"sequencenumber\":251,\"key\":\"frontiers_media_sa\",\"tenantterm\":\"frontiers media s.a\",\"frontiersdefaultterm\":\"frontiers media s.a\",\"category\":\"customer\"},{\"sequencenumber\":252,\"key\":\"review_editors\",\"tenantterm\":\"community reviewers\",\"frontiersdefaultterm\":\"review editors\",\"category\":\"label (role)\"},{\"sequencenumber\":253,\"key\":\"journal_card_chief_editor\",\"tenantterm\":\"chief editor\",\"frontiersdefaultterm\":\"chief editor\",\"category\":\"label (role)\"},{\"sequencenumber\":254,\"key\":\"journal_card_chief_editors\",\"tenantterm\":\"chief editors\",\"frontiersdefaultterm\":\"chief editors\",\"category\":\"label (role)\"},{\"sequencenumber\":255,\"key\":\"call_for_papers\",\"tenantterm\":\"call for papers\",\"frontiersdefaultterm\":\"call for papers\",\"category\":\"label\"},{\"sequencenumber\":256,\"key\":\"calls_for_papers\",\"tenantterm\":\"calls for papers\",\"frontiersdefaultterm\":\"calls for papers\",\"category\":\"label\"},{\"sequencenumber\":257,\"key\":\"supervising_editor\",\"tenantterm\":\"supervising editor\",\"frontiersdefaultterm\":\"supervising editor\",\"description\":\"a chief or assistant chief editor who is assigned to a manuscript to supervise.\",\"category\":\"role\",\"externalkey\":\"supervising_editor\"},{\"sequencenumber\":258,\"key\":\"supervising_editors\",\"tenantterm\":\"supervising editors\",\"frontiersdefaultterm\":\"supervising editors\",\"description\":\"a chief or assistant chief editor who is assigned to a manuscript to supervise.\",\"category\":\"role\",\"externalkey\":\"supervising_editors\"},{\"sequencenumber\":259,\"key\":\"reviewer_endorse\",\"tenantterm\":\"endorse\",\"frontiersdefaultterm\":\"endorse\",\"category\":\"label\"},{\"sequencenumber\":260,\"key\":\"reviewer_endorsed\",\"tenantterm\":\"endorsed\",\"frontiersdefaultterm\":\"endorsed\",\"category\":\"label\"},{\"sequencenumber\":261,\"key\":\"reviewer_endorse_publication\",\"tenantterm\":\"endorse publication\",\"frontiersdefaultterm\":\"endorse publication\",\"category\":\"label\"},{\"sequencenumber\":262,\"key\":\"reviewer_endorsed_publication\",\"tenantterm\":\"endorsed publication\",\"frontiersdefaultterm\":\"endorsed publication\",\"category\":\"label\"},{\"sequencenumber\":263,\"key\":\"editor_role\",\"tenantterm\":\"editor role\",\"frontiersdefaultterm\":\"editor role\",\"category\":\"label\"},{\"sequencenumber\":264,\"key\":\"editor_roles\",\"tenantterm\":\"editor roles\",\"frontiersdefaultterm\":\"editor roles\",\"category\":\"label\"},{\"sequencenumber\":265,\"key\":\"editorial_role\",\"tenantterm\":\"editorial role\",\"frontiersdefaultterm\":\"editorial role\",\"category\":\"label\"},{\"sequencenumber\":266,\"key\":\"editorial_roles\",\"tenantterm\":\"editorial roles\",\"frontiersdefaultterm\":\"editorial roles\",\"category\":\"label\"},{\"sequencenumber\":267,\"key\":\"call_for_paper\",\"tenantterm\":\"call for paper\",\"frontiersdefaultterm\":\"call for paper\",\"category\":\"label\"},{\"sequencenumber\":268,\"key\":\"research_topic_abstract\",\"tenantterm\":\"manuscript summary\",\"frontiersdefaultterm\":\"manuscript summary\",\"category\":\"process\"},{\"sequencenumber\":269,\"key\":\"research_topic_abstracts\",\"tenantterm\":\"manuscript summaries\",\"frontiersdefaultterm\":\"manuscript summaries\",\"category\":\"process\"},{\"sequencenumber\":270,\"key\":\"submissions_team_manager\",\"tenantterm\":\"journal manager\",\"frontiersdefaultterm\":\"content manager\",\"category\":\"process\"},{\"sequencenumber\":271,\"key\":\"submissions_team\",\"tenantterm\":\"journal team\",\"frontiersdefaultterm\":\"content team\",\"category\":\"process\"},{\"sequencenumber\":272,\"key\":\"topic_coordinator\",\"tenantterm\":\"topic coordinator\",\"frontiersdefaultterm\":\"topic coordinator\",\"category\":\"process\"},{\"sequencenumber\":273,\"key\":\"topic_coordinators\",\"tenantterm\":\"topic coordinators\",\"frontiersdefaultterm\":\"topic coordinators\",\"category\":\"process\"},{\"sequencenumber\":274,\"key\":\"frontiers_journal_team\",\"tenantterm\":\"frontiers journal team\",\"frontiersdefaultterm\":\"frontiers journal team\",\"category\":\"label (role)\"},{\"sequencenumber\":275,\"key\":\"research_topic_ic_submission\",\"tenantterm\":\"rt:ic submission\",\"frontiersdefaultterm\":\"rt:ic submission\",\"category\":\"label (role)\"},{\"sequencenumber\":276,\"key\":\"research_topic_ic_submission_participant\",\"tenantterm\":\"rt:ic submission participant\",\"frontiersdefaultterm\":\"rt:ic submission participant\",\"category\":\"label (role)\"}]}'\n",impactgtmid:"gtm-njf2ml9b",impactgtmauth:"fyo-7nodm9w37qgfms03sa",impactgtmpreview:"env-1",impactgooglemapsapikey:"aizasyctehx2eu8pwfi86gw9fi00ftcykk6ifr8",qualtricsv4tov3:"'{\"enabled\":true,\"interceptid\":\"si_er0e2ze69oz8yn4\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",qualtricsumux:"'{\"enabled\":false,\"interceptid\":\"si_89fphy3etxqbwtu\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",qualtricscontinuousbrand:"'{\"enabled\":true,\"interceptid\":\"si_9zut94ut81fcrh4\",\"projectid\":\"zn_cloy77jhkpc8gai\",\"brandid\":\"frontiersin\"}'\n",defaultarticletemplate:"v3"},app:{baseurl:"/",buildid:"f2d5b6d8-d01c-4534-90b9-d659f10f17ca",buildassetsdir:"ap-2024/_nuxt",cdnurl:""}}

## Scraping Notes

- Successfully scraped from DOI.org
